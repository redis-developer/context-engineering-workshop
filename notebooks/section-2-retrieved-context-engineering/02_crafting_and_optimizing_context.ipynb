{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2abf8d931d184b7",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Crafting and Optimizing Context\n",
    "\n",
    "## From RAG Basics to Practical Context Engineering\n",
    "\n",
    "In the previous notebook, you built a working RAG system and saw why context quality matters. Now you'll learn to engineer context with professional-level rigor.\n",
    "\n",
    "**What makes context \"good\"?**\n",
    "\n",
    "This notebook teaches you that **context engineering is real engineering** - it requires the same rigor, analysis, and deliberate decision-making as any other engineering discipline. Context isn't just \"data you feed to an LLM\" - it requires thoughtful preparation, quality assessment, and optimization.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "**The Engineering Mindset:**\n",
    "- Why context quality matters (concrete impact on accuracy, relevance, cost)\n",
    "- The transformation workflow: Raw Data â†’ Engineered Context â†’ Quality Responses\n",
    "- Contrasts between naive and engineered approaches\n",
    "\n",
    "**Data Engineering for Context:**\n",
    "- Systematic transformation: Extract â†’ Clean â†’ Transform â†’ Optimize â†’ Store\n",
    "- Engineering decisions based on YOUR domain requirements\n",
    "- When to use different approaches (RAG, Structured Views, Hybrid)\n",
    "\n",
    "**Introduction to Chunking:**\n",
    "- When does your data need chunking? (Critical first question)\n",
    "- Different chunking strategies and their trade-offs\n",
    "- How to choose based on YOUR data characteristics\n",
    "\n",
    "**Context Preparation Pipelines:**\n",
    "- Three pipeline architectures (Request-Time, Batch, Event-Driven)\n",
    "- How to choose based on YOUR constraints\n",
    "- Building reusable context preparation workflows\n",
    "\n",
    "**Time to complete:** 90-105 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Section 2, Notebook 1 (RAG Fundamentals and Implementation)\n",
    "- Redis 8 running locally\n",
    "- OpenAI API key set\n",
    "- Understanding of RAG basics and vector embeddings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d806e0faab3793",
   "metadata": {},
   "source": [
    "## Part 1: Context is Data - and Data Requires Engineering\n",
    "\n",
    "### The Naive Approach (What NOT to Do)\n",
    "\n",
    "Let's start by seeing what happens when you treat context as \"just data\" without engineering discipline.\n",
    "\n",
    "**Scenario:** A student asks \"What machine learning courses are available?\"\n",
    "\n",
    "Let's see what happens with a naive approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c09fc7b40bee0a",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35c7eed6e9e9574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:13.405597Z",
     "start_time": "2025-11-04T21:16:13.396647Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:18.074137Z",
     "iopub.status.busy": "2025-11-05T13:43:18.073930Z",
     "iopub.status.idle": "2025-11-05T13:43:18.085939Z",
     "shell.execute_reply": "2025-11-05T13:43:18.085211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify required environment variables\n",
    "required_vars = [\"OPENAI_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(\n",
    "        f\"\"\"âš ï¸  Missing required environment variables: {', '.join(missing_vars)}\n",
    "\n",
    "Please create a .env file with:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "REDIS_URL=redis://localhost:6379\n",
    "\"\"\"\n",
    "    )\n",
    "    sys.exit(1)\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(\"âœ… Environment variables loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d00b06cb8ec2e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:16.311922Z",
     "start_time": "2025-11-04T21:16:13.740863Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:18.087959Z",
     "iopub.status.busy": "2025-11-05T13:43:18.087809Z",
     "iopub.status.idle": "2025-11-05T13:43:19.949380Z",
     "shell.execute_reply": "2025-11-05T13:43:19.948960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:43:19 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencies loaded"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Import dependencies\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import redis\n",
    "import tiktoken\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from redis_context_course import CourseManager, redis_config\n",
    "\n",
    "# Initialize\n",
    "course_manager = CourseManager()\n",
    "redis_client = redis.from_url(REDIS_URL, decode_responses=True)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Token counter\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "print(\"âœ… Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30bf7641e7c2bb4",
   "metadata": {},
   "source": [
    "### Naive Approach: Dump Everything\n",
    "\n",
    "The simplest approach is to include all course data in every request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6674fd4ec1bbcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:17.334336Z",
     "start_time": "2025-11-04T21:16:16.832182Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:19.950745Z",
     "iopub.status.busy": "2025-11-05T13:43:19.950638Z",
     "iopub.status.idle": "2025-11-05T13:43:20.435566Z",
     "shell.execute_reply": "2025-11-05T13:43:20.434886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:43:20 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Naive Approach Results:\n",
      "   Courses included: 10\n",
      "   Token count: 1,689\n",
      "   Estimated cost per request: $0.0042\n",
      "\n",
      "   For 100 courses, this would be ~16,890 tokens!\n",
      "\n",
      "\n",
      "ðŸ“„ Sample of raw JSON context:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"course_catalog:01K9A41NZ4FAYCBY18A6Z1Y86H\",\n",
      "    \"course_code\": \"CS004\",\n",
      "    \"title\": \"Database Systems\",\n",
      "    \"description\": \"Design and implementation of database systems. SQL, normalization, transactions, and database administration.\",\n",
      "    \"department\": \"Computer Science\",\n",
      "    \"credits\": 3,\n",
      "    \"difficulty_level\": \"intermediate\",\n",
      "    \"format\": \"online\",\n",
      "    \"instructor\": \"John Zamora\",\n",
      "    \"prerequisites\": [],\n",
      "    \"created_at\": \"2025-11-05 08:43:20.429564\",\n",
      "    \"updated_at\": \"2...\n"
     ]
    }
   ],
   "source": [
    "# Naive Approach: Get all courses and dump as JSON\n",
    "all_courses = await course_manager.get_all_courses()\n",
    "\n",
    "# Convert to raw JSON (what many developers do first)\n",
    "raw_context = json.dumps(\n",
    "    [\n",
    "        {\n",
    "            \"id\": c.id,\n",
    "            \"course_code\": c.course_code,\n",
    "            \"title\": c.title,\n",
    "            \"description\": c.description,\n",
    "            \"department\": c.department,\n",
    "            \"credits\": c.credits,\n",
    "            \"difficulty_level\": c.difficulty_level.value,\n",
    "            \"format\": c.format.value,\n",
    "            \"instructor\": c.instructor,\n",
    "            \"prerequisites\": (\n",
    "                [p.course_code for p in c.prerequisites] if c.prerequisites else []\n",
    "            ),\n",
    "            \"created_at\": str(c.created_at) if hasattr(c, \"created_at\") else None,\n",
    "            \"updated_at\": str(c.updated_at) if hasattr(c, \"updated_at\") else None,\n",
    "        }\n",
    "        for c in all_courses[:10]  # Just first 10 for demo\n",
    "    ],\n",
    "    indent=2,\n",
    ")\n",
    "\n",
    "token_count = count_tokens(raw_context)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Naive Approach Results:\n",
    "   Courses included: {len(all_courses[:10])}\n",
    "   Token count: {token_count:,}\n",
    "   Estimated cost per request: ${(token_count / 1_000_000) * 2.50:.4f}\n",
    "\n",
    "   For 100 courses, this would be ~{token_count * 10:,} tokens!\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Show a sample\n",
    "print(\"\\nðŸ“„ Sample of raw JSON context:\")\n",
    "print(raw_context[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f12aa3d9a92a5cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:17.347983Z",
     "start_time": "2025-11-04T21:16:17.344365Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:20.437326Z",
     "iopub.status.busy": "2025-11-05T13:43:20.437167Z",
     "iopub.status.idle": "2025-11-05T13:43:20.441839Z",
     "shell.execute_reply": "2025-11-05T13:43:20.441287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Database Systems',\n",
       " 'Web Development',\n",
       " 'Web Development',\n",
       " 'Web Development',\n",
       " 'Web Development',\n",
       " 'Linear Algebra',\n",
       " 'Linear Algebra',\n",
       " 'Linear Algebra',\n",
       " 'Linear Algebra',\n",
       " 'Linear Algebra',\n",
       " 'Calculus I',\n",
       " 'Calculus I',\n",
       " 'Calculus I',\n",
       " 'Calculus I',\n",
       " 'Calculus I',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Marketing Strategy',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Cognitive Psychology',\n",
       " 'Data Structures and Algorithms',\n",
       " 'Principles of Management',\n",
       " 'Principles of Management',\n",
       " 'Principles of Management',\n",
       " 'Introduction to Psychology',\n",
       " 'Introduction to Psychology',\n",
       " 'Introduction to Psychology',\n",
       " 'Data Visualization',\n",
       " 'Data Visualization',\n",
       " 'Data Visualization',\n",
       " 'Data Visualization',\n",
       " 'Machine Learning',\n",
       " 'Introduction to Programming',\n",
       " 'Introduction to Programming',\n",
       " 'Introduction to Programming',\n",
       " 'Statistics for Data Science',\n",
       " 'Statistics for Data Science',\n",
       " 'Statistics for Data Science',\n",
       " 'Statistics for Data Science',\n",
       " 'Statistics for Data Science',\n",
       " 'Statistics for Data Science']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[course.title for course in all_courses]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb7ba88280036",
   "metadata": {},
   "source": [
    "### Test the Naive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cbb2ba9a1070a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:20.589130Z",
     "start_time": "2025-11-04T21:16:19.252966Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:20.443317Z",
     "iopub.status.busy": "2025-11-05T13:43:20.443202Z",
     "iopub.status.idle": "2025-11-05T13:43:22.866201Z",
     "shell.execute_reply": "2025-11-05T13:43:22.865381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:43:22 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Query: \"What machine learning courses are available?\"\n",
      "\n",
      "Response:\n",
      "Currently, there are no machine learning courses listed in the available course catalog. If you are interested in machine learning, you might consider exploring related courses such as \"Database Systems\" or \"Linear Algebra,\" which can provide foundational knowledge useful in the field of machine learning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with a real query\n",
    "query = \"What machine learning courses are available?\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "Available Courses:\n",
    "{raw_context}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ¤– Query: \"{query}\"\n",
    "\n",
    "Response:\n",
    "{response.content}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e999b3edc323c9a",
   "metadata": {},
   "source": [
    "### Problems with the Naive Approach\n",
    "\n",
    "As discussed in previous notebooks, this approach has several problems:\n",
    "\n",
    "1. **Excessive Token Usage**\n",
    "   - 10 courses = ~1,703 tokens\n",
    "   - 100 courses would be ~17,030 tokens\n",
    "\n",
    "\n",
    "2. **Raw JSON is Inefficient**\n",
    "   - Includes internal fields (IDs, timestamps, created_at, updated_at)\n",
    "   - Verbose formatting (indentation, field names repeated)\n",
    "\n",
    "\n",
    "3. **No Filtering**\n",
    "   - Student asked about ML, but got all courses, even irrelevant ones\n",
    "   - **Dilutes relevant information with noise**\n",
    "\n",
    "\n",
    "4. **Poor Response Quality**\n",
    "   - Generic responses (\"We have many courses...\")\n",
    "   - May miss the most relevant courses\n",
    "   - Can't provide personalized recommendations\n",
    "\n",
    "\n",
    "5. **Not Scalable**\n",
    "   - What if you have 1,000 courses? 10,000?\n",
    "   - What if courses change daily?\n",
    "   - Requires code changes to update\n",
    "\n",
    "**Therefore, the goal is not only to give the LLM \"all the data\" - it's to *give it the useful data.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803dbc94b12fa6f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Engineering Mindset\n",
    "\n",
    "Context is data that flows through a pipeline. Like any data engineering problem, it requires:\n",
    "\n",
    "### 1. Requirements Analysis\n",
    "- What is the intended use case?\n",
    "- What queries will users ask?\n",
    "- What information do they need?\n",
    "- What constraints exist (token budget, latency, cost)?\n",
    "\n",
    "### 2. Data Transformation\n",
    "- Raw data â†’ Cleaned data â†’ Structured data â†’ LLM-optimized context\n",
    "\n",
    "### 3. Quality Metrics\n",
    "- How do we measure if context is \"good\"?\n",
    "- Relevance, completeness, efficiency, accuracy\n",
    "\n",
    "### 4. Testing and Iteration\n",
    "- Test with real queries\n",
    "- Measure quality metrics\n",
    "- Iterate based on results\n",
    "\n",
    "**The Engineering Question:** \"How do we transform raw course data into high-quality context that produces accurate, relevant, efficient responses?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730c35637eb3303",
   "metadata": {},
   "source": [
    "### Three Engineering Approaches\n",
    "\n",
    "Let's compare three approaches with concrete examples:\n",
    "\n",
    "| Approach | Description | Token Usage | Response Quality | Maintenance | Verdict |\n",
    "|----------|-------------|-------------|------------------|-------------|---------|\n",
    "| **Naive** | Include all raw data | 50K tokens | Poor (generic) | Easy | âŒ Not practical |\n",
    "| **RAG** | Semantic search for relevant courses | 3K tokens | Good (relevant) | Moderate | âœ… Good for most cases |\n",
    "| **Structured Views** | Pre-compute LLM-optimized summaries | 2K tokens | Excellent (overview + details) | Higher | âœ… Best for real-world use |\n",
    "| **Hybrid** | Structured view + RAG | 5K tokens | Excellent (best of both) | Higher | âœ… Best for real-world use |\n",
    "\n",
    "Let's implement each approach and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825e363289a6d65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Data Engineering Workflow - From Raw to Optimized\n",
    "\n",
    "### The Data Engineering Pipeline\n",
    "\n",
    "Context preparation follows a systematic workflow:\n",
    "\n",
    "```\n",
    "Raw Data (Database/API)\n",
    "    â†“\n",
    "[Step 1: Extract] - Get the data\n",
    "    â†“\n",
    "[Step 2: Clean] - Remove noise, fix inconsistencies\n",
    "    â†“\n",
    "[Step 3: Transform] - Structure for LLM consumption\n",
    "    â†“\n",
    "[Step 4: Optimize] - Reduce tokens, improve clarity\n",
    "    â†“\n",
    "[Step 5: Store] - Vector DB, cache, or pre-compute\n",
    "    â†“\n",
    "Engineered Context (Ready for LLM)\n",
    "```\n",
    "\n",
    "Let's walk through this pipeline with a real example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055906b662e63d",
   "metadata": {},
   "source": [
    "### Step 1: Extract (Raw Data)\n",
    "\n",
    "First, let's look at what raw course data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d43d9871aa5b9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:22.016096Z",
     "start_time": "2025-11-04T21:16:22.011996Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:22.868531Z",
     "iopub.status.busy": "2025-11-05T13:43:22.868335Z",
     "iopub.status.idle": "2025-11-05T13:43:22.873158Z",
     "shell.execute_reply": "2025-11-05T13:43:22.872458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 1: Raw Database Record\n",
      "================================================================================\n",
      "{\n",
      "  \"id\": \"course_catalog:01K9A41NZ4FAYCBY18A6Z1Y86H\",\n",
      "  \"course_code\": \"CS004\",\n",
      "  \"title\": \"Database Systems\",\n",
      "  \"description\": \"Design and implementation of database systems. SQL, normalization, transactions, and database administration.\",\n",
      "  \"department\": \"Computer Science\",\n",
      "  \"credits\": 3,\n",
      "  \"difficulty_level\": \"intermediate\",\n",
      "  \"format\": \"online\",\n",
      "  \"instructor\": \"John Zamora\",\n",
      "  \"prerequisites\": [],\n",
      "  \"created_at\": \"2025-11-05 08:43:20.429564\",\n",
      "  \"updated_at\": \"2025-11-05 08:43:20.429571\"\n",
      "}\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 161\n"
     ]
    }
   ],
   "source": [
    "# Get a sample course\n",
    "sample_course = all_courses[0]\n",
    "\n",
    "# Show raw database record\n",
    "raw_record = {\n",
    "    \"id\": sample_course.id,\n",
    "    \"course_code\": sample_course.course_code,\n",
    "    \"title\": sample_course.title,\n",
    "    \"description\": sample_course.description,\n",
    "    \"department\": sample_course.department,\n",
    "    \"credits\": sample_course.credits,\n",
    "    \"difficulty_level\": sample_course.difficulty_level.value,\n",
    "    \"format\": sample_course.format.value,\n",
    "    \"instructor\": sample_course.instructor,\n",
    "    \"prerequisites\": (\n",
    "        [p.course_code for p in sample_course.prerequisites]\n",
    "        if sample_course.prerequisites\n",
    "        else []\n",
    "    ),\n",
    "    \"created_at\": (\n",
    "        str(sample_course.created_at)\n",
    "        if hasattr(sample_course, \"created_at\")\n",
    "        else \"2024-01-15T08:30:00Z\"\n",
    "    ),\n",
    "    \"updated_at\": (\n",
    "        str(sample_course.updated_at)\n",
    "        if hasattr(sample_course, \"updated_at\")\n",
    "        else \"2024-09-01T14:22:00Z\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "raw_json = json.dumps(raw_record, indent=2)\n",
    "raw_tokens = count_tokens(raw_json)\n",
    "\n",
    "print(\"ðŸ“„ Step 1: Raw Database Record\")\n",
    "print(\"=\" * 80)\n",
    "print(raw_json)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š Token count: {raw_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736e4af9c549cec",
   "metadata": {},
   "source": [
    "Issues with above:\n",
    " - Internal fields (IDs, timestamps) waste tokens\n",
    " - Verbose JSON formatting\n",
    " - Prerequisites are codes, not human-readable\n",
    " - No structure for LLM consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9d3b83e4a304a",
   "metadata": {},
   "source": [
    "### Step 2: Clean (Remove Noise)\n",
    "\n",
    "Remove fields that don't help the LLM answer user queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17d341ad154ff9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:23.517460Z",
     "start_time": "2025-11-04T21:16:23.513732Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:22.874974Z",
     "iopub.status.busy": "2025-11-05T13:43:22.874828Z",
     "iopub.status.idle": "2025-11-05T13:43:22.878494Z",
     "shell.execute_reply": "2025-11-05T13:43:22.877880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 2: Cleaned Record\n",
      "================================================================================\n",
      "{\n",
      "  \"course_code\": \"CS004\",\n",
      "  \"title\": \"Database Systems\",\n",
      "  \"description\": \"Design and implementation of database systems. SQL, normalization, transactions, and database administration.\",\n",
      "  \"department\": \"Computer Science\",\n",
      "  \"credits\": 3,\n",
      "  \"difficulty_level\": \"intermediate\",\n",
      "  \"format\": \"online\",\n",
      "  \"instructor\": \"John Zamora\",\n",
      "  \"prerequisites\": []\n",
      "}\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 89 (saved 72 tokens, 44.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Clean - Remove internal fields\n",
    "cleaned_record = {\n",
    "    \"course_code\": sample_course.course_code,\n",
    "    \"title\": sample_course.title,\n",
    "    \"description\": sample_course.description,\n",
    "    \"department\": sample_course.department,\n",
    "    \"credits\": sample_course.credits,\n",
    "    \"difficulty_level\": sample_course.difficulty_level.value,\n",
    "    \"format\": sample_course.format.value,\n",
    "    \"instructor\": sample_course.instructor,\n",
    "    \"prerequisites\": (\n",
    "        [p.course_code for p in sample_course.prerequisites]\n",
    "        if sample_course.prerequisites\n",
    "        else []\n",
    "    ),\n",
    "}\n",
    "\n",
    "cleaned_json = json.dumps(cleaned_record, indent=2)\n",
    "cleaned_tokens = count_tokens(cleaned_json)\n",
    "\n",
    "print(\"ðŸ“„ Step 2: Cleaned Record\")\n",
    "print(\"=\" * 80)\n",
    "print(cleaned_json)\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"\\nðŸ“Š Token count: {cleaned_tokens} (saved {raw_tokens - cleaned_tokens} tokens, {((raw_tokens - cleaned_tokens) / raw_tokens * 100):.1f}% reduction)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0245185126ebc8d",
   "metadata": {},
   "source": [
    "\n",
    "Improvements:\n",
    " - Removed id, created_at, updated_at\n",
    " - Still has all information needed to answer queries\n",
    "\n",
    "Still has minor problems:\n",
    " - JSON formatting is verbose (this is a *minor* issue as LLMs can handle it; however)\n",
    " - Prerequisites are still codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916054c3caf3246f",
   "metadata": {},
   "source": [
    "### Step 3: Transform (Structure for LLM)\n",
    "\n",
    "Convert to a format optimized for LLM consumption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce586982d559bf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:24.995047Z",
     "start_time": "2025-11-04T21:16:24.990842Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:22.879993Z",
     "iopub.status.busy": "2025-11-05T13:43:22.879888Z",
     "iopub.status.idle": "2025-11-05T13:43:22.883531Z",
     "shell.execute_reply": "2025-11-05T13:43:22.883057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 3: Transformed to LLM-Friendly Format\n",
      "================================================================================\n",
      "CS004: Database Systems\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: intermediate\n",
      "Format: online\n",
      "Instructor: John Zamora\n",
      "Description: Design and implementation of database systems. SQL, normalization, transactions, and database administration.\n",
      "    \n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 50 (saved 39 tokens, 43.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Transform - Convert to LLM-friendly format\n",
    "\n",
    "\n",
    "def transform_course_to_text(course) -> str:\n",
    "    \"\"\"Transform course object to LLM-optimized text format.\"\"\"\n",
    "\n",
    "    # Build prerequisites text\n",
    "    prereq_text = \"\"\n",
    "    if course.prerequisites:\n",
    "        prereq_codes = [p.course_code for p in course.prerequisites]\n",
    "        prereq_text = f\"\\nPrerequisites: {', '.join(prereq_codes)}\"\n",
    "\n",
    "    # Build course text\n",
    "    course_text = f\"\"\"{course.course_code}: {course.title}\n",
    "Department: {course.department}\\nCredits: {course.credits}\\nLevel: {course.difficulty_level.value}\\nFormat: {course.format.value}\n",
    "Instructor: {course.instructor}{prereq_text}\n",
    "Description: {course.description}\n",
    "    \"\"\"\n",
    "\n",
    "    return course_text\n",
    "\n",
    "\n",
    "transformed_text = transform_course_to_text(sample_course)\n",
    "transformed_tokens = count_tokens(transformed_text)\n",
    "\n",
    "print(\"ðŸ“„ Step 3: Transformed to LLM-Friendly Format\")\n",
    "print(\"=\" * 80)\n",
    "print(transformed_text)\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"\\nðŸ“Š Token count: {transformed_tokens} (saved {cleaned_tokens - transformed_tokens} tokens, {((cleaned_tokens - transformed_tokens) / cleaned_tokens * 100):.1f}% reduction)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21134e639bb161",
   "metadata": {},
   "source": [
    "\n",
    "âœ… Improvements:\n",
    " - Natural text format with the correct metadata\n",
    " - Clear structure with labels\n",
    " - No JSON overhead (brackets, quotes, commas)\n",
    "\n",
    "**Note:** In case the description is too long, we can apply compression techniques such as summarization to keep the description within a desired token limit. Section 3 will cover compression in more detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d61a0bd31afaa",
   "metadata": {},
   "source": [
    "### Step 4: Optimize (Further Reduce Tokens)\n",
    "\n",
    "For even more efficiency, we can create a summarized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d542adf08de72190",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:26.480662Z",
     "start_time": "2025-11-04T21:16:26.477068Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:22.885015Z",
     "iopub.status.busy": "2025-11-05T13:43:22.884900Z",
     "iopub.status.idle": "2025-11-05T13:43:22.888179Z",
     "shell.execute_reply": "2025-11-05T13:43:22.887609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 4: Optimized (Ultra-Compact)\n",
      "================================================================================\n",
      "CS004: Database Systems - Design and implementation of database systems. SQL, normalization, transactions, and database admini...\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 24 (saved 26 tokens, 52.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Optimize - Create ultra-compact version\n",
    "# TODO: Maybe use summarization here? Maybe for that we need a longer description or some other metadata?\n",
    "\n",
    "def optimize_course_text(course) -> str:\n",
    "    \"\"\"Create ultra-compact course description.\"\"\"\n",
    "    prereqs = (\n",
    "        f\" (Prereq: {', '.join([p.course_code for p in course.prerequisites])})\"\n",
    "        if course.prerequisites\n",
    "        else \"\"\n",
    "    )\n",
    "    return (\n",
    "        f\"{course.course_code}: {course.title} - {course.description[:100]}...{prereqs}\"\n",
    "    )\n",
    "\n",
    "\n",
    "optimized_text = optimize_course_text(sample_course)\n",
    "optimized_tokens = count_tokens(optimized_text)\n",
    "\n",
    "print(\"ðŸ“„ Step 4: Optimized (Ultra-Compact)\")\n",
    "print(\"=\" * 80)\n",
    "print(optimized_text)\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"\\nðŸ“Š Token count: {optimized_tokens} (saved {transformed_tokens - optimized_tokens} tokens, {((transformed_tokens - optimized_tokens) / transformed_tokens * 100):.1f}% reduction)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034058ec3845a04",
   "metadata": {},
   "source": [
    "Improvements:\n",
    "   - Truncated description to 100 chars\n",
    "   - Removed metadata (instructor, format, credits)\n",
    "\n",
    "Trade-off:\n",
    "   - Lost some detail (may need for specific queries)\n",
    "   - Best for overview/catalog views\n",
    "\n",
    "**Note:** This is just an example of what you can do to be more efficient. This is where you have to be creative and engineer based on the usercase and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd00f8a8fc98f7",
   "metadata": {},
   "source": [
    "### Step 5: Store (Choose Storage Strategy)\n",
    "\n",
    "Now we need to decide HOW to store this engineered context:\n",
    "\n",
    "**Option 1: Vector Database (RAG)**\n",
    "- Store transformed text with embeddings\n",
    "- Retrieve relevant courses at query time\n",
    "- Good for: Large datasets, specific queries\n",
    "\n",
    "**Option 2: Pre-Computed Views**\n",
    "- Create structured summaries ahead of time\n",
    "- Store in Redis as cached views\n",
    "- Good for: Common queries, overview information\n",
    "\n",
    "**Option 3: Hybrid**\n",
    "- Combine both approaches\n",
    "- Pre-compute catalog view + RAG for details\n",
    "- Good for: Real-world systems\n",
    "\n",
    "Let's implement all three and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d1ca669115e9b",
   "metadata": {},
   "source": [
    "### Summary: The Transformation Pipeline\n",
    "\n",
    "Let's see the complete transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfae248ca80f0af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:39.408618Z",
     "start_time": "2025-11-04T21:16:39.405135Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:22.889561Z",
     "iopub.status.busy": "2025-11-05T13:43:22.889472Z",
     "iopub.status.idle": "2025-11-05T13:43:22.892238Z",
     "shell.execute_reply": "2025-11-05T13:43:22.891735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMPLE PIPELINE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Step 1: Raw Database Record\n",
      "   Token count: 161\n",
      "   Format: JSON with all fields\n",
      "\n",
      "Step 2: Cleaned Record\n",
      "   Token count: 89 (44.7% reduction)\n",
      "   Removed: Internal fields (IDs, timestamps)\n",
      "\n",
      "Step 3: Transformed to LLM Format\n",
      "   Token count: 50 (43.8% reduction from Step 2)\n",
      "   Format: Natural text, structured\n",
      "\n",
      "Step 4: Optimized (Ultra-Compact)\n",
      "   Token count: 24 (52.0% reduction from Step 3)\n",
      "   Format: Single line, truncated\n",
      "\n",
      "TOTAL REDUCTION: 161 â†’ 24 tokens (85.1% reduction)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸŽ¯ Key Insight:\n",
      "   Through systematic engineering, we reduced token usage by ~70%\n",
      "   while IMPROVING readability for the LLM!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Step 1: Raw Database Record\n",
    "   Token count: {raw_tokens}\n",
    "   Format: JSON with all fields\n",
    "\n",
    "Step 2: Cleaned Record\n",
    "   Token count: {cleaned_tokens} ({((raw_tokens - cleaned_tokens) / raw_tokens * 100):.1f}% reduction)\n",
    "   Removed: Internal fields (IDs, timestamps)\n",
    "\n",
    "Step 3: Transformed to LLM Format\n",
    "   Token count: {transformed_tokens} ({((cleaned_tokens - transformed_tokens) / cleaned_tokens * 100):.1f}% reduction from Step 2)\n",
    "   Format: Natural text, structured\n",
    "\n",
    "Step 4: Optimized (Ultra-Compact)\n",
    "   Token count: {optimized_tokens} ({((transformed_tokens - optimized_tokens) / transformed_tokens * 100):.1f}% reduction from Step 3)\n",
    "   Format: Single line, truncated\n",
    "\n",
    "TOTAL REDUCTION: {raw_tokens} â†’ {optimized_tokens} tokens ({((raw_tokens - optimized_tokens) / raw_tokens * 100):.1f}% reduction)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nðŸŽ¯ Key Insight:\")\n",
    "print(\"   Through systematic engineering, we reduced token usage by ~70%\")\n",
    "print(\"   while IMPROVING readability for the LLM!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc3b3449d3d842a",
   "metadata": {},
   "source": [
    "The key insight states that we reduced token usage.\n",
    "\n",
    "However, it should be noted that reduction is not the goal. The goal is to optimize the content and provide the most relevant information to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974af3948d4ec98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Engineering Decision - When to Use Each Approach\n",
    "\n",
    "Now let's implement the three approaches and compare them with real queries.\n",
    "\n",
    "### Approach 1: RAG (Semantic Search)\n",
    "\n",
    "Retrieve only relevant courses using vector search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1552972433032e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:40.200346Z",
     "start_time": "2025-11-04T21:16:40.193910Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:22.893549Z",
     "iopub.status.busy": "2025-11-05T13:43:22.893468Z",
     "iopub.status.idle": "2025-11-05T13:43:22.900749Z",
     "shell.execute_reply": "2025-11-05T13:43:22.900353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  Index 'course_index' not found. Please run Section 2 notebooks to create it.\n",
      "   For this demo, we'll simulate RAG results.\n"
     ]
    }
   ],
   "source": [
    "from redisvl.index import SearchIndex\n",
    "from redisvl.query import VectorQuery\n",
    "from redisvl.query.filter import Tag\n",
    "\n",
    "# Initialize vector search\n",
    "index_name = \"course_index\"\n",
    "\n",
    "# Check if index exists, create if not\n",
    "try:\n",
    "    index = SearchIndex.from_existing(index_name, redis_url=REDIS_URL)\n",
    "    print(f\"âœ… Using existing index: {index_name}\")\n",
    "except:\n",
    "    print(\n",
    "        f\"âš ï¸  Index '{index_name}' not found. Please run Section 2 notebooks to create it.\"\n",
    "    )\n",
    "    print(\"   For this demo, we'll simulate RAG results.\")\n",
    "    index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5ddc04a807cc174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:40.589240Z",
     "start_time": "2025-11-04T21:16:40.585751Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:22.901995Z",
     "iopub.status.busy": "2025-11-05T13:43:22.901919Z",
     "iopub.status.idle": "2025-11-05T13:43:22.905035Z",
     "shell.execute_reply": "2025-11-05T13:43:22.904538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š RAG Approach Results:\n",
      "   Query: \"What machine learning courses are available?\"\n",
      "   Courses retrieved: 5\n",
      "   Token count: 270\n",
      "\n",
      "ðŸ“„ Context Preview:\n",
      "CS003: Web Development\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: intermediate\n",
      "Format: in_person\n",
      "Instructor: Cody Ayala\n",
      "Description: Full-stack web development using modern frameworks. HTML, CSS, JavaScript, React, and backend APIs.\n",
      "    \n",
      "\n",
      "CS002: Web Development\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: intermediate\n",
      "Format: in_person\n",
      "Instructor: Kimberly Robertson\n",
      "Description: Full-stack web development using modern frameworks. HTML, CSS, JavaScript, React, and backend APIs.\n",
      "    \n",
      "\n",
      "CS008:...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulate RAG retrieval (in production, this would use vector search)\n",
    "\n",
    "\n",
    "async def rag_approach(query: str, limit: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant courses using semantic search.\"\"\"\n",
    "\n",
    "    # In production: Use vector search\n",
    "    # For demo: Filter courses by keyword matching\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    relevant_courses = []\n",
    "    for course in all_courses:\n",
    "        # Simple keyword matching (in production, use embeddings)\n",
    "        if any(\n",
    "            keyword in course.title.lower() or keyword in course.description.lower()\n",
    "            for keyword in [\"machine learning\", \"ml\", \"ai\", \"data science\", \"neural\"]\n",
    "        ):\n",
    "            relevant_courses.append(course)\n",
    "            if len(relevant_courses) >= limit:\n",
    "                break\n",
    "\n",
    "    # Transform to LLM-friendly format\n",
    "    context = \"\\n\\n\".join([transform_course_to_text(c) for c in relevant_courses])\n",
    "    return context\n",
    "\n",
    "\n",
    "# Test RAG approach\n",
    "query = \"What machine learning courses are available?\"\n",
    "rag_context = await rag_approach(query, limit=5)\n",
    "rag_tokens = count_tokens(rag_context)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š RAG Approach Results:\n",
    "   Query: \"{query}\"\n",
    "   Courses retrieved: 5\n",
    "   Token count: {rag_tokens:,}\n",
    "\n",
    "ðŸ“„ Context Preview:\n",
    "{rag_context[:500]}...\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43b96177edaa59",
   "metadata": {},
   "source": [
    "### Approach 2: Structured Views (Pre-Computed Summaries)\n",
    "\n",
    "Create a pre-computed catalog view that's optimized for LLM consumption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49944033c6dec60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:41.448177Z",
     "start_time": "2025-11-04T21:16:41.439641Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:22.906358Z",
     "iopub.status.busy": "2025-11-05T13:43:22.906278Z",
     "iopub.status.idle": "2025-11-05T13:43:22.913907Z",
     "shell.execute_reply": "2025-11-05T13:43:22.913549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Structured View Approach Results:\n",
      "   Total courses: 50\n",
      "   Token count: 585\n",
      "   Cached in Redis: âœ…\n",
      "\n",
      "ðŸ“„ Catalog Preview:\n",
      "# Redis University Course Catalog\n",
      "\n",
      "## Business (10 courses)\n",
      "- BUS033: Marketing Strategy (intermediate)\n",
      "- BUS035: Marketing Strategy (intermediate)\n",
      "- BUS032: Marketing Strategy (intermediate)\n",
      "- BUS034: Marketing Strategy (intermediate)\n",
      "- BUS037: Marketing Strategy (intermediate)\n",
      "- BUS039: Marketing Strategy (intermediate)\n",
      "- BUS040: Marketing Strategy (intermediate)\n",
      "- BUS038: Principles of Management (beginner)\n",
      "- BUS036: Principles of Management (beginner)\n",
      "- BUS031: Principles of Management (beginner)\n",
      "\n",
      "## Computer Science (10 courses)\n",
      "- CS004: Database Systems (intermediate)\n",
      "- CS003: Web Develo...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach 2: Structured Views\n",
    "# Pre-compute a catalog summary organized by department\n",
    "\n",
    "\n",
    "async def create_catalog_view() -> str:\n",
    "    \"\"\"Create a pre-computed catalog view organized by department.\"\"\"\n",
    "\n",
    "    # Group courses by department\n",
    "    by_department = {}\n",
    "    for course in all_courses:\n",
    "        dept = course.department\n",
    "        if dept not in by_department:\n",
    "            by_department[dept] = []\n",
    "        by_department[dept].append(course)\n",
    "\n",
    "    # Build catalog view\n",
    "    catalog_sections = []\n",
    "\n",
    "    for dept_name in sorted(by_department.keys()):\n",
    "        courses = by_department[dept_name]\n",
    "\n",
    "        # Create department section\n",
    "        dept_section = f\"\\n## {dept_name} ({len(courses)} courses)\\n\"\n",
    "\n",
    "        # Add course summaries (optimized format)\n",
    "        course_summaries = []\n",
    "        for course in courses[:10]:  # Limit for demo\n",
    "            summary = f\"- {course.course_code}: {course.title} ({course.difficulty_level.value})\"\n",
    "            course_summaries.append(summary)\n",
    "\n",
    "        dept_section += \"\\n\".join(course_summaries)\n",
    "        catalog_sections.append(dept_section)\n",
    "\n",
    "    catalog_view = \"# Redis University Course Catalog\\n\" + \"\\n\".join(catalog_sections)\n",
    "    return catalog_view\n",
    "\n",
    "\n",
    "# Create and cache the view\n",
    "catalog_view = await create_catalog_view()\n",
    "catalog_tokens = count_tokens(catalog_view)\n",
    "\n",
    "# Store in Redis for reuse\n",
    "redis_client.set(\"course_catalog_view\", catalog_view)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Structured View Approach Results:\n",
    "   Total courses: {len(all_courses)}\n",
    "   Token count: {catalog_tokens:,}\n",
    "   Cached in Redis: âœ…\n",
    "\n",
    "ðŸ“„ Catalog Preview:\n",
    "{catalog_view[:600]}...\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8297b02702e162a",
   "metadata": {},
   "source": [
    "### Approach 3: Hybrid (Best of Both Worlds)\n",
    "\n",
    "Combine structured view (overview) + RAG (specific details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1316764e1710f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:42.408022Z",
     "start_time": "2025-11-04T21:16:42.402929Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:22.915343Z",
     "iopub.status.busy": "2025-11-05T13:43:22.915253Z",
     "iopub.status.idle": "2025-11-05T13:43:22.918650Z",
     "shell.execute_reply": "2025-11-05T13:43:22.918244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Hybrid Approach Results:\n",
      "   Query: \"What machine learning courses are available?\"\n",
      "   Token count: 760\n",
      "\n",
      "   Components:\n",
      "   - Catalog overview: 585 tokens\n",
      "   - Specific details (RAG): 270 tokens\n",
      "\n",
      "ðŸ“„ Context Structure:\n",
      "   1. Full catalog overview (all departments)\n",
      "   2. Detailed info for 3 most relevant courses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach 3: Hybrid\n",
    "\n",
    "\n",
    "async def hybrid_approach(query: str) -> str:\n",
    "    \"\"\"Combine catalog overview with RAG for specific details.\"\"\"\n",
    "\n",
    "    # Part 1: Get catalog overview (from cache)\n",
    "    catalog_overview = redis_client.get(\"course_catalog_view\")\n",
    "\n",
    "    # Part 2: Get specific course details via RAG\n",
    "    specific_courses = await rag_approach(query, limit=3)\n",
    "\n",
    "    # Combine\n",
    "    hybrid_context = f\"\"\"# Course Catalog Overview\n",
    "{catalog_overview}\n",
    "\n",
    "---\n",
    "\n",
    "# Detailed Information for Your Query\n",
    "{specific_courses}\n",
    "\"\"\"\n",
    "\n",
    "    return hybrid_context\n",
    "\n",
    "\n",
    "# Test hybrid approach\n",
    "hybrid_context = await hybrid_approach(query)\n",
    "hybrid_tokens = count_tokens(hybrid_context)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Hybrid Approach Results:\n",
    "   Query: \"{query}\"\n",
    "   Token count: {hybrid_tokens:,}\n",
    "\n",
    "   Components:\n",
    "   - Catalog overview: {catalog_tokens:,} tokens\n",
    "   - Specific details (RAG): {rag_tokens:,} tokens\n",
    "\n",
    "ðŸ“„ Context Structure:\n",
    "   1. Full catalog overview (all departments)\n",
    "   2. Detailed info for 3 most relevant courses\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b810bfe1cef703fd",
   "metadata": {},
   "source": [
    "### Compare All Three Approaches\n",
    "\n",
    "Let's test all three with the same query and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cb3ddacd3f133f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:47.445269Z",
     "start_time": "2025-11-04T21:16:43.510177Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:22.920072Z",
     "iopub.status.busy": "2025-11-05T13:43:22.919993Z",
     "iopub.status.idle": "2025-11-05T13:43:26.239645Z",
     "shell.execute_reply": "2025-11-05T13:43:26.238709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARING THREE APPROACHES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:43:24 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:43:25 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:43:26 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: \"What machine learning courses are available?\"\n",
      "\n",
      "================================================================================\n",
      "APPROACH 1: RAG (Semantic Search)\n",
      "================================================================================\n",
      "Token count: 270\n",
      "Response:\n",
      "I'm sorry, but there are no machine learning courses currently available in the course list provided. If you are interested in related fields, you might consider taking \"MATH022: Linear Algebra,\" as it covers essential topics like vector spaces and matrices that are foundational for machine learning.\n",
      "\n",
      "================================================================================\n",
      "APPROACH 2: Structured View (Pre-Computed)\n",
      "================================================================================\n",
      "Token count: 585\n",
      "Response:\n",
      "We offer one advanced machine learning course:\n",
      "\n",
      "- CS007: Machine Learning (advanced)\n",
      "\n",
      "================================================================================\n",
      "APPROACH 3: Hybrid (View + RAG)\n",
      "================================================================================\n",
      "Token count: 760\n",
      "Response:\n",
      "The available machine learning course is:\n",
      "\n",
      "- CS007: Machine Learning (advanced)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test all three approaches\n",
    "query = \"What machine learning courses are available?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARING THREE APPROACHES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Approach 1: RAG\n",
    "messages_rag = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "Available Courses:\n",
    "{rag_context}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "response_rag = llm.invoke(messages_rag)\n",
    "\n",
    "# Approach 2: Structured View\n",
    "messages_view = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "{catalog_view}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "response_view = llm.invoke(messages_view)\n",
    "\n",
    "# Approach 3: Hybrid\n",
    "messages_hybrid = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "{hybrid_context}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "response_hybrid = llm.invoke(messages_hybrid)\n",
    "\n",
    "# Display comparison\n",
    "print(\n",
    "    f\"\"\"\n",
    "Query: \"{query}\"\n",
    "\n",
    "{'=' * 80}\n",
    "APPROACH 1: RAG (Semantic Search)\n",
    "{'=' * 80}\n",
    "Token count: {rag_tokens:,}\n",
    "Response:\n",
    "{response_rag.content}\n",
    "\n",
    "{'=' * 80}\n",
    "APPROACH 2: Structured View (Pre-Computed)\n",
    "{'=' * 80}\n",
    "Token count: {catalog_tokens:,}\n",
    "Response:\n",
    "{response_view.content}\n",
    "\n",
    "{'=' * 80}\n",
    "APPROACH 3: Hybrid (View + RAG)\n",
    "{'=' * 80}\n",
    "Token count: {hybrid_tokens:,}\n",
    "Response:\n",
    "{response_hybrid.content}\n",
    "\n",
    "{'=' * 80}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae3ea8350df39",
   "metadata": {},
   "source": [
    "### Decision Framework: Which Approach to Use?\n",
    "\n",
    "Here's how to choose based on YOUR requirements:\n",
    "\n",
    "| Factor | RAG | Structured Views | Hybrid |\n",
    "|--------|-----|------------------|--------|\n",
    "| **Token Efficiency** | âœ… Good (3K) | âœ…âœ… Excellent (2K) | âš ï¸ Moderate (5K) |\n",
    "| **Response Quality** | âœ… Good (relevant) | âœ… Good (overview) | âœ…âœ… Excellent (both) |\n",
    "| **Latency** | âš ï¸ Moderate (search) | âœ…âœ… Fast (cached) | âš ï¸ Moderate (search) |\n",
    "| **Maintenance** | âœ… Low (auto-updates) | âš ï¸ Higher (rebuild views) | âš ï¸ Higher (both) |\n",
    "| **Best For** | Specific queries | Overview queries | Real-world systems |\n",
    "\n",
    "**Decision Process:**\n",
    "\n",
    "1. **Analyze YOUR data characteristics:**\n",
    "   - How many items? (10s, 100s, 1000s, millions?)\n",
    "   - How often does it change? (Real-time, daily, weekly?)\n",
    "   - What's the average item size? (100 words, 1000 words, 10K words?)\n",
    "\n",
    "2. **Analyze YOUR query patterns:**\n",
    "   - Specific queries (\"Show me RU101\") â†’ RAG\n",
    "   - Overview queries (\"What courses exist?\") â†’ Structured Views\n",
    "   - Mixed queries â†’ Hybrid\n",
    "\n",
    "3. **Analyze YOUR constraints:**\n",
    "   - Tight token budget â†’ Structured Views\n",
    "   - Real-time updates required â†’ RAG\n",
    "   - Best quality needed â†’ Hybrid\n",
    "\n",
    "**Example Decision:**\n",
    "\n",
    "For Redis University:\n",
    "- âœ… **Data:** 100-500 courses, updated weekly, 200-500 words each\n",
    "- âœ… **Queries:** Mix of overview (\"What's available?\") and specific (\"ML courses?\")\n",
    "- âœ… **Constraints:** Moderate token budget, weekly updates acceptable\n",
    "- âœ… **Decision:** **Hybrid approach** (pre-compute catalog + RAG for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b924daaecb8ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Introduction to Chunking - When and Why\n",
    "\n",
    "So far, we've worked with course data where each course is a complete, self-contained unit. But what happens when you have **long documents** that contain multiple distinct topics?\n",
    "\n",
    "This is where **chunking** *may* become necessaryâ€”but it's not always the right choice.\n",
    "\n",
    "### The Critical First Question: Does My Data Need Chunking?\n",
    "\n",
    "**Chunking is NOT a default step** - it's an engineering decision that depends on multiple factors:\n",
    "\n",
    "1. **Document type:** Structured records vs. long-form text vs. PDFs\n",
    "2. **Data characteristics:** Small discrete items vs. large continuous documents\n",
    "3. **Application requirements:** Query patterns, retrieval precision needs\n",
    "4. **Embedding model limitations:** Context window size\n",
    "\n",
    "âš ï¸ **Important:** With modern long-context models (128K+ tokens), \"fitting in context\" is less of a concern. The real question is about **retrieval precision** and **data modeling**.\n",
    "\n",
    "Let's understand when chunking helpsâ€”and when it hurts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef992eb86e53dda",
   "metadata": {},
   "source": [
    "### The \"Don't Chunk\" Strategy: A Valid Option\n",
    "\n",
    "For structured data and naturally small documents, **not chunking is often the best strategy**:\n",
    "\n",
    "**Examples where whole-record embedding works better:**\n",
    "- âœ… **Course catalogs** - Each course is a complete, self-contained unit\n",
    "- âœ… **Product listings** - All product info should be retrieved together\n",
    "- âœ… **FAQ entries** - Question + answer form an atomic unit\n",
    "- âœ… **Database records** - Structured data with natural boundaries\n",
    "- âœ… **Support tickets** - Single issue with context\n",
    "\n",
    "**Why \"don't chunk\" works here:**\n",
    "- Each unit is **semantically complete** - all relevant info is together\n",
    "- Natural boundaries exist - chunking would split related information\n",
    "- Retrieval precision is maximized - you get exactly what's relevant\n",
    "- Simpler implementation - no chunking logic, no overlap decisions\n",
    "\n",
    "> ðŸ’¡ **Key Insight:** For our course catalog, each course is already an optimal retrieval unit. Chunking it would only hurt quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "192ce568978f11a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:47.462636Z",
     "start_time": "2025-11-04T21:16:47.459982Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:26.241917Z",
     "iopub.status.busy": "2025-11-05T13:43:26.241753Z",
     "iopub.status.idle": "2025-11-05T13:43:26.245254Z",
     "shell.execute_reply": "2025-11-05T13:43:26.244475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Example: Course Description\n",
      "================================================================================\n",
      "CS004: Database Systems\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: intermediate\n",
      "Format: online\n",
      "Instructor: John Zamora\n",
      "Description: Design and implementation of database systems. SQL, normalization, transactions, and database administration.\n",
      "    \n",
      "================================================================================\n",
      "\n",
      "Token count: 50\n",
      "Semantic completeness: âœ… Complete (has all info about this course)\n",
      "Chunking needed? âŒ NO\n",
      "\n",
      "Why not?\n",
      "- Under 500 tokens (well within limits)\n",
      "- Self-contained (doesn't reference other sections)\n",
      "- Semantically complete (has all course details)\n",
      "- Breaking it up would lose context\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Course data (NO chunking needed)\n",
    "sample_course_text = transform_course_to_text(all_courses[0])\n",
    "sample_tokens = count_tokens(sample_course_text)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Example: Course Description\n",
    "{'=' * 80}\n",
    "{sample_course_text}\n",
    "{'=' * 80}\n",
    "\n",
    "Token count: {sample_tokens}\n",
    "Semantic completeness: âœ… Complete (has all info about this course)\n",
    "Chunking needed? âŒ NO\n",
    "\n",
    "Why whole-record embedding works here:\n",
    "- Self-contained (doesn't reference other sections)\n",
    "- Semantically complete (has all course details)\n",
    "- Natural boundary (one course = one retrieval unit)\n",
    "- Breaking it up would hurt retrieval quality\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c1dca350c1f2d",
   "metadata": {},
   "source": [
    "### When Chunking May Help\n",
    "\n",
    "Chunking *can* improve retrieval when documents have certain characteristics:\n",
    "\n",
    "**Document types that may benefit from chunking:**\n",
    "- Research papers with multiple distinct sections\n",
    "- Technical documentation spanning many topics\n",
    "- Books and long-form content\n",
    "- Legal contracts with multiple clauses\n",
    "- Medical records with multiple visits/conditions\n",
    "\n",
    "**Why chunking helps in these cases:**\n",
    "- **Multiple distinct topics** - Different sections should be retrieved separately for precision\n",
    "- **Improves retrieval precision** - Find specific sections, not whole document\n",
    "- **Better data modeling** - Just like database design, structure affects quality\n",
    "- **Addresses research-documented context quality problems** (see below)\n",
    "\n",
    "âš ï¸ **Note:** Modern embedding models (jina-embeddings-v2, etc.) support 8K+ tokens. Model limits are less of a concern than **retrieval precision**.\n",
    "\n",
    "### Research Background: Why Long Context Can Hurt\n",
    "\n",
    "Even with large context windows (128K+ tokens), research shows that **how you structure context matters more than fitting everything in**:\n",
    "\n",
    "**1. \"Lost in the Middle\" (Stanford/UC Berkeley, 2023)**\n",
    "\n",
    "*Source: [arXiv:2307.03172](https://arxiv.org/abs/2307.03172), published in TACL 2024*\n",
    "\n",
    "LLMs exhibit a **\"U-shaped\" attention pattern**:\n",
    "- High recall for information at the **beginning** and **end** of context\n",
    "- **Significantly degraded performance** for information in the middle\n",
    "- This happens even in models explicitly designed for long contexts\n",
    "\n",
    "**Implication for chunking:** For long documents, chunking ensures the relevant section is retrieved and placed prominently in context, rather than buried in the middle of a 50-page document.\n",
    "\n",
    "**2. \"Context Rot\" (Chroma Research, 2025)**\n",
    "\n",
    "*Source: [research.trychroma.com/context-rot](https://research.trychroma.com/context-rot)*\n",
    "\n",
    "Tested 18 LLMs (GPT-4.1, Claude 4, Gemini 2.5, Qwen3) and found:\n",
    "- **Performance degrades as input length increases**, even when relevant info is present\n",
    "- **Distractor effect**: Irrelevant content actively hurts model performance\n",
    "- Even 4 distractor documents can significantly degrade output quality\n",
    "- Position of irrelevant content matters (middle is worst)\n",
    "\n",
    "**Implication for chunking:** Retrieving smaller, focused chunks reduces \"distractor tokens\" that poison the context.\n",
    "\n",
    "**3. Needle in the Haystack (NIAH) Benchmark**\n",
    "\n",
    "*Source: [Greg Kamradt, github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)*\n",
    "\n",
    "Tests whether LLMs can find a specific fact (\"needle\") buried in long context (\"haystack\"):\n",
    "- Models often **fail to retrieve information** even when it's present in context\n",
    "- Performance varies by position (middle is typically worst)\n",
    "- **Limitation**: NIAH tests lexical retrieval only, not semantic understanding\n",
    "\n",
    "**Implication for chunking:** For structured data like course catalogs, NIAH is **irrelevant**â€”each record IS the needle. Chunking would only fragment complete units.\n",
    "\n",
    "---\n",
    "\n",
    "**The Key Insight:**\n",
    "\n",
    "These research findings don't prescribe a universal chunking ruleâ€”they inform your design decisions:\n",
    "\n",
    "- **Structured records** (courses, products, FAQs): The \"lost in the middle\" problem typically doesn't apply because each record is already a focused, atomic unit. However, if your records are unusually large or contain multiple distinct topics, chunking may still help.\n",
    "\n",
    "- **Long-form documents**: Context rot and positional bias become more relevant as document length increases, but the degree depends on your specific content, query patterns, and model capabilities. Chunking can help surface relevant sections, but over-chunking fragments context.\n",
    "\n",
    "- **Mixed content types**: Real-world data rarely fits neat categories. A research paper with embedded tables, a product listing with extensive reviews, or a FAQ with nested sub-questions all require case-by-case judgment.\n",
    "\n",
    "The research provides **mental models for reasoning about trade-offs**, not binary rules. Experiment with your actual data and queries.\n",
    "\n",
    "### Emerging Strategies (2024)\n",
    "\n",
    "Recent research has introduced new approaches that challenge traditional chunking:\n",
    "\n",
    "1. **Late Chunking** (Jina AI): Embed the entire document first, then chunk the embeddings. Preserves cross-chunk context without ColBERT-level storage costs.\n",
    "\n",
    "2. **Contextual Retrieval** (Anthropic): Add LLM-generated context to each chunk before embedding. Can reduce retrieval failures by 49-67%.\n",
    "\n",
    "3. **Hybrid Search**: Combine vector embeddings with BM25 keyword search. Often outperforms either approach alone.\n",
    "\n",
    "The key insight: **chunking strategy is a design choice, not a one-size-fits-all prescription**.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Advanced Chunking: Three Distinct Document Types\n",
    "\n",
    "**Critical Insight:** Chunking is **subjective and document-dependent**. There is no universal strategy.\n",
    "\n",
    "The optimal approach depends entirely on:\n",
    "- **Document type and structure** (research papers vs. contracts vs. transcripts)\n",
    "- **Content heterogeneity** (text, tables, equations, code, images)\n",
    "- **Query patterns** (specific facts vs. conceptual understanding)\n",
    "- **Domain requirements** (legal precision vs. scientific accuracy vs. conversational context)\n",
    "- **Models being used** (embedding model capabilities, LLM context windows)\n",
    "- **Performance metrics** (retrieval precision, answer quality, latency, cost)\n",
    "\n",
    "This section explores three fundamentally different document types that require distinct chunking approaches:\n",
    "\n",
    "1. **Research Papers & Long-Form Documents** (Multimodal Content)\n",
    "2. **Legal Contracts & Clause-Level Documents** (Advanced Data Engineering)\n",
    "3. **Transcripts & Meeting Notes** (Temporal & Speaker-Based Chunking)\n"
   ],
   "id": "53c612523bb5301a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Document Type 1: Research Papers & Long-Form Documents (Multimodal Content)\n",
    "\n",
    "**âš ï¸ Important:** This category is **broad and subjective**. It includes:\n",
    "- Research papers (scientific, technical, medical)\n",
    "- Books and textbooks\n",
    "- Technical documentation\n",
    "- Financial reports\n",
    "- Medical records\n",
    "- Product manuals\n",
    "- **Even legal contracts** (which we'll discuss separately due to unique challenges)\n",
    "\n",
    "**What makes these documents similar:**\n",
    "- Heterogeneous content types (text, tables, equations, figures, code)\n",
    "- Clear or implicit structural hierarchy (sections, chapters, subsections)\n",
    "- Multiple distinct topics within single document\n",
    "- Cross-references between sections\n",
    "- Length varies from 10 pages to 1,000+ pages\n",
    "\n",
    "**Key Challenge:** Multimodal content requires specialized handling to preserve meaning.\n",
    "\n",
    "#### Why Chunking Matters: Research-Backed Benefits\n",
    "\n",
    "**1. Token Limit Management**\n",
    "- Ensures content fits within model context windows\n",
    "- Even with 128K+ context models, focused chunks improve processing efficiency\n",
    "- Reduces computational costs and latency\n",
    "- *Source: Anthropic Contextual Retrieval (2024)*\n",
    "\n",
    "**2. Improved Retrieval Precision**\n",
    "- Smaller, focused chunks create more accurate embeddings\n",
    "- **49-67% reduction in retrieval failures** with proper chunking + contextual retrieval\n",
    "- Reduces \"distractor effect\" from irrelevant content\n",
    "- *Source: Anthropic Contextual Retrieval, September 2024*\n",
    "\n",
    "**3. Context Preservation**\n",
    "- Keeps semantically related information together\n",
    "- Maintains author's logical flow and argumentation\n",
    "- Prevents fragmentation of complex ideas\n",
    "- **Chunk overlap (10-20%)** helps preserve context across boundaries\n",
    "\n",
    "**4. Computational Efficiency**\n",
    "- Faster processing with smaller chunks\n",
    "- Reduced costs (fewer tokens per query)\n",
    "- Better cache hit rates in production systems\n",
    "- *Source: \"Advancing Semantic Caching for LLMs\" (Redis/Virginia Tech, arXiv:2504.02268, 2025)*\n",
    "\n",
    "#### Chunking Strategies for Long-Form Documents\n",
    "\n",
    "**Strategy 1: Page-Level Chunking**\n",
    "- **Best for:** Financial reports, legal documents, books with natural page boundaries\n",
    "- **How:** Split by page numbers, preserving page-level context\n",
    "- **Pros:** Natural boundaries, easy to cite sources (\"See page 42\")\n",
    "- **Cons:** May split mid-sentence or mid-paragraph\n",
    "\n",
    "**Strategy 2: Structure-Aware Chunking (Markdown/HTML)**\n",
    "- **Best for:** Documents with clear hierarchical structure (research papers, technical docs)\n",
    "- **How:** Split by headings (H1, H2, H3), sections, subsections\n",
    "- **Pros:** Respects logical boundaries, preserves semantic units\n",
    "- **Cons:** Requires structured input (Markdown, HTML, LaTeX)\n",
    "\n",
    "**Strategy 3: Recursive Chunking**\n",
    "- **Best for:** Mixed content with varying structure\n",
    "- **How:** Try splitting by paragraphs â†’ sentences â†’ words (progressive fallback)\n",
    "- **Pros:** Handles edge cases gracefully, respects natural boundaries\n",
    "- **Cons:** More complex implementation\n",
    "\n",
    "**Strategy 4: Semantic Chunking**\n",
    "- **Best for:** Unstructured text without clear boundaries\n",
    "- **How:** Use embeddings to detect topic shifts, group semantically similar sentences\n",
    "- **Pros:** Content-aware, doesn't rely on formatting\n",
    "- **Cons:** Computationally expensive, may not align with author's structure\n",
    "\n",
    "**Strategy 5: Hybrid Approaches**\n",
    "- **Best for:** Production systems with diverse document types\n",
    "- **How:** Combine multiple strategies (e.g., structure-aware + semantic)\n",
    "- **Pros:** Flexible, adapts to different content types\n",
    "- **Cons:** More complex to implement and maintain\n",
    "\n",
    "**âš ï¸ Important:** The \"best\" strategy depends on YOUR specific documents and query patterns. **Experimentation is required.**\n",
    "\n",
    "#### Chunk Overlap: Preserving Context Across Boundaries\n",
    "\n",
    "**Why overlap matters:**\n",
    "- Prevents information loss at chunk boundaries\n",
    "- Maintains context for sentences that span chunks\n",
    "- Improves retrieval recall (same info appears in multiple chunks)\n",
    "\n",
    "**Recommended overlap:**\n",
    "- **10-20% of chunk size** is typical\n",
    "- Example: 1,000-token chunks with 100-200 token overlap\n",
    "- Too little: Risk losing context at boundaries\n",
    "- Too much: Redundancy, increased storage/compute costs\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Chunk 1: [Tokens 1-1000]\n",
    "Chunk 2: [Tokens 900-1900]  â† 100 tokens overlap with Chunk 1\n",
    "Chunk 3: [Tokens 1800-2800] â† 100 tokens overlap with Chunk 2\n",
    "```\n"
   ],
   "id": "9770175b8757aa7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Real-World Example: Analyzing a Research Paper\n",
    "\n",
    "Let's analyze a **real research paper** to understand chunking decisions:\n",
    "\n",
    "**Paper:** \"Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and Synthetic Data\"\n",
    "**Source:** arXiv:2504.02268 (Redis/Virginia Tech, April 2025)\n",
    "**Length:** 12 pages, ~42,000 characters\n",
    "\n",
    "**Paper Structure:**\n",
    "- Abstract (1 paragraph, ~200 words)\n",
    "- Introduction (3 pages, ~1,500 words)\n",
    "- Methodology (4 pages, includes equations and algorithms)\n",
    "- Experimental Results (3 pages, includes tables and figures)\n",
    "- Discussion & Conclusion (2 pages)\n",
    "- References (1 page)\n",
    "\n",
    "**Content Types Present:**\n",
    "1. **Text sections**: Standard prose (Introduction, Discussion)\n",
    "2. **Mathematical formulas**: Contrastive loss functions, similarity metrics\n",
    "3. **Tables**: Performance comparison across 13 embedding models (5 metrics each)\n",
    "4. **Figures**: Bar charts showing precision/recall/F1 scores\n",
    "5. **Code snippets**: Python examples for fine-tuning ModernBERT\n",
    "6. **References**: Citations to related work\n",
    "\n",
    "**Critical Question: Is chunking the ONLY way to process this paper?**\n",
    "\n",
    "âŒ **NO** - Chunking is ONE approach, but not the only one. Consider these alternatives:\n",
    "\n",
    "**Option 1: Hierarchical Retrieval (No Chunking)**\n",
    "- Create summary embedding for entire paper\n",
    "- Store full paper as retrievable unit\n",
    "- Use summary for initial search, full paper for detailed analysis\n",
    "- **Best for:** Overview queries (\"What papers discuss semantic caching?\")\n",
    "- **Pros:** Simple, preserves full context\n",
    "- **Cons:** Poor precision for specific queries\n",
    "\n",
    "**Option 2: Section-Based Chunking (Structure-Aware)**\n",
    "- Chunk by major sections (Abstract, Intro, Methods, Results, Discussion)\n",
    "- Keep tables/figures WITH their explanatory text\n",
    "- Preserve mathematical formulas with variable definitions\n",
    "- **Best for:** Specific queries (\"What methodology did they use?\")\n",
    "- **Pros:** Good precision, respects paper structure\n",
    "- **Cons:** Sections may still be large (1,500+ words)\n",
    "\n",
    "**Option 3: Hybrid Approach (Recommended)**\n",
    "- Summary embedding + section embeddings + full paper storage\n",
    "- Multi-level retrieval: summary â†’ relevant sections â†’ full context\n",
    "- **Best for:** Production systems with diverse query patterns\n",
    "- **Pros:** Flexible, handles both overview and specific queries\n",
    "- **Cons:** More complex implementation\n",
    "\n",
    "**Option 4: Multimodal RAG**\n",
    "- Extract tables/figures as separate entities\n",
    "- Use vision models (GPT-4V, Claude 3) for figure understanding\n",
    "- Link text chunks to related visual elements\n",
    "- **Best for:** Papers with critical visual information (charts, diagrams)\n",
    "- **Pros:** Handles visual content that text embeddings miss\n",
    "- **Cons:** Expensive (vision model API costs), complex pipeline\n",
    "\n",
    "**Our Expert Recommendation for This Paper:**\n",
    "\n",
    "Use **Option 3 (Hybrid)** because:\n",
    "1. Paper has clear section structure (good for structure-aware chunking)\n",
    "2. Contains multimodal content (tables, figures) that need special handling\n",
    "3. Queries could be overview (\"What's this paper about?\") or specific (\"What were the precision results?\")\n",
    "4. Hybrid approach provides flexibility without excessive complexity\n"
   ],
   "id": "4a9e75572032c2a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Handling Multimodal Content: Tables, Figures, Equations, Code\n",
    "\n",
    "**Critical Challenge:** Research papers and technical documents contain heterogeneous content types that require specialized handling.\n",
    "\n",
    "**Content Type 1: Tables**\n",
    "\n",
    "**Problem:** Tables are structured data that lose meaning when chunked mid-table or separated from context.\n",
    "\n",
    "**Best Practices:**\n",
    "- âœ… **Chunk table WITH its caption and explanation**\n",
    "  - Include: Table title, column headers, data rows, AND explanatory text\n",
    "  - Example: \"Table 1 shows HNSW performance trade-offs. As M increases from 16 to 64, recall improves from 0.89 to 0.97, but latency increases from 2.1ms to 8.7ms...\"\n",
    "- âœ… **Preserve table structure** in text format (Markdown, CSV, or structured description)\n",
    "- âœ… **Add metadata** to chunk: table number, section, key findings\n",
    "- âŒ **Don't chunk table separately** from its context - it becomes meaningless\n",
    "- âŒ **Don't split tables** across multiple chunks\n",
    "\n",
    "**Advanced Approach:** Extract tables as structured data (JSON/CSV) and store separately with links to text chunks.\n",
    "\n",
    "**Content Type 2: Mathematical Formulas**\n",
    "\n",
    "**Problem:** Formulas without context are meaningless. Variables need definitions.\n",
    "\n",
    "**Best Practices:**\n",
    "- âœ… **Chunk formula WITH its explanation and variable definitions**\n",
    "  - Include: Formula, variable definitions, interpretation, example\n",
    "  - Example: \"The recall-latency trade-off can be modeled as: Latency(M, ef) = Î±Â·MÂ² + Î²Â·ef + Î³, where M = number of connections per layer, ef = size of dynamic candidate list...\"\n",
    "- âœ… **Keep formula + derivation + application** together\n",
    "- âœ… **Include units and constraints** (e.g., \"where M âˆˆ [16, 64]\")\n",
    "- âŒ **Don't separate formula** from its meaning\n",
    "- âŒ **Don't chunk mid-derivation**\n",
    "\n",
    "**Advanced Approach:** Use LaTeX-aware chunking to preserve mathematical notation.\n",
    "\n",
    "**Content Type 3: Figures and Charts**\n",
    "\n",
    "**Problem:** Figures contain visual information that text embeddings cannot capture.\n",
    "\n",
    "**Best Practices:**\n",
    "- âœ… **Chunk figure caption WITH its discussion in text**\n",
    "  - Include: Figure number, caption, key findings, interpretation\n",
    "  - Example: \"Figure 1 shows precision/recall comparison across 13 embedding models. LangCache-Embed achieves 0.84 precision, outperforming OpenAI's text-embedding-3-small (0.65) by 29%...\"\n",
    "- âœ… **Describe visual patterns** in text (trends, comparisons, outliers)\n",
    "- âœ… **Extract data from charts** if possible (convert to table/text)\n",
    "- âŒ **Don't rely on figure alone** - text embeddings can't \"see\" images\n",
    "\n",
    "**Advanced Approach (Multimodal RAG):**\n",
    "- Use vision models (GPT-4V, Claude 3, Gemini) to generate figure descriptions\n",
    "- Store figure embeddings separately using CLIP or similar vision-language models\n",
    "- Link text chunks to related figures for retrieval\n",
    "\n",
    "**Content Type 4: Code Snippets**\n",
    "\n",
    "**Problem:** Code without context is hard to understand. Why these values? What's the use case?\n",
    "\n",
    "**Best Practices:**\n",
    "- âœ… **Chunk code WITH its context and rationale**\n",
    "  - Include: Code, explanation, use case, parameter justification\n",
    "  - Example: \"For real-world deployment, we recommend M=32 and ef_construction=200 because this balances recall (0.94) and latency (4.3ms)...\"\n",
    "- âœ… **Include comments and docstrings** in the chunk\n",
    "- âœ… **Add example usage** if relevant\n",
    "- âŒ **Don't chunk code** without explaining WHY these values\n",
    "- âŒ **Don't separate code** from its output/results\n",
    "\n",
    "**Advanced Approach:** Use code-specific embeddings (CodeBERT, GraphCodeBERT) for better semantic understanding.\n",
    "\n",
    "**Summary: Multimodal Content Chunking Principles**\n",
    "\n",
    "1. **Context is king**: Never chunk content (table/formula/figure/code) without its explanation\n",
    "2. **Preserve structure**: Maintain formatting that aids understanding (table structure, code indentation)\n",
    "3. **Add metadata**: Enrich chunks with type, section, number (e.g., \"Table 3 from Section 4.2\")\n",
    "4. **Consider alternatives**: For critical visual content, multimodal RAG may be necessary\n",
    "5. **Experiment**: Test retrieval quality with your actual queries and documents\n"
   ],
   "id": "d41ba887c65c93d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example: Research paper (NEEDS chunking)\n",
    "# Let's simulate a long research paper about Redis\n",
    "\n",
    "research_paper = \"\"\"\n",
    "# Optimizing Vector Search Performance in Redis\n",
    "\n",
    "## Abstract\n",
    "This paper presents a comprehensive analysis of vector search optimization techniques in Redis,\n",
    "examining the trade-offs between search quality, latency, and memory usage. We evaluate multiple\n",
    "indexing strategies including HNSW and FLAT indexes across datasets ranging from 10K to 10M vectors.\n",
    "Our results demonstrate that careful index configuration can improve search latency by up to 10x\n",
    "while maintaining 95%+ recall. We also introduce novel compression techniques that reduce memory\n",
    "usage by 75% with minimal impact on search quality.\n",
    "\n",
    "## 1. Introduction\n",
    "Vector databases have become essential infrastructure for modern AI applications, enabling semantic\n",
    "search, recommendation systems, and retrieval-augmented generation (RAG). Redis, traditionally known\n",
    "as an in-memory data structure store, has evolved to support high-performance vector search through\n",
    "the RediSearch module. However, optimizing vector search performance requires understanding complex\n",
    "trade-offs between multiple dimensions...\n",
    "\n",
    "[... 5,000 more words covering methodology, experiments, results, discussion ...]\n",
    "\n",
    "## 2. Background and Related Work\n",
    "Previous work on vector search optimization has focused primarily on algorithmic improvements to\n",
    "approximate nearest neighbor (ANN) search. Malkov and Yashunin (2018) introduced HNSW, which has\n",
    "become the de facto standard for high-dimensional vector search. Johnson et al. (2019) developed\n",
    "FAISS, demonstrating that product quantization can significantly reduce memory usage...\n",
    "\n",
    "[... 2,000 more words ...]\n",
    "\n",
    "## 3. Performance Analysis and Results\n",
    "\n",
    "### 3.1 HNSW Configuration Trade-offs\n",
    "\n",
    "Table 1 shows the performance comparison across different HNSW configurations. As M increases from 16 to 64,\n",
    "we observe significant improvements in recall (0.89 to 0.97) but at the cost of increased latency (2.1ms to 8.7ms)\n",
    "and memory usage (1.2GB to 3.8GB). The sweet spot for most real-world workloads is M=32 with ef_construction=200,\n",
    "which achieves 0.94 recall with 4.3ms latency.\n",
    "\n",
    "Table 1: HNSW Performance Comparison\n",
    "| M  | ef_construction | Recall@10 | Latency (ms) | Memory (GB) | Build Time (min) |\n",
    "|----|-----------------|-----------|--------------|-------------|------------------|\n",
    "| 16 | 100            | 0.89      | 2.1          | 1.2         | 8                |\n",
    "| 32 | 200            | 0.94      | 4.3          | 2.1         | 15               |\n",
    "| 64 | 400            | 0.97      | 8.7          | 3.8         | 32               |\n",
    "\n",
    "The data clearly demonstrates the fundamental trade-off between search quality and resource consumption.\n",
    "For applications requiring high recall (>0.95), the increased latency and memory costs are unavoidable.\n",
    "\n",
    "### 3.2 Mathematical Model\n",
    "\n",
    "The recall-latency trade-off can be modeled as a quadratic function of the HNSW parameters:\n",
    "\n",
    "Latency(M, ef) = Î±Â·MÂ² + Î²Â·ef + Î³\n",
    "\n",
    "Where:\n",
    "- M = number of connections per layer (controls graph connectivity)\n",
    "- ef = size of dynamic candidate list (controls search breadth)\n",
    "- Î±, Î², Î³ = dataset-specific constants (fitted from experimental data)\n",
    "\n",
    "For our e-commerce dataset, we fitted: Î±=0.002, Î²=0.015, Î³=1.2 (RÂ²=0.94)\n",
    "\n",
    "This model allows us to predict latency for untested configurations and optimize for specific\n",
    "recall targets. The quadratic dependency on M explains why doubling M more than doubles latency.\n",
    "\n",
    "## 4. Implementation Recommendations\n",
    "\n",
    "Based on our findings, we recommend the following configuration for real-world deployments:\n",
    "\n",
    "```python\n",
    "# Optimal HNSW configuration for balanced performance\n",
    "index_params = {\n",
    "    \"M\": 32,                  # Balance recall and latency\n",
    "    \"ef_construction\": 200,   # Higher quality index\n",
    "    \"ef_runtime\": 100         # Fast search with good recall\n",
    "}\n",
    "```\n",
    "\n",
    "This configuration achieves 0.94 recall with 4.3ms p95 latency, suitable for most real-time applications.\n",
    "For applications with stricter latency requirements (<2ms), consider M=16 with ef_construction=100,\n",
    "accepting the lower recall of 0.89. For applications requiring maximum recall (>0.95), use M=64\n",
    "with ef_construction=400, but ensure adequate memory and accept higher latency.\n",
    "\n",
    "[... 1,500 more words with additional analysis ...]\n",
    "\n",
    "## 5. Discussion and Conclusion\n",
    "Our findings demonstrate that vector search optimization is fundamentally about understanding\n",
    "YOUR specific requirements and constraints. There is no one-size-fits-all configuration. The choice\n",
    "between HNSW parameters depends on your specific recall requirements, latency budget, and memory constraints.\n",
    "We provide a mathematical model and practical guidelines to help practitioners make informed decisions...\n",
    "\"\"\"\n",
    "\n",
    "paper_tokens = count_tokens(research_paper)\n",
    "print(f\"Token count: {paper_tokens:,} | Words: ~{len(research_paper.split())}\")"
   ],
   "id": "e4f638f39d95535c"
  },
  {
   "cell_type": "markdown",
   "id": "5fa2bdec6f414d76",
   "metadata": {},
   "source": [
    "**ðŸ“Š Analysis: Research Paper Example**\n",
    "\n",
    "**Document:** \"Optimizing Vector Search Performance in Redis\"\n",
    "\n",
    "**Structure:** Abstract, Introduction, Background, Methodology, Results, Discussion\n",
    "\n",
    "**Chunking needed?** âœ… **YES**\n",
    "\n",
    "**Why This Document May Benefit from Chunking (Even with Large Context Windows):**\n",
    "\n",
    "> **Note:** Modern LLMs can handle 128K+ tokens, so \"fitting in context\" isn't the issue. The real value of chunking is **better data modeling and retrieval precision**.\n",
    "\n",
    "**1. Retrieval Precision vs. Recall Trade-off**\n",
    "\n",
    "Without chunking (embed entire paper):\n",
    "- Query: \"What compression techniques were used?\"\n",
    "- Retrieved: Entire 15,000-token paper (includes Abstract, Background, Results, Discussion)\n",
    "- Problem: 80% of retrieved content is irrelevant to the query\n",
    "- LLM must process 15,000 tokens to find 200 tokens of relevant information\n",
    "\n",
    "With chunking (embed by section):\n",
    "- Query: \"What compression techniques were used?\"\n",
    "- Retrieved: Methodology section (800 tokens)\n",
    "- Result: 90%+ of retrieved content is directly relevant\n",
    "- LLM processes 800 focused tokens with high signal-to-noise ratio\n",
    "\n",
    "**2. Structured Content Requires Specialized Chunking**\n",
    "\n",
    "Research papers contain heterogeneous content types that need different handling. Without specialized chunking, there will be a danger of mixing incompatible content types, chunking in the middle of tables, etc.\n",
    "\n",
    "**Tables and Charts:**\n",
    "```\n",
    "Table 1: HNSW Performance Comparison\n",
    "| M  | ef_construction | Recall@10 | Latency (ms) | Memory (GB) |\n",
    "|----|-----------------|-----------|--------------|-------------|\n",
    "| 16 | 100            | 0.89      | 2.1          | 1.2         |\n",
    "| 32 | 200            | 0.94      | 4.3          | 2.1         |\n",
    "| 64 | 400            | 0.97      | 8.7          | 3.8         |\n",
    "```\n",
    "\n",
    "**Best practice:** Chunk table WITH its caption and explanation:\n",
    "- âœ… \"Table 1 shows HNSW performance trade-offs. As M increases from 16 to 64, recall improves from 0.89 to 0.97, but latency increases from 2.1ms to 8.7ms...\"\n",
    "- âŒ Don't chunk table separately from context - it becomes meaningless\n",
    "\n",
    "**Mathematical Formulas:**\n",
    "```\n",
    "The recall-latency trade-off can be modeled as:\n",
    "Latency(M, ef) = Î±Â·MÂ² + Î²Â·ef + Î³\n",
    "\n",
    "Where:\n",
    "- M = number of connections per layer\n",
    "- ef = size of dynamic candidate list\n",
    "- Î±, Î², Î³ = dataset-specific constants\n",
    "```\n",
    "\n",
    "**Best practice:** Chunk formula WITH its explanation and variable definitions\n",
    "- âœ… Keep formula + explanation + interpretation together\n",
    "- âŒ Don't separate formula from its meaning\n",
    "\n",
    "**Code Snippets:**\n",
    "```python\n",
    "# Optimal HNSW configuration for our use case\n",
    "index_params = {\n",
    "    \"M\": 32,              # Balance recall and latency\n",
    "    \"ef_construction\": 200,  # Higher quality index\n",
    "    \"ef_runtime\": 100     # Fast search\n",
    "}\n",
    "```\n",
    "\n",
    "**Best practice:** Chunk code WITH its context and rationale\n",
    "- âœ… \"For real-world deployment, we recommend M=32 and ef_construction=200 because...\"\n",
    "- âŒ Don't chunk code without explaining WHY these values\n",
    "\n",
    "**3. Query-Specific Retrieval Patterns**\n",
    "\n",
    "Different queries need different chunks:\n",
    "\n",
    "| Query | Needs | Without Chunking | With Chunking |\n",
    "|-------|-------|------------------|---------------|\n",
    "| \"What compression techniques?\" | Methodology section | Entire paper (15K tokens) | Methodology (800 tokens) |\n",
    "| \"What were recall results?\" | Results + Table 1 | Entire paper (15K tokens) | Results section (600 tokens) |\n",
    "| \"How does HNSW work?\" | Background + Formula | Entire paper (15K tokens) | Background (500 tokens) |\n",
    "| \"What's the recommended config?\" | Discussion + Code | Entire paper (15K tokens) | Discussion (400 tokens) |\n",
    "\n",
    "**Impact:** 10-20x reduction in irrelevant context, leading to faster responses and better quality.\n",
    "\n",
    "**4. Embedding Quality: Focused vs. Averaged**\n",
    "\n",
    "**Without chunking:**\n",
    "- Embedding represents \"a paper about vector search, HNSW, compression, benchmarks, Redis...\"\n",
    "- Generic, averaged representation\n",
    "- Matches weakly with specific queries\n",
    "\n",
    "**With chunking:**\n",
    "- Methodology chunk: \"compression techniques, quantization, memory reduction, implementation details...\"\n",
    "- Results chunk: \"recall metrics, latency measurements, performance comparisons, benchmark data...\"\n",
    "- Each embedding is focused and matches strongly with relevant queries\n",
    "\n",
    "**ðŸ’¡ Key Insight:** Chunking isn't about fitting in context windows - it's about **data modeling for retrieval**. Just like you wouldn't store all customer data in one database row, you shouldn't embed all document content in one vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6dbd30ec917f3",
   "metadata": {},
   "source": [
    "### Chunking Strategies: Engineering Trade-Offs\n",
    "\n",
    "Once you've determined that your data needs chunking, the next question is: **How should you chunk it?**\n",
    "\n",
    "There's no single \"best\" chunking strategy - the optimal approach depends on YOUR data characteristics and query patterns. Let's explore different strategies and their trade-offs.\n",
    "\n",
    "**ðŸ”§ Using LangChain for Professional-Grade Chunking**\n",
    "\n",
    "In this section, we'll use **LangChain's text splitting utilities** for Strategies 2 and 3. LangChain provides battle-tested, robust implementations that handle edge cases and optimize for LLM consumption.\n",
    "\n",
    "**Why LangChain?**\n",
    "- **Industry-standard**: Used by thousands of real-world applications\n",
    "- **Smart boundary detection**: Respects natural text boundaries (paragraphs, sentences, words)\n",
    "- **Local embeddings**: Free semantic chunking with HuggingFace models (no API costs)\n",
    "- **Well-tested**: Handles edge cases (empty chunks, unicode, special characters)\n",
    "\n",
    "We'll use:\n",
    "- `RecursiveCharacterTextSplitter` (Strategy 2): Smart fixed-size chunking with boundary awareness\n",
    "- `SemanticChunker` + `HuggingFaceEmbeddings` (Strategy 3): Meaning-based chunking with local models\n",
    "\n",
    "### Strategy 1: Document-Based Chunking (Structure-Aware)\n",
    "\n",
    "**Concept:** Split documents based on their inherent structure (sections, paragraphs, headings, and as mentioned earlier, tables, code, and formulas).\n",
    "\n",
    "**Best for:** Structured documents with clear logical divisions (research papers, technical docs, books, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e395e0c41fc50ae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:48.570565Z",
     "start_time": "2025-11-04T21:16:48.565095Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:26.253942Z",
     "iopub.status.busy": "2025-11-05T13:43:26.253817Z",
     "iopub.status.idle": "2025-11-05T13:43:26.257900Z",
     "shell.execute_reply": "2025-11-05T13:43:26.257513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Strategy 1: Document-Based (Structure-Aware) Chunking\n",
      "================================================================================\n",
      "Original document: 1,035 tokens\n",
      "Number of chunks: 7\n",
      "\n",
      "Chunk breakdown:\n",
      "\n",
      "   Chunk 1: 8 tokens - # Optimizing Vector Search Performance in Redis...\n",
      "\n",
      "   Chunk 2: 108 tokens - ## Abstract This paper presents a comprehensive analysis of vector search optimization techniques in Redis, examining the trade-offs between search quality, latency, and memory usage. We evaluate multiple indexing strategies including HNSW and FLAT indexes across datasets ranging from 10K to 10M vec...\n",
      "\n",
      "   Chunk 3: 98 tokens - ## 1. Introduction Vector databases have become essential infrastructure for modern AI applications, enabling semantic search, recommendation systems, and retrieval-augmented generation (RAG). Redis, traditionally known as an in-memory data structure store, has evolved to support high-performance ve...\n",
      "\n",
      "   Chunk 4: 98 tokens - ## 2. Background and Related Work Previous work on vector search optimization has focused primarily on algorithmic improvements to approximate nearest neighbor (ANN) search. Malkov and Yashunin (2018) introduced HNSW, which has become the de facto standard for high-dimensional vector search. Johnson...\n",
      "\n",
      "   Chunk 5: 464 tokens - ## 3. Performance Analysis and Results  ### 3.1 HNSW Configuration Trade-offs  Table 1 shows the performance comparison across different HNSW configurations. As M increases from 16 to 64, we observe significant improvements in recall (0.89 to 0.97) but at the cost of increased latency (2.1ms to 8.7m...\n",
      "\n",
      "   Chunk 6: 187 tokens - ## 4. Implementation Recommendations  Based on our findings, we recommend the following configuration for production deployments:  ```python # Optimal HNSW configuration for balanced performance index_params = {     \"M\": 32,                  # Balance recall and latency     \"ef_construction\": 200,  ...\n",
      "\n",
      "   Chunk 7: 73 tokens - ## 5. Discussion and Conclusion Our findings demonstrate that vector search optimization is fundamentally about understanding YOUR specific requirements and constraints. There is no one-size-fits-all configuration. The choice between HNSW parameters depends on your specific recall requirements, late...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Document-Based Chunking\n",
    "# Split research paper by sections (using markdown headers)\n",
    "\n",
    "\n",
    "def chunk_by_structure(text: str, separator: str = \"\\n## \") -> List[str]:\n",
    "    \"\"\"Split text by structural markers (e.g., markdown headers).\"\"\"\n",
    "\n",
    "    # Split by headers\n",
    "    sections = text.split(separator)\n",
    "\n",
    "    # Clean and format chunks\n",
    "    chunks = []\n",
    "    for i, section in enumerate(sections):\n",
    "        if section.strip():\n",
    "            # Add header back (except for first chunk which is title)\n",
    "            if i > 0:\n",
    "                chunk = \"## \" + section\n",
    "            else:\n",
    "                chunk = section\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Apply to research paper\n",
    "structure_chunks = chunk_by_structure(research_paper)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Strategy 1: Document-Based (Structure-Aware) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Number of chunks: {len(structure_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(structure_chunks):\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    # Show first 100 chars of each chunk\n",
    "    preview = chunk[:300].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f1d657ce79b657",
   "metadata": {},
   "source": [
    "**Strategy 1 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- Respects document structure (sections stay together)\n",
    "- Semantically coherent (each chunk is a complete section)\n",
    "- Easy to implement for structured documents\n",
    "- Preserves author's logical organization\n",
    "- **Keeps tables, formulas, and code WITH their context** (e.g., \"## 3. Performance Analysis\" section includes Table 1 WITH its explanation, and \"## 3.2 Mathematical Model\" includes the formula WITH its variable definitions)\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Variable chunk sizes (some sections longer than others)\n",
    "- Requires documents to have clear structure\n",
    "- May create chunks that are still too large\n",
    "- Doesn't work for unstructured text\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Research papers with clear sections\n",
    "- Technical documentation with headers\n",
    "- Books with chapters/sections\n",
    "- Any markdown/HTML content with structural markers\n",
    "\n",
    "ðŸ’¡ **Key Insight:**\n",
    "Notice how Chunk 3 (\"## 3. Performance Analysis and Results\") contains Table 1 along with its explanation and interpretation. This is the correct approach - the table is meaningless without context. Similarly, the mathematical formula in section 3.2 stays with its variable definitions and interpretation. This is why structure-aware chunking is superior to fixed-size chunking for technical documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ccfaddbd73afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:16:59.014056Z",
     "start_time": "2025-11-04T21:16:59.012297Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35cb95169c19d8fd",
   "metadata": {},
   "source": [
    "### Strategy 2: Fixed-Size Chunking (Token-Based)\n",
    "\n",
    "**Concept:** Split text into chunks of a predetermined size (e.g., 512 tokens) with overlap.\n",
    "\n",
    "**Best for:** Unstructured text, quick prototyping, when you need consistent chunk sizes.\n",
    "\n",
    "Trade-offs:\n",
    "- Ignores document structure (may split mid-sentence or mid-paragraph or mid-table)\n",
    "- Can break semantic coherence\n",
    "- May split important information across chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c370a104c561f59a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:17:00.068503Z",
     "start_time": "2025-11-04T21:17:00.051846Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:26.259835Z",
     "iopub.status.busy": "2025-11-05T13:43:26.259723Z",
     "iopub.status.idle": "2025-11-05T13:43:26.273094Z",
     "shell.execute_reply": "2025-11-05T13:43:26.272720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Running fixed-size chunking with LangChain...\n",
      "   Trying to split on: paragraphs â†’ sentences â†’ words â†’ characters\n",
      "\n",
      "ðŸ“Š Strategy 2: Fixed-Size (LangChain) Chunking\n",
      "================================================================================\n",
      "Original document: 1,035 tokens\n",
      "Target chunk size: 800 characters (~200 words)\n",
      "Overlap: 100 characters\n",
      "Number of chunks: 8\n",
      "\n",
      "Chunk breakdown:\n",
      "\n",
      "   Chunk 1: 117 tokens - # Optimizing Vector Search Performance in Redis  ## Abstract This paper presents a comprehensive ana...\n",
      "   Chunk 2: 98 tokens - ## 1. Introduction Vector databases have become essential infrastructure for modern AI applications,...\n",
      "   Chunk 3: 134 tokens - [... 5,000 more words covering methodology, experiments, results, discussion ...]  ## 2. Background ...\n",
      "   Chunk 4: 128 tokens - ## 3. Performance Analysis and Results  ### 3.1 HNSW Configuration Trade-offs  Table 1 shows the per...\n",
      "   Chunk 5: 206 tokens - Table 1: HNSW Performance Comparison | M  | ef_construction | Recall@10 | Latency (ms) | Memory (GB)...\n",
      "... (3 more chunks)\n"
     ]
    }
   ],
   "source": [
    "# Strategy 2: Fixed-Size Chunking (Using LangChain)\n",
    "# Industry-standard approach with smart boundary detection\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter with smart boundary detection\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # Target chunk size in characters\n",
    "    chunk_overlap=100,  # Overlap to preserve context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Try these in order\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "print(\"ðŸ”„ Running fixed-size chunking with LangChain...\")\n",
    "print(\"   Trying to split on: paragraphs â†’ sentences â†’ words â†’ characters\\n\")\n",
    "\n",
    "# Apply to research paper\n",
    "fixed_chunks_docs = text_splitter.create_documents([research_paper])\n",
    "fixed_chunks = [doc.page_content for doc in fixed_chunks_docs]\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Strategy 2: Fixed-Size (LangChain) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Target chunk size: 800 characters (~200 words)\n",
    "Overlap: 100 characters\n",
    "Number of chunks: {len(fixed_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(fixed_chunks[:5]):  # Show first 5\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:100].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "print(f\"... ({len(fixed_chunks) - 5} more chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e403c38d9a9d0a06",
   "metadata": {},
   "source": [
    "**Strategy 2 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- **Respects natural boundaries**: Tries paragraphs â†’ sentences â†’ words â†’ characters\n",
    "- Consistent chunk sizes (predictable token usage)\n",
    "- Works on any text (structured or unstructured)\n",
    "- Fast processing\n",
    "- **Doesn't split mid-sentence** (unless absolutely necessary)\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Ignores document structure (doesn't understand sections)\n",
    "- Can break semantic coherence (may split related content)\n",
    "- Overlap creates redundancy (increases storage/cost)\n",
    "- May split important information across chunks\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Unstructured text (no clear sections)\n",
    "- Quick prototyping and baselines\n",
    "- When consistent chunk sizes are required\n",
    "- Simple documents where structure doesn't matter\n",
    "\n",
    "ðŸ’¡ **How RecursiveCharacterTextSplitter Works:**\n",
    "\n",
    "Unlike naive fixed-size splitting, this algorithm:\n",
    "\n",
    "1. **Tries separators in order**: `[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]`\n",
    "2. **Splits on first successful separator** that keeps chunks under target size\n",
    "3. **Falls back to next separator** if chunks are still too large\n",
    "4. **Preserves natural boundaries** (paragraphs > sentences > words > characters)\n",
    "\n",
    "**Example:**\n",
    "- Target: 800 characters\n",
    "- First try: Split on `\\n\\n` (paragraphs)\n",
    "- If paragraph > 800 chars: Split on `\\n` (lines)\n",
    "- If line > 800 chars: Split on `. ` (sentences)\n",
    "- And so on...\n",
    "\n",
    "**Why this is better than naive splitting:**\n",
    "- âœ… Respects natural text boundaries\n",
    "- âœ… Doesn't split mid-sentence (unless necessary)\n",
    "- âœ… Maintains readability\n",
    "- âœ… Better for LLM comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37f437b3aa4541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:17:04.920222Z",
     "start_time": "2025-11-04T21:17:04.917767Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca46498e87604fa5",
   "metadata": {},
   "source": [
    "### Strategy 3: Semantic Chunking (Meaning-Based)\n",
    "\n",
    "**Concept:** Split text based on semantic similarity using embeddings - create new chunks when topic changes significantly.\n",
    "\n",
    "**How it works:**\n",
    "1. Split text into sentences or paragraphs\n",
    "2. Generate embeddings for each segment\n",
    "3. Calculate similarity between consecutive segments\n",
    "4. Create chunk boundaries where similarity drops (topic shift detected)\n",
    "\n",
    "**Best for:** Dense academic text, legal documents, narratives where semantic boundaries don't align with structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de5dadba814863e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T21:17:06.687906Z",
     "start_time": "2025-11-04T21:17:06.551885Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:26.274572Z",
     "iopub.status.busy": "2025-11-05T13:43:26.274474Z",
     "iopub.status.idle": "2025-11-05T13:43:28.853108Z",
     "shell.execute_reply": "2025-11-05T13:43:28.852649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:43:27 sentence_transformers.SentenceTransformer INFO   Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Running semantic chunking with LangChain...\n",
      "   Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\n",
      "   Breakpoint detection: 25th percentile of similarity scores\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Strategy 3: Semantic (LangChain) Chunking\n",
      "================================================================================\n",
      "Original document: 1,035 tokens\n",
      "Breakpoint method: Percentile (25th percentile)\n",
      "Number of chunks: 25\n",
      "\n",
      "Chunk breakdown:\n",
      "\n",
      "   Chunk 1: 70 tokens -  # Optimizing Vector Search Performance in Redis  ## Abstract This paper presents a comprehensive analysis of vector search optimization techniques in Redis, examining the trade-offs between search qu...\n",
      "\n",
      "   Chunk 2: 26 tokens - Our results demonstrate that careful index configuration can improve search latency by up to 10x while maintaining 95%+ recall....\n",
      "\n",
      "   Chunk 3: 22 tokens - We also introduce novel compression techniques that reduce memory usage by 75% with minimal impact on search quality....\n",
      "\n",
      "   Chunk 4: 4 tokens - ## 1....\n",
      "\n",
      "   Chunk 5: 60 tokens - Introduction Vector databases have become essential infrastructure for modern AI applications, enabling semantic search, recommendation systems, and retrieval-augmented generation (RAG). Redis, tradit...\n",
      "\n",
      "   Chunk 6: 16 tokens - However, optimizing vector search performance requires understanding complex trade-offs between multiple dimensions......\n",
      "\n",
      "   Chunk 7: 2 tokens - [......\n",
      "\n",
      "   Chunk 8: 18 tokens - 5,000 more words covering methodology, experiments, results, discussion ...]  ## 2....\n",
      "\n",
      "   Chunk 9: 60 tokens - Background and Related Work Previous work on vector search optimization has focused primarily on algorithmic improvements to approximate nearest neighbor (ANN) search. Malkov and Yashunin (2018) intro...\n",
      "\n",
      "   Chunk 10: 4 tokens - Johnson et al....\n",
      "\n",
      "   Chunk 11: 20 tokens - (2019) developed FAISS, demonstrating that product quantization can significantly reduce memory usage......\n",
      "\n",
      "   Chunk 12: 2 tokens - [......\n",
      "\n",
      "   Chunk 13: 10 tokens - 2,000 more words ...]  ## 3....\n",
      "\n",
      "   Chunk 14: 91 tokens - Performance Analysis and Results  ### 3.1 HNSW Configuration Trade-offs  Table 1 shows the performance comparison across different HNSW configurations. As M increases from 16 to 64, we observe signifi...\n",
      "\n",
      "   Chunk 15: 33 tokens - The sweet spot for most production workloads is M=32 with ef_construction=200, which achieves 0.94 recall with 4.3ms latency....\n",
      "\n",
      "   Chunk 16: 177 tokens - Table 1: HNSW Performance Comparison | M  | ef_construction | Recall@10 | Latency (ms) | Memory (GB) | Build Time (min) | |----|-----------------|-----------|--------------|-------------|-------------...\n",
      "\n",
      "   Chunk 17: 159 tokens - ### 3.2 Mathematical Model  The recall-latency trade-off can be modeled as a quadratic function of the HNSW parameters:  Latency(M, ef) = Î±Â·MÂ² + Î²Â·ef + Î³  Where: - M = number of connections per layer ...\n",
      "\n",
      "   Chunk 18: 4 tokens - ## 4....\n",
      "\n",
      "   Chunk 19: 139 tokens - Implementation Recommendations  Based on our findings, we recommend the following configuration for production deployments:  ```python # Optimal HNSW configuration for balanced performance index_param...\n",
      "\n",
      "   Chunk 20: 31 tokens - For applications requiring maximum recall (>0.95), use M=64 with ef_construction=400, but ensure adequate memory and accept higher latency....\n",
      "\n",
      "   Chunk 21: 2 tokens - [......\n",
      "\n",
      "   Chunk 22: 35 tokens - 1,500 more words with additional analysis ...]  ## 5. Discussion and Conclusion Our findings demonstrate that vector search optimization is fundamentally about understanding YOUR specific requirements...\n",
      "\n",
      "   Chunk 23: 10 tokens - There is no one-size-fits-all configuration....\n",
      "\n",
      "   Chunk 24: 37 tokens - The choice between HNSW parameters depends on your specific recall requirements, latency budget, and memory constraints. We provide a mathematical model and practical guidelines to help practitioners ...\n",
      "\n",
      "   Chunk 25: 0 tokens - ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Strategy 3: Semantic Chunking (Using LangChain)\n",
    "# Industry-standard approach with local embeddings (no API costs!)\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Suppress tokenizer warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Initialize local embeddings (no API costs!)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "# Create semantic chunker with percentile-based breakpoint detection\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",  # Split at bottom 25% of similarities\n",
    "    breakpoint_threshold_amount=25,  # 25th percentile\n",
    "    buffer_size=1,  # Compare consecutive sentences\n",
    ")\n",
    "\n",
    "print(\"ðŸ”„ Running semantic chunking with LangChain...\")\n",
    "print(\"   Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\")\n",
    "print(\"   Breakpoint detection: 25th percentile of similarity scores\\n\")\n",
    "\n",
    "# Apply to research paper\n",
    "semantic_chunks_docs = semantic_chunker.create_documents([research_paper])\n",
    "\n",
    "# Extract text from Document objects\n",
    "semantic_chunks = [doc.page_content for doc in semantic_chunks_docs]\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Strategy 3: Semantic (LangChain) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Breakpoint method: Percentile (25th percentile)\n",
    "Number of chunks: {len(semantic_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:200].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4d65b4ad36fd9",
   "metadata": {},
   "source": [
    "**Strategy 3 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- **Detects actual topic changes** using semantic similarity (not just structural markers)\n",
    "- Preserves semantic coherence (topics stay together even without headers)\n",
    "- Better retrieval quality (chunks are topically focused)\n",
    "- Adapts to content (works on unstructured text)\n",
    "- Reduces context loss at boundaries (doesn't split mid-topic)\n",
    "- **Free and local**: Uses sentence-transformers (no API costs)\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Slower processing (must compute embeddings for each sentence)\n",
    "- Variable chunk sizes (depends on topic boundaries)\n",
    "- Higher computational cost (embedding computation + similarity calculations)\n",
    "- Requires initial model download (~90MB for all-MiniLM-L6-v2)\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Dense academic papers with complex topic transitions\n",
    "- Legal documents where semantic sections don't have headers\n",
    "- Narratives where topics don't align with structure\n",
    "- Unstructured text (emails, transcripts, conversations)\n",
    "- When retrieval quality is more important than processing speed\n",
    "\n",
    "ðŸ’¡ **How Percentile-Based Breakpoint Detection Works:**\n",
    "\n",
    "Instead of using a fixed similarity threshold (e.g., 0.75), the percentile method:\n",
    "\n",
    "1. **Computes all similarities** between consecutive sentences\n",
    "2. **Calculates percentiles** of the similarity distribution\n",
    "3. **Creates breakpoints** where similarity is in the bottom X percentile\n",
    "\n",
    "**Example:**\n",
    "- Similarities: [0.92, 0.88, 0.45, 0.91, 0.35, 0.89]\n",
    "- 25th percentile: 0.45\n",
    "- Breakpoints created at: positions 2 (0.45) and 4 (0.35)\n",
    "\n",
    "**Why this is better than fixed threshold:**\n",
    "- âœ… Adapts to document's similarity distribution\n",
    "- âœ… Works across different document types\n",
    "- âœ… No manual threshold tuning needed\n",
    "- âœ… More robust to outliers\n",
    "\n",
    "**Alternative Breakpoint Methods:**\n",
    "- `\"gradient\"`: Detects sudden drops in similarity (topic shifts)\n",
    "- `\"standard_deviation\"`: Uses statistical deviation from mean\n",
    "- `\"interquartile\"`: Uses IQR-based outlier detection\n",
    "\n",
    "This is fundamentally different from structure-based chunking - it detects semantic boundaries regardless of headers or formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fca0e15cdba8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Deep Dive: Document Types and Specialized Chunking Strategies\n",
    "\n",
    "The chunking strategies we've covered (document-based, fixed-size, semantic) are foundational approaches. However, real-world documents often require specialized handling based on their content type and structure.\n",
    "\n",
    "Let's explore three distinct document categories that require different chunking considerations:\n",
    "\n",
    "### 1. Multi-Modal Documents: Research Papers, Books, Technical Reports\n",
    "\n",
    "These documents contain **heterogeneous content types** that require specialized handling beyond simple text chunking.\n",
    "\n",
    "**Content Types in Multi-Modal Documents:**\n",
    "\n",
    "1. **Text sections** (prose, paragraphs)\n",
    "   - Standard semantic or structure-aware chunking works well\n",
    "   - Can use any of the strategies we've covered\n",
    "\n",
    "2. **Mathematical formulas and equations**\n",
    "   - âš ï¸ **Critical:** Must be kept intact with surrounding context\n",
    "   - Splitting a formula from its explanation renders it meaningless\n",
    "   - Example: `Latency(M, ef) = Î±Â·MÂ² + Î²Â·ef + Î³` needs variable definitions\n",
    "\n",
    "3. **Charts, graphs, and figures**\n",
    "   - May require **image extraction** and separate processing\n",
    "   - Options:\n",
    "     - **Multimodal embeddings** (CLIP, GPT-4V) to embed images directly\n",
    "     - **OCR** to extract text from images\n",
    "     - **Caption + description** as text proxy\n",
    "   - Keep figure WITH its caption and reference in text\n",
    "\n",
    "4. **Tables**\n",
    "   - âš ï¸ **Critical:** Should be kept as complete units\n",
    "   - Options:\n",
    "     - Keep table WITH caption and explanation (best for retrieval)\n",
    "     - Convert to structured format (JSON, CSV) for programmatic access\n",
    "     - Use multimodal models to understand table structure\n",
    "   - Never split a table across chunks\n",
    "\n",
    "**Key Benefits of Proper Chunking for Multi-Modal Documents:**\n",
    "\n",
    "Based on research and industry best practices:\n",
    "\n",
    "1. **Token Limit Management**\n",
    "   - Ensures content fits within model context windows\n",
    "   - Prevents truncation of critical information\n",
    "   - Allows processing of arbitrarily large documents\n",
    "\n",
    "2. **Improved Retrieval Precision** ([Chroma Research, 2024](https://research.trychroma.com/evaluating-chunking))\n",
    "   - Smaller, focused chunks create more accurate embeddings\n",
    "   - Reduces \"distractor effect\" from irrelevant content\n",
    "   - Improves ranking of relevant information\n",
    "\n",
    "3. **Context Preservation**\n",
    "   - Keeps semantically related information together\n",
    "   - Maintains the author's logical flow\n",
    "   - Preserves relationships between text, formulas, and figures\n",
    "\n",
    "4. **Computational Efficiency**\n",
    "   - Faster embedding generation (smaller chunks)\n",
    "   - Reduced API costs (fewer tokens per request)\n",
    "   - Better memory utilization\n",
    "\n",
    "**Chunk Overlap: A Critical Technique**\n",
    "\n",
    "For multi-modal documents, **chunk overlap** (10-20%) is often beneficial:\n",
    "\n",
    "- **Prevents splitting mid-concept**: Overlap ensures related information isn't fragmented\n",
    "- **Preserves context across boundaries**: Important for formulas, tables, and figures\n",
    "- **Improves retrieval recall**: Increases chances of capturing relevant information\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Chunk 1: \"...The HNSW algorithm uses parameter M to control connectivity. [OVERLAP START] Higher M values improve recall but increase latency. [OVERLAP END]\"\n",
    "\n",
    "Chunk 2: \"[OVERLAP START] Higher M values improve recall but increase latency. [OVERLAP END] Table 1 shows the trade-offs...\"\n",
    "```\n",
    "\n",
    "This ensures that if a query matches \"M parameter effects,\" both chunks can be retrieved."
   ],
   "id": "e5562d68caafb951"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "132c0b2f24f99ace"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Other Effective Chunking Strategies (Beyond the Basics)\n",
    "\n",
    "While we've covered the four main strategies, research and industry practice have identified several other effective approaches:\n",
    "\n",
    "**1. Page-Level Chunking**\n",
    "\n",
    "**Concept:** Use natural page boundaries as chunk boundaries.\n",
    "\n",
    "**Best for:** Structured documents like financial reports, legal filings, academic papers\n",
    "\n",
    "**Why it works:**\n",
    "- Page boundaries often align with coherent information units\n",
    "- Preserves visual layout (tables, figures stay with their page)\n",
    "- Proven effective for financial documents ([NVIDIA Research, 2024](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses/))\n",
    "\n",
    "**Trade-offs:**\n",
    "- Variable chunk sizes (pages vary in content density)\n",
    "- May not work for digital-first documents without page concept\n",
    "- Requires PDF or paginated source\n",
    "\n",
    "**Example use case:** 10-K financial reports where each page contains related financial data, tables, and explanations.\n",
    "\n",
    "**2. Structure-Aware (Markdown/HTML) Chunking**\n",
    "\n",
    "**Concept:** Split by hierarchical boundaries (headings, subheadings) while preserving document structure.\n",
    "\n",
    "**Best for:** Technical documentation, wikis, markdown-based content\n",
    "\n",
    "**Why it works:**\n",
    "- Maintains logical flow of information\n",
    "- Respects author's intended organization\n",
    "- Natural semantic boundaries\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Split by heading hierarchy\n",
    "def chunk_by_headers(markdown_text):\n",
    "    # Split on ## headers (sections)\n",
    "    # Keep ### subsections with their parent section\n",
    "    # Preserve code blocks, tables within sections\n",
    "```\n",
    "\n",
    "**3. Recursive Chunking**\n",
    "\n",
    "**Concept:** Split by progressively smaller separators until target size is reached.\n",
    "\n",
    "**Separator hierarchy:** `paragraphs â†’ sentences â†’ words â†’ characters`\n",
    "\n",
    "**Why it works:**\n",
    "- Preserves semantic coherence at multiple levels\n",
    "- Adapts to content structure\n",
    "- Avoids hard breaks mid-sentence\n",
    "\n",
    "**This is what LangChain's `RecursiveCharacterTextSplitter` does** (Strategy 2 above).\n",
    "\n",
    "**4. Semantic Chunking with Embeddings**\n",
    "\n",
    "**Concept:** Use embeddings to group semantically related sentences into coherent ideas.\n",
    "\n",
    "**Why it works:**\n",
    "- Detects topic boundaries even without structural markers\n",
    "- Works across structural boundaries (e.g., related content in different sections)\n",
    "- Adapts to content semantics, not just structure\n",
    "\n",
    "**This is what LangChain's `SemanticChunker` does** (Strategy 3 above).\n",
    "\n",
    "**5. Contextual Retrieval** ([Anthropic, 2024](https://www.anthropic.com/news/contextual-retrieval))\n",
    "\n",
    "**Concept:** Add LLM-generated context to each chunk before embedding.\n",
    "\n",
    "**How it works:**\n",
    "1. For each chunk, generate a brief context: \"This chunk is from Section 3.2 discussing HNSW parameters...\"\n",
    "2. Prepend context to chunk before embedding\n",
    "3. Embed: `[context] + [chunk]`\n",
    "\n",
    "**Results:** Reduces retrieval failures by 49-67% (Anthropic research)\n",
    "\n",
    "**Why it works:**\n",
    "- Chunks become self-contained (don't rely on surrounding context)\n",
    "- Improves embedding quality (more context = better representation)\n",
    "- Helps with cross-references and dependencies\n",
    "\n",
    "**6. Late Chunking** ([Jina AI, 2024](https://arxiv.org/abs/2409.04701))\n",
    "\n",
    "**Concept:** Embed the entire document first, then chunk the embeddings.\n",
    "\n",
    "**How it works:**\n",
    "1. Generate embeddings for entire document using long-context model\n",
    "2. Chunk the embedding space (not the text)\n",
    "3. Each chunk's embedding preserves full document context\n",
    "\n",
    "**Results:** Preserves cross-chunk context without ColBERT-level storage costs\n",
    "\n",
    "**Why it works:**\n",
    "- Embeddings capture full document context\n",
    "- Chunking happens in embedding space, preserving relationships\n",
    "- Best of both worlds: whole-document context + chunk-level retrieval\n",
    "\n",
    "**7. Agentic/Proposition-Based Chunking**\n",
    "\n",
    "**Concept:** Use LLMs to extract atomic propositions (facts) from text, then chunk by propositions.\n",
    "\n",
    "**How it works:**\n",
    "1. LLM extracts individual facts/claims from text\n",
    "2. Group related propositions into chunks\n",
    "3. Each chunk contains semantically related facts\n",
    "\n",
    "**Best for:** Dense academic text, legal documents, fact-heavy content\n",
    "\n",
    "**Trade-offs:** High computational cost (LLM calls for every document)"
   ],
   "id": "c1c97b3ecb80ab64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9245017126688c70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Legal Contracts and Clause-Level Documents\n",
    "\n",
    "**âš ï¸ Important:** Legal contracts represent a **fundamentally different challenge** that goes beyond simple chunking into the realm of **complex data engineering and knowledge modeling**.\n",
    "\n",
    "**Why Legal Documents Are Different:**\n",
    "\n",
    "1. **Clause-Level Granularity**\n",
    "   - Each clause may need to be a separate retrieval unit\n",
    "   - Clauses have specific legal meanings and purposes\n",
    "   - Example: \"Termination Clause,\" \"Indemnification Clause,\" \"Force Majeure\"\n",
    "\n",
    "2. **Cross-References and Dependencies**\n",
    "   - Clauses often reference other clauses: \"as defined in Section 3.2\"\n",
    "   - Understanding one clause may require reading referenced clauses\n",
    "   - Circular dependencies are common\n",
    "\n",
    "3. **Hierarchical Structure**\n",
    "   - Parent-child relationships: Sections â†’ Subsections â†’ Clauses â†’ Sub-clauses\n",
    "   - Inheritance of terms and conditions\n",
    "   - Nested definitions and exceptions\n",
    "\n",
    "4. **Legal Precedence and Overrides**\n",
    "   - Some clauses modify or override others\n",
    "   - \"Notwithstanding Section 2.1...\" creates dependency\n",
    "   - Amendment clauses change earlier provisions\n",
    "\n",
    "5. **Rich Metadata Requirements**\n",
    "   - Clause type (termination, payment, liability, etc.)\n",
    "   - Parties involved (which party has obligations)\n",
    "   - Effective dates and conditions\n",
    "   - Jurisdiction and governing law\n",
    "\n",
    "**What This Requires:**\n",
    "\n",
    "Simple chunking is **insufficient** for legal documents. You need:\n",
    "\n",
    "**1. Advanced Data Modeling**\n",
    "- **Knowledge graphs** to capture clause relationships\n",
    "- **Hierarchical structures** to preserve document organization\n",
    "- **Dependency tracking** for cross-references\n",
    "\n",
    "**Example knowledge graph:**\n",
    "```\n",
    "Clause 3.2 (Payment Terms)\n",
    "  â”œâ”€ references â†’ Clause 1.5 (Definitions: \"Net 30\")\n",
    "  â”œâ”€ modified_by â†’ Clause 8.1 (Amendment: \"Net 45 for Q4\")\n",
    "  â””â”€ depends_on â†’ Clause 2.1 (Delivery Conditions)\n",
    "```\n",
    "\n",
    "**2. Custom Chunking Logic**\n",
    "- Domain-specific rules for legal document structure\n",
    "- Clause boundary detection (not just paragraphs)\n",
    "- Preservation of numbering and hierarchy\n",
    "\n",
    "**3. Context Assembly Strategies**\n",
    "- **Recursive retrieval**: When retrieving Clause 3.2, also fetch referenced clauses\n",
    "- **Graph traversal**: Follow dependency links to build complete context\n",
    "- **Hierarchical expansion**: Include parent section context\n",
    "\n",
    "**Example Retrieval Flow:**\n",
    "```\n",
    "Query: \"What are the payment terms?\"\n",
    "\n",
    "1. Retrieve: Clause 3.2 (Payment Terms)\n",
    "2. Detect references: \"as defined in Section 1.5\"\n",
    "3. Fetch: Clause 1.5 (Definitions)\n",
    "4. Detect modifications: Clause 8.1 modifies 3.2\n",
    "5. Fetch: Clause 8.1 (Amendment)\n",
    "6. Assemble context: [3.2 + 1.5 + 8.1] with relationship metadata\n",
    "```\n",
    "\n",
    "**Research and Industry Approaches:**\n",
    "\n",
    "- **Multi-Graph Multi-Agent Systems** ([Medium, 2024](https://medium.com/enterprise-rag/legal-document-rag-multi-graph-multi-agent-recursive-retrieval-through-legal-clauses-c90e073e0052))\n",
    "  - Use multiple knowledge graphs to model different relationship types\n",
    "  - Agent-based recursive retrieval through clause dependencies\n",
    "\n",
    "- **GraphRAG for Legal Contracts** ([Neo4j, 2024](https://neo4j.com/blog/developer/agentic-graphrag-for-commercial-contracts/))\n",
    "  - Build knowledge graphs from contract structure\n",
    "  - Use graph queries to navigate clause relationships\n",
    "\n",
    "**ðŸ’¡ Discussion Question:**\n",
    "\n",
    "Do you agree that legal contracts represent a fundamentally different challenge that goes beyond chunking into the realm of complex data engineering and knowledge modeling?\n",
    "\n",
    "**Our Position:** Yes, absolutely. Legal documents require:\n",
    "- **Not just chunking**, but **relationship modeling**\n",
    "- **Not just retrieval**, but **dependency resolution**\n",
    "- **Not just embeddings**, but **structured knowledge graphs**\n",
    "\n",
    "This is an **advanced topic beyond the scope of this module**, but it's important to recognize when simple chunking strategies are insufficient.\n",
    "\n",
    "**Recommendation:** For legal document RAG systems:\n",
    "1. Start with clause-level chunking as a baseline\n",
    "2. Add metadata enrichment (clause type, parties, dates)\n",
    "3. Build knowledge graphs for relationships\n",
    "4. Implement recursive retrieval for dependencies\n",
    "5. Consider specialized legal NLP tools (e.g., LexNLP, Blackstone)\n",
    "\n",
    "This is a **research-level problem** that requires domain expertise in both legal document structure and advanced RAG techniques."
   ],
   "id": "7e805120e59e95d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b224e9547a53e10f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Transcripts and Meeting Notes: Temporal and Speaker-Based Chunking\n",
    "\n",
    "**Document Characteristics:**\n",
    "- Conversational flow with speaker changes\n",
    "- Temporal structure (time-based progression)\n",
    "- Topic shifts without clear structural markers\n",
    "- Interruptions, overlaps, and informal language\n",
    "- Context depends on who said what and when\n",
    "\n",
    "**Examples:**\n",
    "- Meeting transcripts (Zoom, Teams, Google Meet)\n",
    "- Podcast transcripts\n",
    "- Interview transcripts\n",
    "- Customer service call recordings\n",
    "- Presentation Q&A sessions\n",
    "\n",
    "**Why Standard Chunking Fails:**\n",
    "\n",
    "Traditional chunking strategies (fixed-size, semantic) don't account for:\n",
    "- **Speaker identity**: Who said what matters for context\n",
    "- **Temporal flow**: Conversations build on previous statements\n",
    "- **Turn-taking**: Natural conversation boundaries\n",
    "- **Topic drift**: Conversations meander without clear section breaks\n",
    "\n",
    "**Specialized Chunking Strategies for Transcripts:**\n",
    "\n",
    "**Strategy 1: Speaker-Based Chunking**\n",
    "\n",
    "**Concept:** Chunk by speaker turns, keeping each speaker's complete statement together.\n",
    "\n",
    "**Best for:** Interviews, Q&A sessions, debates\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "def chunk_by_speaker(transcript):\n",
    "    \"\"\"\n",
    "    Input: \"Speaker A: Hello, how are you? Speaker B: I'm good, thanks!\"\n",
    "    Output: [\n",
    "        \"Speaker A: Hello, how are you?\",\n",
    "        \"Speaker B: I'm good, thanks!\"\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # Split on speaker labels\n",
    "    # Keep speaker identity with each chunk\n",
    "    # Preserve turn-taking structure\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Preserves speaker context\n",
    "- Natural conversation boundaries\n",
    "- Easy to attribute statements\n",
    "\n",
    "**Cons:**\n",
    "- Variable chunk sizes (some speakers talk more)\n",
    "- May split related topics across speakers\n",
    "\n",
    "**Strategy 2: Time-Based Chunking**\n",
    "\n",
    "**Concept:** Chunk by time intervals (e.g., 2-minute segments).\n",
    "\n",
    "**Best for:** Long meetings, podcasts, lectures\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "def chunk_by_time(transcript_with_timestamps, interval_seconds=120):\n",
    "    \"\"\"\n",
    "    Input: [(0, \"Speaker A: ...\"), (45, \"Speaker B: ...\"), (130, \"Speaker A: ...\")]\n",
    "    Output: [\n",
    "        \"00:00-02:00: [Speaker A: ...] [Speaker B: ...]\",\n",
    "        \"02:00-04:00: [Speaker A: ...]\"\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # Group utterances by time windows\n",
    "    # Preserve timestamps for reference\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Consistent chunk sizes\n",
    "- Easy to reference (\"at 15:30 in the meeting\")\n",
    "- Works well for long recordings\n",
    "\n",
    "**Cons:**\n",
    "- May split mid-sentence or mid-topic\n",
    "- Doesn't respect conversation flow\n",
    "\n",
    "**Strategy 3: Topic-Based Chunking (Semantic + Speaker-Aware)**\n",
    "\n",
    "**Concept:** Detect topic shifts using embeddings + speaker changes.\n",
    "\n",
    "**Best for:** Multi-topic meetings, panel discussions\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "def chunk_by_topic_and_speaker(transcript):\n",
    "    \"\"\"\n",
    "    1. Compute embeddings for each utterance\n",
    "    2. Detect topic shifts (low similarity between consecutive utterances)\n",
    "    3. Also create boundaries at speaker changes\n",
    "    4. Merge short chunks (< min_size)\n",
    "    \"\"\"\n",
    "    # Hybrid approach: semantic + speaker-aware\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Respects both topic and speaker boundaries\n",
    "- Most semantically coherent chunks\n",
    "- Adapts to conversation structure\n",
    "\n",
    "**Cons:**\n",
    "- Computationally expensive (embeddings for each utterance)\n",
    "- Requires speaker diarization\n",
    "\n",
    "**Strategy 4: Silence-Aware Merging**\n",
    "\n",
    "**Concept:** Use silence/pauses in audio to detect natural boundaries.\n",
    "\n",
    "**Best for:** Presentations, lectures with clear pauses\n",
    "\n",
    "**How it works:**\n",
    "- Audio processing detects pauses (e.g., >2 seconds of silence)\n",
    "- Merge utterances between pauses into chunks\n",
    "- Preserves natural speaking rhythm\n",
    "\n",
    "**Source:** VoxRAG (arXiv:2505.17326, 2025) - transcription-free RAG using silence-aware chunking\n",
    "\n",
    "**Pros:**\n",
    "- Natural conversation boundaries\n",
    "- Works without speaker diarization\n",
    "- Respects speaker's pacing\n",
    "\n",
    "**Cons:**\n",
    "- Requires audio access (not just transcript)\n",
    "- Pauses don't always align with topic shifts\n",
    "\n",
    "**Advanced Technique: Speaker Diarization + Chunking**\n",
    "\n",
    "**What is Speaker Diarization?**\n",
    "- Automatic detection of \"who spoke when\"\n",
    "- Assigns speaker labels (Speaker 1, Speaker 2, etc.)\n",
    "- Essential for multi-speaker transcripts\n",
    "\n",
    "**How to combine with chunking:**\n",
    "1. **Diarize audio** â†’ identify speakers\n",
    "2. **Transcribe with speaker labels** â†’ \"Speaker A: Hello...\"\n",
    "3. **Chunk by speaker + topic** â†’ preserve context\n",
    "\n",
    "**Tools:**\n",
    "- **Parakeet/Whisper**: Fast transcription with speaker labels\n",
    "- **pyannote.audio**: State-of-the-art speaker diarization\n",
    "- **AssemblyAI**: Commercial API with built-in diarization\n",
    "\n",
    "**Example Use Case: Meeting Minutes RAG**\n",
    "\n",
    "**Scenario:** Build a RAG system for company meeting transcripts.\n",
    "\n",
    "**Requirements:**\n",
    "- Answer questions like \"What did Sarah say about the budget?\"\n",
    "- Retrieve action items assigned to specific people\n",
    "- Find discussions about specific topics\n",
    "\n",
    "**Recommended Approach:**\n",
    "1. **Transcribe with speaker diarization** (Whisper + pyannote)\n",
    "2. **Chunk by speaker + topic** (hybrid approach)\n",
    "3. **Enrich with metadata**:\n",
    "   - Speaker name\n",
    "   - Timestamp\n",
    "   - Meeting title/date\n",
    "   - Detected topics (using LLM)\n",
    "4. **Store chunks with metadata** in vector DB\n",
    "5. **Implement metadata filtering** (e.g., \"speaker:Sarah AND topic:budget\")\n",
    "\n",
    "**Metadata Enrichment Example:**\n",
    "```python\n",
    "chunk = {\n",
    "    \"text\": \"Sarah: I think we should increase the marketing budget by 20%...\",\n",
    "    \"metadata\": {\n",
    "        \"speaker\": \"Sarah Johnson\",\n",
    "        \"timestamp\": \"00:15:30\",\n",
    "        \"meeting\": \"Q4 Planning - 2025-04-15\",\n",
    "        \"topic\": \"budget\",\n",
    "        \"action_item\": False\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**ðŸ’¡ Key Insight:** Transcripts require **context-aware chunking** that preserves:\n",
    "- **Who** said it (speaker identity)\n",
    "- **When** they said it (temporal context)\n",
    "- **What** they were discussing (topic/semantic content)\n",
    "\n",
    "Simple text chunking loses this critical context.\n",
    "\n",
    "**Recommendation:** For transcript RAG systems:\n",
    "1. Start with speaker-based chunking as baseline\n",
    "2. Add speaker diarization for multi-speaker content\n",
    "3. Enrich chunks with metadata (speaker, time, topic)\n",
    "4. Consider hybrid chunking (speaker + semantic) for complex conversations\n",
    "5. Use metadata filtering to improve retrieval precision\n",
    "\n",
    "This is an **emerging area** with active research (VoxRAG, 2025) exploring transcription-free approaches."
   ],
   "id": "f7fddfa635761eb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ff972f09018f8a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Implementation Guidance: Choosing Your Chunking Strategy\n",
    "\n",
    "The optimal chunking strategy requires **experimentation** based on your specific context. There's no universal \"best practice.\"\n",
    "\n",
    "**Factors to Consider:**\n",
    "\n",
    "1. **Specific Use Case and Query Patterns**\n",
    "   - What questions will users ask?\n",
    "   - Do they need summaries or specific facts?\n",
    "   - Are queries broad or narrow?\n",
    "\n",
    "2. **Document Type and Structure**\n",
    "   - Structured (sections, headers) or unstructured (prose)?\n",
    "   - Multi-modal (text + images + tables) or text-only?\n",
    "   - Single-topic or multi-topic documents?\n",
    "\n",
    "3. **Models Being Used**\n",
    "   - **Embedding model**: Context window size, quality on long vs. short text\n",
    "   - **LLM**: Context window, attention patterns, cost per token\n",
    "   - **Multimodal capabilities**: Can it handle images, tables?\n",
    "\n",
    "4. **Performance Metrics**\n",
    "   - **Retrieval precision**: Are you getting the right chunks?\n",
    "   - **Answer quality**: Are LLM responses accurate and complete?\n",
    "   - **Latency**: How fast do you need results?\n",
    "   - **Cost**: Token usage, API calls, storage\n",
    "\n",
    "**Experimentation Framework:**\n",
    "\n",
    "```python\n",
    "# 1. Define your test queries\n",
    "test_queries = [\n",
    "    \"What are the payment terms?\",\n",
    "    \"Explain the HNSW algorithm\",\n",
    "    \"What compression techniques were used?\"\n",
    "]\n",
    "\n",
    "# 2. Try multiple chunking strategies\n",
    "strategies = [\n",
    "    (\"no_chunking\", whole_document),\n",
    "    (\"document_based\", chunk_by_structure),\n",
    "    (\"fixed_size_512\", fixed_chunking_512),\n",
    "    (\"fixed_size_1024\", fixed_chunking_1024),\n",
    "    (\"semantic\", semantic_chunking),\n",
    "]\n",
    "\n",
    "# 3. Measure performance\n",
    "for strategy_name, chunking_fn in strategies:\n",
    "    chunks = chunking_fn(documents)\n",
    "\n",
    "    # Measure retrieval precision\n",
    "    precision = evaluate_retrieval(chunks, test_queries)\n",
    "\n",
    "    # Measure answer quality (LLM evaluation)\n",
    "    quality = evaluate_answers(chunks, test_queries, llm)\n",
    "\n",
    "    # Measure efficiency\n",
    "    avg_chunk_size = np.mean([len(c) for c in chunks])\n",
    "    total_tokens = sum([count_tokens(c) for c in chunks])\n",
    "\n",
    "    print(f\"{strategy_name}: Precision={precision:.2f}, Quality={quality:.2f}, Tokens={total_tokens}\")\n",
    "```\n",
    "\n",
    "**Key Metrics to Track:**\n",
    "\n",
    "| Metric | What It Measures | How to Evaluate |\n",
    "|--------|------------------|-----------------|\n",
    "| **Retrieval Precision** | Are the right chunks retrieved? | Manual review or automated eval (LLM-as-judge) |\n",
    "| **Answer Quality** | Are LLM responses accurate? | Human evaluation or LLM-based scoring |\n",
    "| **Token Efficiency** | How many tokens per query? | Count tokens in retrieved chunks |\n",
    "| **Latency** | How fast is retrieval? | Measure end-to-end query time |\n",
    "| **Coverage** | Do chunks cover all important info? | Check if key facts are retrievable |\n",
    "\n",
    "**Iterative Improvement Process:**\n",
    "\n",
    "1. **Start simple**: Begin with fixed-size or document-based chunking\n",
    "2. **Measure baseline**: Establish performance metrics\n",
    "3. **Identify failures**: Where does retrieval fail? Where are answers wrong?\n",
    "4. **Hypothesize improvements**: \"Tables are being split\" â†’ Try structure-aware chunking\n",
    "5. **Test and compare**: Measure impact of changes\n",
    "6. **Iterate**: Repeat until performance is acceptable\n",
    "\n",
    "**Common Failure Patterns and Solutions:**\n",
    "\n",
    "| Problem | Likely Cause | Solution |\n",
    "|---------|--------------|----------|\n",
    "| Tables split across chunks | Fixed-size chunking | Use structure-aware chunking |\n",
    "| Formulas without context | Naive chunking | Keep formulas with explanations |\n",
    "| Missing cross-references | Single-chunk retrieval | Implement recursive retrieval |\n",
    "| Generic answers | Chunks too large | Reduce chunk size or use semantic chunking |\n",
    "| Incomplete answers | Chunks too small | Increase chunk size or add overlap |\n",
    "\n",
    "**ðŸ’¡ Final Insight:**\n",
    "\n",
    "Chunking is **data modeling for retrieval**. Just like database schema design, there's no one-size-fits-all solution. The \"best\" chunking strategy is the one that:\n",
    "- Matches your document structure\n",
    "- Aligns with your query patterns\n",
    "- Meets your performance requirements\n",
    "- Balances complexity with maintainability\n",
    "\n",
    "**Experiment, measure, iterate.**"
   ],
   "id": "bf1ee61a2c702ebd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1239169f5ffbbee9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy 4: Hierarchical Chunking (Multi-Level)\n",
    "\n",
    "**Concept:** Create multiple levels of chunks - large chunks for overview, small chunks for details.\n",
    "\n",
    "**Best for:** Very large documents where users need both high-level summaries and specific details."
   ],
   "id": "af4a3f5fe1886cd5"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7299e6c8f02028dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:28.854802Z",
     "iopub.status.busy": "2025-11-05T13:43:28.854432Z",
     "iopub.status.idle": "2025-11-05T13:43:28.858931Z",
     "shell.execute_reply": "2025-11-05T13:43:28.858526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Strategy 4: Hierarchical (Multi-Level) Chunking\n",
      "================================================================================\n",
      "Original document: 1,035 tokens\n",
      "\n",
      "Level 1 (Sections): 7 chunks\n",
      "\n",
      "   L1-1: 8 tokens - # Optimizing Vector Search Performance in Redis...\n",
      "   L1-2: 108 tokens - ## Abstract This paper presents a comprehensive analysis of vector search optimi...\n",
      "   L1-3: 98 tokens - ## 1. Introduction Vector databases have become essential infrastructure for mod...\n",
      "   L1-4: 98 tokens - ## 2. Background and Related Work Previous work on vector search optimization ha...\n",
      "   L1-5: 464 tokens - ## 3. Performance Analysis and Results  ### 3.1 HNSW Configuration Trade-offs  T...\n",
      "   L1-6: 187 tokens - ## 4. Implementation Recommendations  Based on our findings, we recommend the fo...\n",
      "   L1-7: 73 tokens - ## 5. Discussion and Conclusion Our findings demonstrate that vector search opti...\n",
      "\n",
      "Level 2 (Subsections): 13 chunks\n",
      "\n",
      "   L2-1: 8 tokens - # Optimizing Vector Search Performance in Redis...\n",
      "   L2-2: 108 tokens - ## Abstract This paper presents a comprehensive analysis of vector search optimi...\n",
      "   L2-3: 98 tokens - ## 1. Introduction Vector databases have become essential infrastructure for mod...\n",
      "   L2-4: 98 tokens - ## 2. Background and Related Work Previous work on vector search optimization ha...\n",
      "   L2-5: 107 tokens - Table 1 shows the performance comparison across different HNSW configurations. A...\n",
      "... (8 more L2 chunks)\n"
     ]
    }
   ],
   "source": [
    "# Strategy 4: Hierarchical Chunking\n",
    "\n",
    "\n",
    "def chunk_hierarchically(text: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Create multiple levels of chunks.\n",
    "    Level 1: Large sections (by ## headers)\n",
    "    Level 2: Subsections (by paragraphs within sections)\n",
    "    \"\"\"\n",
    "\n",
    "    # Level 1: Split by major sections\n",
    "    level1_chunks = chunk_by_structure(text, separator=\"\\n## \")\n",
    "\n",
    "    # Level 2: Further split large sections into paragraphs\n",
    "    level2_chunks = []\n",
    "    for section in level1_chunks:\n",
    "        # If section is large, split into paragraphs\n",
    "        if count_tokens(section) > 400:\n",
    "            paragraphs = [\n",
    "                p.strip() for p in section.split(\"\\n\\n\") if p.strip() and len(p) > 50\n",
    "            ]\n",
    "            level2_chunks.extend(paragraphs)\n",
    "        else:\n",
    "            level2_chunks.append(section)\n",
    "\n",
    "    return {\n",
    "        \"level1\": level1_chunks,  # Large sections\n",
    "        \"level2\": level2_chunks,  # Smaller subsections\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply to research paper\n",
    "hierarchical_chunks = chunk_hierarchically(research_paper)\n",
    "\n",
    "print(\n",
    "    f\"\"\"ðŸ“Š Strategy 4: Hierarchical (Multi-Level) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "\n",
    "Level 1 (Sections): {len(hierarchical_chunks['level1'])} chunks\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(hierarchical_chunks[\"level1\"]):\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:80].replace(\"\\n\", \" \")\n",
    "    print(f\"   L1-{i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Level 2 (Subsections): {len(hierarchical_chunks['level2'])} chunks\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(hierarchical_chunks[\"level2\"][:5]):  # Show first 5\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:80].replace(\"\\n\", \" \")\n",
    "    print(f\"   L2-{i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "print(f\"... ({len(hierarchical_chunks['level2']) - 5} more L2 chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf78622dce4de75",
   "metadata": {},
   "source": [
    "**Strategy 4 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- Supports both overview and detailed queries\n",
    "- Flexible retrieval (can search at different levels)\n",
    "- Preserves document hierarchy\n",
    "- Better for complex documents\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- More complex to implement and maintain\n",
    "- Requires more storage (multiple levels)\n",
    "- Need strategy to choose which level to search\n",
    "- Higher indexing cost\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Very large documents (textbooks, manuals)\n",
    "- When users need both summaries and details\n",
    "- Technical documentation with nested structure\n",
    "- Legal contracts with sections and subsections\n",
    "\n",
    "ðŸ’¡ **Retrieval Strategy:**\n",
    "- Start with Level 1 for overview\n",
    "- If user needs more detail, retrieve Level 2 chunks\n",
    "- Can combine: \"Show section summary + relevant details\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811e94608a8eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cc19c72b00b7f36",
   "metadata": {},
   "source": [
    "### Comparing Chunking Strategies: Decision Framework\n",
    "\n",
    "Now let's compare all four strategies side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33137223a8d1c166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:28.860140Z",
     "iopub.status.busy": "2025-11-05T13:43:28.860075Z",
     "iopub.status.idle": "2025-11-05T13:43:28.864102Z",
     "shell.execute_reply": "2025-11-05T13:43:28.863676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHUNKING STRATEGY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Document: Research Paper (1,035 tokens)\n",
      "\n",
      "Strategy              | Chunks | Avg Size | Complexity | Best For\n",
      "--------------------- | ------ | -------- | ---------- | --------\n",
      "Document-Based        |      7 |      148 | Low        | Structured docs\n",
      "Fixed-Size            |      8 |      140 | Low        | Unstructured text\n",
      "Semantic              |     25 |       41 | High       | Dense academic text\n",
      "Hierarchical (L1)     |      7 |      148 | Medium     | Large complex docs\n",
      "Hierarchical (L2)     |     13 |       76 | Medium     | Large complex docs\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "{'=' * 80}\n",
    "CHUNKING STRATEGY COMPARISON\n",
    "{'=' * 80}\n",
    "\n",
    "Document: Research Paper ({paper_tokens:,} tokens)\n",
    "\n",
    "Strategy              | Chunks | Avg Size | Complexity | Best For\n",
    "--------------------- | ------ | -------- | ---------- | --------\n",
    "Document-Based        | {len(structure_chunks):>6} | {sum(count_tokens(c) for c in structure_chunks) // len(structure_chunks):>8} | Low        | Structured docs\n",
    "Fixed-Size            | {len(fixed_chunks):>6} | {sum(count_tokens(c) for c in fixed_chunks) // len(fixed_chunks):>8} | Low        | Unstructured text\n",
    "Semantic              | {len(semantic_chunks):>6} | {sum(count_tokens(c) for c in semantic_chunks) // len(semantic_chunks):>8} | High       | Dense academic text\n",
    "Hierarchical (L1)     | {len(hierarchical_chunks['level1']):>6} | {sum(count_tokens(c) for c in hierarchical_chunks['level1']) // len(hierarchical_chunks['level1']):>8} | Medium     | Large complex docs\n",
    "Hierarchical (L2)     | {len(hierarchical_chunks['level2']):>6} | {sum(count_tokens(c) for c in hierarchical_chunks['level2']) // len(hierarchical_chunks['level2']):>8} | Medium     | Large complex docs\n",
    "\n",
    "{'=' * 80}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1367dee484df00",
   "metadata": {},
   "source": [
    "### YOUR Chunking Decision Framework\n",
    "\n",
    "Chunking strategy is a **design choice** that depends on your specific context. There's no universal \"correct\" chunk size.\n",
    "\n",
    "**Step 1: Start with Document Type**\n",
    "\n",
    "| Document Type | Default Approach | Reasoning |\n",
    "|---------------|------------------|----------|\n",
    "| **Structured records** (courses, products, FAQs) | Don't chunk | Natural boundaries already exist |\n",
    "| **Long-form text** (papers, books, docs) | Consider chunking | May need retrieval precision |\n",
    "| **PDFs with visual layout** | Page-level | Preserves tables, figures |\n",
    "| **Code** | Function/class boundaries | Semantic structure matters |\n",
    "\n",
    "**Step 2: Evaluate These Factors**\n",
    "\n",
    "1. **Semantic completeness:** Is each item self-contained?\n",
    "   - âœ… Yes â†’ Don't chunk (preserve natural boundaries)\n",
    "   - âŒ No â†’ Consider chunking strategy\n",
    "\n",
    "2. **Query patterns:** What will users ask?\n",
    "   - Specific facts â†’ Smaller, focused chunks help\n",
    "   - Summaries/overviews â†’ Larger chunks or hierarchical\n",
    "   - Mixed â†’ Consider hierarchical approach\n",
    "\n",
    "3. **Topic density:** How many distinct topics per document?\n",
    "   - Single topic â†’ Whole-document embedding often works\n",
    "   - Multiple distinct topics â†’ Chunking may improve precision\n",
    "\n",
    "4. **Embedding model:** What's your model's context window?\n",
    "   - Modern models (jina-v2, etc.) support 8K+ tokens\n",
    "   - Model limits are often less constraining than retrieval precision needs\n",
    "\n",
    "**Step 3: Consider Constraints**\n",
    "\n",
    "- **Latency:** Simple approaches (fixed-size, no chunking) are faster\n",
    "- **Quality:** Semantic chunking adds cost but may improve relevance\n",
    "- **Complexity:** More sophisticated â‰  better for your use case\n",
    "\n",
    "**Example Decisions:**\n",
    "\n",
    "| Domain | Data Characteristics | Decision | Why |\n",
    "|--------|---------------------|----------|-----|\n",
    "| **Course Catalog** | Small, self-contained records | **Don't chunk** | Each course is a complete retrieval unit |\n",
    "| **Research Papers** | Multi-section, dense topics | Document-Based | Sections are natural semantic units |\n",
    "| **Support Tickets** | Single issue per ticket | **Don't chunk** | Already at optimal granularity |\n",
    "| **Legal Contracts** | Nested structure, many clauses | Hierarchical | Need both overview and clause-level detail |\n",
    "| **Product Docs** | Mixed structure, varied content | Fixed-Size or Semantic | Balance simplicity with quality |\n",
    "\n",
    "> ðŸ’¡ **Key Takeaway:** Ask \"What is my natural retrieval unit?\" before deciding on a chunking strategy. For many structured data use cases, the answer is \"don't chunk.\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Summary: Chunking is Subjective, Document-Dependent, and Requires Experimentation\n",
    "\n",
    "**What We've Learned:**\n",
    "\n",
    "Chunking is **not a one-size-fits-all solution**. The optimal strategy depends entirely on:\n",
    "\n",
    "1. **Document Type and Structure**\n",
    "   - Research papers â†’ Structure-aware or hybrid (summary + sections)\n",
    "   - Legal contracts â†’ Knowledge graphs + recursive retrieval (beyond simple chunking)\n",
    "   - Transcripts â†’ Speaker-aware + temporal chunking\n",
    "   - Structured records â†’ Often don't chunk at all\n",
    "\n",
    "2. **Content Heterogeneity**\n",
    "   - Text-only â†’ Standard chunking strategies work\n",
    "   - Multimodal (tables, figures, equations, code) â†’ Specialized handling required\n",
    "   - Keep content WITH its context (table + caption, formula + definitions)\n",
    "\n",
    "3. **Query Patterns**\n",
    "   - Overview queries â†’ Larger chunks or hierarchical (summary level)\n",
    "   - Specific facts â†’ Smaller, focused chunks\n",
    "   - Mixed queries â†’ Hybrid approach (multiple retrieval levels)\n",
    "\n",
    "4. **Models Being Used**\n",
    "   - Embedding model capabilities (context window, quality on long vs. short text)\n",
    "   - LLM context window and attention patterns\n",
    "   - Multimodal capabilities (vision models for figures)\n",
    "\n",
    "5. **Performance Requirements**\n",
    "   - Retrieval precision (are you getting the right chunks?)\n",
    "   - Answer quality (are LLM responses accurate?)\n",
    "   - Latency (how fast do you need results?)\n",
    "   - Cost (token usage, API calls, storage)\n",
    "\n",
    "**Key Insights from Research:**\n",
    "\n",
    "- **49-67% improvement** in retrieval with contextual retrieval (Anthropic, 2024)\n",
    "- **Chunk overlap (10-20%)** preserves context across boundaries\n",
    "- **Lost in the Middle** and **Context Rot** show that structure matters more than fitting everything in\n",
    "- **Semantic chunking doesn't always outperform fixed-size** - experimentation is required\n",
    "\n",
    "**Three Document Types, Three Different Approaches:**\n",
    "\n",
    "| Document Type | Key Challenge | Recommended Approach |\n",
    "|---------------|---------------|----------------------|\n",
    "| **Research Papers & Long-Form** | Multimodal content (text, tables, figures, equations) | Hybrid: summary + section-based + multimodal handling |\n",
    "| **Legal Contracts** | Cross-references, hierarchical dependencies, legal precedence | Knowledge graphs + recursive retrieval (advanced topic) |\n",
    "| **Transcripts & Meetings** | Speaker identity, temporal flow, topic drift | Speaker-aware + time-based + metadata enrichment |\n",
    "\n",
    "**The Expert's Approach:**\n",
    "\n",
    "1. **Analyze your documents** - What content types? What structure?\n",
    "2. **Understand your queries** - What will users ask? How specific?\n",
    "3. **Start simple** - Fixed-size or structure-aware chunking as baseline\n",
    "4. **Measure performance** - Retrieval precision, answer quality, latency\n",
    "5. **Identify failures** - Where does retrieval fail? Where are answers wrong?\n",
    "6. **Iterate and improve** - Test alternatives, measure impact, refine\n",
    "7. **Consider alternatives** - Sometimes chunking isn't the answer (hierarchical retrieval, knowledge graphs)\n",
    "\n",
    "**Critical Questions to Ask:**\n",
    "\n",
    "- **Is chunking even necessary?** (For structured records, often NO)\n",
    "- **What is my natural retrieval unit?** (Course, section, clause, speaker turn?)\n",
    "- **What content types need special handling?** (Tables, formulas, figures, code?)\n",
    "- **What are my query patterns?** (Overview vs. specific facts?)\n",
    "- **What are my performance constraints?** (Latency, cost, quality trade-offs?)\n",
    "\n",
    "**Final Insight:**\n",
    "\n",
    "Chunking is **data modeling for retrieval**. Just like database schema design, there's no universal solution. The \"best\" chunking strategy is the one that:\n",
    "- Matches your document structure\n",
    "- Aligns with your query patterns\n",
    "- Meets your performance requirements\n",
    "- Balances complexity with maintainability\n",
    "\n",
    "**Experiment, measure, iterate.** This is engineering, not magic.\n"
   ],
   "id": "d8c55e7b505ca453"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 5: Building Practical Context Pipelines\n",
    "\n",
    "Now that you understand data transformation and chunking, let's discuss how to build reusable pipelines.\n",
    "\n",
    "### Three Pipeline Architectures\n",
    "\n",
    "There are three main approaches to context preparation in real-world applications:\n",
    "\n",
    "### Architecture 1: Request-Time Processing\n",
    "\n",
    "**Concept:** Transform data on-the-fly when a query arrives.\n",
    "\n",
    "```\n",
    "User Query â†’ Retrieve Raw Data â†’ Transform â†’ Chunk (if needed) â†’ Embed â†’ Search â†’ Return Context\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Always up-to-date (no stale data)\n",
    "- âœ… No pre-processing required\n",
    "- âœ… Simple to implement\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Higher latency (processing happens during request)\n",
    "- âŒ Repeated work (same transformations for every query)\n",
    "- âŒ Not suitable for large datasets\n",
    "\n",
    "**Best for:**\n",
    "- Small datasets (< 1,000 documents)\n",
    "- Frequently changing data\n",
    "- Simple transformations"
   ],
   "id": "374f309f47646cce"
  },
  {
   "cell_type": "markdown",
   "id": "785624fc38e46d77",
   "metadata": {},
   "source": [
    "### Architecture 2: Batch Processing\n",
    "\n",
    "**Concept:** Pre-process all data in batches (nightly, weekly) and store results.\n",
    "\n",
    "```\n",
    "[Scheduled Job]\n",
    "Raw Data â†’ Extract â†’ Clean â†’ Transform â†’ Chunk â†’ Embed â†’ Store in Vector DB\n",
    "\n",
    "[Query Time]\n",
    "User Query â†’ Search Vector DB â†’ Return Pre-Processed Context\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Fast query time (all processing done ahead)\n",
    "- âœ… Efficient (process once, use many times)\n",
    "- âœ… Can use expensive transformations (LLM-based chunking, semantic analysis)\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Data can be stale (until next batch run)\n",
    "- âŒ Requires scheduling infrastructure\n",
    "- âŒ Higher storage costs (store processed data)\n",
    "\n",
    "**Best for:**\n",
    "- Large datasets (> 10,000 documents)\n",
    "- Infrequently changing data (daily/weekly updates)\n",
    "- Complex transformations (semantic chunking, LLM summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d956da9e767913",
   "metadata": {},
   "source": [
    "### Architecture 3: Event-Driven Processing\n",
    "\n",
    "**Concept:** Process data as it changes (real-time updates).\n",
    "\n",
    "```\n",
    "Data Change Event â†’ Trigger Pipeline â†’ Extract â†’ Clean â†’ Transform â†’ Chunk â†’ Embed â†’ Update Vector DB\n",
    "\n",
    "[Query Time]\n",
    "User Query â†’ Search Vector DB â†’ Return Context\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Always up-to-date (real-time)\n",
    "- âœ… Fast query time (pre-processed)\n",
    "- âœ… Efficient (only process changed data)\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Complex infrastructure (event streams, queues)\n",
    "- âŒ Requires change detection\n",
    "- âŒ Higher operational complexity\n",
    "\n",
    "**Best for:**\n",
    "- Real-time data (news, social media, live updates)\n",
    "- Large datasets that change frequently\n",
    "- When both freshness and speed are critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8677cf43366989a",
   "metadata": {},
   "source": [
    "### Choosing YOUR Pipeline Architecture\n",
    "\n",
    "Use this decision tree:\n",
    "\n",
    "**Question 1: How often does your data change?**\n",
    "- Real-time (seconds/minutes) â†’ Event-Driven\n",
    "- Frequently (hourly/daily) â†’ Batch or Event-Driven\n",
    "- Infrequently (weekly/monthly) â†’ Batch\n",
    "- Rarely (manual updates) â†’ Request-Time or Batch\n",
    "\n",
    "**Question 2: How large is your dataset?**\n",
    "- Small (< 1,000 docs) â†’ Request-Time\n",
    "- Medium (1,000-100,000 docs) â†’ Batch\n",
    "- Large (> 100,000 docs) â†’ Batch or Event-Driven\n",
    "\n",
    "**Question 3: What are your latency requirements?**\n",
    "- Real-time (< 100ms) â†’ Batch or Event-Driven (pre-processed)\n",
    "- Interactive (< 1s) â†’ Any approach\n",
    "- Batch queries â†’ Request-Time acceptable\n",
    "\n",
    "**Question 4: How complex are your transformations?**\n",
    "- Simple (cleaning, formatting) â†’ Any approach\n",
    "- Moderate (chunking, basic NLP) â†’ Batch or Event-Driven\n",
    "- Complex (LLM-based, semantic analysis) â†’ Batch (pre-compute)\n",
    "\n",
    "**Example Decision:**\n",
    "\n",
    "For Redis University:\n",
    "- âœ… **Data changes:** Weekly (new courses added)\n",
    "- âœ… **Dataset size:** 100-500 courses (medium)\n",
    "- âœ… **Latency:** Interactive (< 1s acceptable)\n",
    "- âœ… **Transformations:** Moderate (structured views + embeddings)\n",
    "- âœ… **Decision:** **Batch Processing** (weekly job to rebuild catalog + embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4003fc42ee8ddc",
   "metadata": {},
   "source": [
    "### Example: Batch Processing Pipeline for Redis University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68dc14ed5084daaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:28.865267Z",
     "iopub.status.busy": "2025-11-05T13:43:28.865199Z",
     "iopub.status.idle": "2025-11-05T13:43:29.552062Z",
     "shell.execute_reply": "2025-11-05T13:43:29.551626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BATCH PROCESSING PIPELINE - Redis University Courses\n",
      "================================================================================\n",
      "\n",
      "[Step 1/5] Extracting course data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:43:29 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Extracted 50 courses\n",
      "\n",
      "   ðŸ“„ Sample raw course:\n",
      "      CS004: Database Systems\n",
      "      Department: Computer Science, Credits: 3, Level: intermediate\n",
      "\n",
      "[Step 2/5] Cleaning data...\n",
      "   âœ… Cleaned: 20 courses (removed 30 test courses)\n",
      "\n",
      "   ðŸ“„ Example removed course:\n",
      "      ðŸ—‘ï¸  BUS033: Marketing Strategy (filtered out)\n",
      "\n",
      "[Step 3/5] Transforming to LLM-friendly format...\n",
      "   âœ… Transformed: 20 courses (1,087 total tokens)\n",
      "\n",
      "   ðŸ“„ Transformation example:\n",
      "      Before: CS004 (Course object)\n",
      "      After (LLM-friendly text):\n",
      "      CS004: Database Systems\n",
      "      Department: Computer Science\n",
      "      Credits: 3\n",
      "      Level: intermediate\n",
      "      Format: online\n",
      "      Instructor: John Zamora\n",
      "      Description: Design and implementation of database systems. SQL, normalization, transaction...\n",
      "\n",
      "[Step 4/5] Creating structured catalog view...\n",
      "   âœ… Created catalog view (585 tokens)\n",
      "   âœ… Cached in Redis\n",
      "\n",
      "   ðŸ“„ Catalog view structure:\n",
      "      # Redis University Course Catalog\n",
      "      \n",
      "      ## Business (10 courses)\n",
      "      - BUS033: Marketing Strategy (intermediate)\n",
      "      - BUS035: Marketing Strategy (intermediate)\n",
      "      - BUS032: Marketing Strategy (intermediate)\n",
      "      - BUS034: Marketing Strategy (intermediate)\n",
      "      - BUS037: Marketing Strategy (intermediate)\n",
      "      - BUS039: Marketing ...\n",
      "\n",
      "[Step 5/5] Storing processed data...\n",
      "   âœ… Stored 20 processed courses in Redis\n",
      "\n",
      "   ðŸ“„ Storage example:\n",
      "      Key: course:processed:CS004\n",
      "      Value: CS004: Database Systems\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: intermediate\n",
      "Format: online\n",
      "I...\n",
      "\n",
      "================================================================================\n",
      "BATCH PROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Summary:\n",
      "- Courses processed: 20\n",
      "- Total tokens: 1,087\n",
      "- Catalog view tokens: 585\n",
      "- Storage: Redis\n",
      "- Next run: 2024-10-07 (weekly)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Batch Processing Pipeline\n",
    "# This would run as a scheduled job (e.g., weekly)\n",
    "\n",
    "\n",
    "async def batch_process_courses():\n",
    "    \"\"\"\n",
    "    Batch processing pipeline for Redis University courses.\n",
    "    Runs weekly to update catalog and embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"BATCH PROCESSING PIPELINE - Redis University Courses\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: Extract\n",
    "    print(\"\\n[Step 1/5] Extracting course data...\")\n",
    "    all_courses = await course_manager.get_all_courses()\n",
    "    print(f\"   âœ… Extracted {len(all_courses)} courses\")\n",
    "\n",
    "    # Show sample raw data\n",
    "    if all_courses:\n",
    "        sample = all_courses[0]\n",
    "        print(f\"\\n   ðŸ“„ Sample raw course:\")\n",
    "        print(f\"      {sample.course_code}: {sample.title}\")\n",
    "        print(f\"      Department: {sample.department}, Credits: {sample.credits}, Level: {sample.difficulty_level.value}\")\n",
    "\n",
    "    # Step 2: Clean\n",
    "    print(\"\\n[Step 2/5] Cleaning data...\")\n",
    "    # Remove test courses, validate fields, etc.\n",
    "    cleaned_courses = [\n",
    "        c for c in all_courses if c.course_code.startswith((\"RU\", \"CS\", \"MATH\"))\n",
    "    ]\n",
    "    print(\n",
    "        f\"   âœ… Cleaned: {len(cleaned_courses)} courses (removed {len(all_courses) - len(cleaned_courses)} test courses)\"\n",
    "    )\n",
    "\n",
    "    # Show what was filtered out\n",
    "    removed_courses = [c for c in all_courses if not c.course_code.startswith((\"RU\", \"CS\", \"MATH\"))]\n",
    "    if removed_courses:\n",
    "        print(f\"\\n   ðŸ“„ Example removed course:\")\n",
    "        print(f\"      ðŸ—‘ï¸  {removed_courses[0].course_code}: {removed_courses[0].title} (filtered out)\")\n",
    "\n",
    "    # Step 3: Transform\n",
    "    print(\"\\n[Step 3/5] Transforming to LLM-friendly format...\")\n",
    "    transformed_courses = [transform_course_to_text(c) for c in cleaned_courses]\n",
    "    total_tokens = sum(count_tokens(t) for t in transformed_courses)\n",
    "    print(\n",
    "        f\"   âœ… Transformed: {len(transformed_courses)} courses ({total_tokens:,} total tokens)\"\n",
    "    )\n",
    "\n",
    "    # Show before/after transformation\n",
    "    if cleaned_courses and transformed_courses:\n",
    "        print(f\"\\n   ðŸ“„ Transformation example:\")\n",
    "        print(f\"      Before: {cleaned_courses[0].course_code} (Course object)\")\n",
    "        print(f\"      After (LLM-friendly text):\")\n",
    "        preview = transformed_courses[0].replace('\\n', '\\n      ')\n",
    "        print(f\"      {preview[:250]}...\")\n",
    "\n",
    "    # Step 4: Create Structured Views\n",
    "    print(\"\\n[Step 4/5] Creating structured catalog view...\")\n",
    "    catalog_view = await create_catalog_view()\n",
    "    catalog_tokens = count_tokens(catalog_view)\n",
    "    redis_client.set(\"course_catalog_view\", catalog_view)\n",
    "    redis_client.set(\"course_catalog_view:updated\", \"2024-09-30\")\n",
    "    print(f\"   âœ… Created catalog view ({catalog_tokens:,} tokens)\")\n",
    "    print(f\"   âœ… Cached in Redis\")\n",
    "\n",
    "    # Show catalog structure\n",
    "    print(f\"\\n   ðŸ“„ Catalog view structure:\")\n",
    "    catalog_preview = catalog_view[:300].replace('\\n', '\\n      ')\n",
    "    print(f\"      {catalog_preview}...\")\n",
    "\n",
    "    # Step 5: Store (in production, would also create embeddings and store in vector DB)\n",
    "    print(\"\\n[Step 5/5] Storing processed data...\")\n",
    "    for i, (course, text) in enumerate(zip(cleaned_courses, transformed_courses)):\n",
    "        key = f\"course:processed:{course.course_code}\"\n",
    "        redis_client.set(key, text)\n",
    "    print(f\"   âœ… Stored {len(cleaned_courses)} processed courses in Redis\")\n",
    "\n",
    "    # Show storage example\n",
    "    if cleaned_courses:\n",
    "        print(f\"\\n   ðŸ“„ Storage example:\")\n",
    "        print(f\"      Key: course:processed:{cleaned_courses[0].course_code}\")\n",
    "        print(f\"      Value: {transformed_courses[0][:100]}...\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BATCH PROCESSING COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"\"\"\n",
    "Summary:\n",
    "- Courses processed: {len(cleaned_courses)}\n",
    "- Total tokens: {total_tokens:,}\n",
    "- Catalog view tokens: {catalog_tokens:,}\n",
    "- Storage: Redis\n",
    "- Next run: 2024-10-07 (weekly)\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Run the batch pipeline\n",
    "await batch_process_courses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e27e9e5b1bf93",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "**1. Context is Data - and Data Requires Engineering**\n",
    "- Context isn't just \"data you feed to an LLM\"\n",
    "- It requires systematic transformation: Raw â†’ Clean â†’ Transform â†’ Optimize â†’ Store\n",
    "- Engineering discipline: requirements analysis, design decisions, quality metrics, testing\n",
    "\n",
    "**2. The Data Engineering Pipeline**\n",
    "- Extract: Get raw data from sources\n",
    "- Clean: Remove noise, fix inconsistencies\n",
    "- Transform: Structure for LLM consumption\n",
    "- Optimize: Reduce tokens, improve clarity\n",
    "- Store: Choose storage strategy (RAG, Views, Hybrid)\n",
    "\n",
    "**3. Three Engineering Approaches**\n",
    "- **RAG:** Semantic search for relevant data (good for specific queries)\n",
    "- **Structured Views:** Pre-computed summaries (excellent for overviews)\n",
    "- **Hybrid:** Combine both (best for real-world use)\n",
    "\n",
    "**4. Chunking is a Design Choice, Not a Default**\n",
    "- **\"Don't chunk\"** is a valid strategy for structured data (courses, products, FAQs)\n",
    "- Consider chunking when documents have multiple distinct topics or need retrieval precision\n",
    "- Four strategies when chunking: Document-Based, Fixed-Size, Semantic, Hierarchical\n",
    "- Emerging approaches: Late Chunking, Contextual Retrieval, Hybrid Search\n",
    "- Choose based on YOUR data type, query patterns, and application requirements\n",
    "\n",
    "**5. Context Pipeline Architectures**\n",
    "- **Request-Time:** Process on-the-fly (simple, always fresh, higher latency)\n",
    "- **Batch:** Pre-process in batches (fast queries, can be stale)\n",
    "- **Event-Driven:** Process on changes (real-time, complex infrastructure)\n",
    "\n",
    "### The Engineering Mindset\n",
    "\n",
    "Every decision should be based on **YOUR specific requirements:**\n",
    "\n",
    "1. **Analyze YOUR data:** Size, structure, update frequency, topic density\n",
    "2. **Analyze YOUR queries:** Specific vs. overview, single vs. cross-section\n",
    "3. **Analyze YOUR constraints:** Token budget, latency, quality requirements\n",
    "4. **Make informed decisions:** Choose approaches that match YOUR needs\n",
    "5. **Measure and iterate:** Test with real queries, measure quality, optimize\n",
    "\n",
    "**Remember:** There is no \"best practice\" that works for everyone. Context engineering is about making deliberate, informed choices based on YOUR domain, application, and constraints.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63994fc42bf9a188",
   "metadata": {},
   "source": [
    "## Part 6: Quality Optimization - Measuring and Improving Context\n",
    "\n",
    "### The Systematic Optimization Process\n",
    "\n",
    "Now that you understand data engineering and context pipelines, let's learn how to systematically optimize context quality.\n",
    "\n",
    "**The Process:**\n",
    "```\n",
    "1. Define Quality Metrics (domain-specific)\n",
    "   â†“\n",
    "2. Establish Baseline (measure current performance)\n",
    "   â†“\n",
    "3. Experiment (try different approaches)\n",
    "   â†“\n",
    "4. Measure (compare against metrics)\n",
    "   â†“\n",
    "5. Iterate (refine based on results)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c89199eb6b0330",
   "metadata": {},
   "source": [
    "### Step 1: Define Quality Metrics for YOUR Domain\n",
    "\n",
    "**The Problem with Generic Metrics:**\n",
    "\n",
    "Don't aim for \"95% accuracy on benchmark X\" - that benchmark wasn't designed for YOUR domain.\n",
    "\n",
    "**DO this instead:** Define what \"quality\" means for YOUR domain, then measure it.\n",
    "\n",
    "### The Four Quality Dimensions\n",
    "\n",
    "Every context engineering solution should be evaluated across four dimensions:\n",
    "\n",
    "1. **Relevance** - Does context include information needed to answer the query?\n",
    "2. **Completeness** - Does context include ALL necessary information?\n",
    "3. **Efficiency** - Is context optimized for token usage?\n",
    "4. **Accuracy** - Is context factually correct and up-to-date?\n",
    "\n",
    "Different domains prioritize these differently.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e5e35147ec43d",
   "metadata": {},
   "source": [
    "### Example: Quality Metrics for Redis University Course Advisor\n",
    "\n",
    "Let's define specific, measurable quality metrics for our course advisor domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be4e1aeb26cfc100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:29.553570Z",
     "iopub.status.busy": "2025-11-05T13:43:29.553489Z",
     "iopub.status.idle": "2025-11-05T13:43:29.556597Z",
     "shell.execute_reply": "2025-11-05T13:43:29.556153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUALITY METRICS FOR REDIS UNIVERSITY COURSE ADVISOR\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "Relevance:\n",
      "  Definition: Does context include courses relevant to the user's query?\n",
      "  Metric: % of queries where retrieved courses match query intent\n",
      "  How to measure: Manual review of 50 sample queries\n",
      "  Target: >90%\n",
      "  Why important: Irrelevant courses waste tokens and confuse users\n",
      "\n",
      "Completeness:\n",
      "  Definition: Does context include all information needed to answer?\n",
      "  Metric: % of responses that mention all prerequisites when asked\n",
      "  How to measure: Automated check: parse response for prerequisite mentions\n",
      "  Target: 100%\n",
      "  Why important: Missing prerequisites leads to hallucinations\n",
      "\n",
      "Efficiency:\n",
      "  Definition: Is context optimized for token usage?\n",
      "  Metric: Average tokens per query\n",
      "  How to measure: Token counter on all context strings\n",
      "  Target: <5,000 tokens\n",
      "  Why important: Exceeding budget increases cost and latency\n",
      "\n",
      "Accuracy:\n",
      "  Definition: Is context factually correct and up-to-date?\n",
      "  Metric: % of responses with correct course information\n",
      "  How to measure: Manual review against course database\n",
      "  Target: >95%\n",
      "  Why important: Incorrect information damages trust\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
     ]
    }
   ],
   "source": [
    "# Define domain-specific quality metrics\n",
    "\n",
    "quality_metrics = {\n",
    "    \"Relevance\": {\n",
    "        \"definition\": \"Does context include courses relevant to the user's query?\",\n",
    "        \"metric\": \"% of queries where retrieved courses match query intent\",\n",
    "        \"measurement\": \"Manual review of 50 sample queries\",\n",
    "        \"target\": \">90%\",\n",
    "        \"why_important\": \"Irrelevant courses waste tokens and confuse users\",\n",
    "    },\n",
    "    \"Completeness\": {\n",
    "        \"definition\": \"Does context include all information needed to answer?\",\n",
    "        \"metric\": \"% of responses that mention all prerequisites when asked\",\n",
    "        \"measurement\": \"Automated check: parse response for prerequisite mentions\",\n",
    "        \"target\": \"100%\",\n",
    "        \"why_important\": \"Missing prerequisites leads to hallucinations\",\n",
    "    },\n",
    "    \"Efficiency\": {\n",
    "        \"definition\": \"Is context optimized for token usage?\",\n",
    "        \"metric\": \"Average tokens per query\",\n",
    "        \"measurement\": \"Token counter on all context strings\",\n",
    "        \"target\": \"<5,000 tokens\",\n",
    "        \"why_important\": \"Exceeding budget increases cost and latency\",\n",
    "    },\n",
    "    \"Accuracy\": {\n",
    "        \"definition\": \"Is context factually correct and up-to-date?\",\n",
    "        \"metric\": \"% of responses with correct course information\",\n",
    "        \"measurement\": \"Manual review against course database\",\n",
    "        \"target\": \">95%\",\n",
    "        \"why_important\": \"Incorrect information damages trust\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\n",
    "    \"\"\"QUALITY METRICS FOR REDIS UNIVERSITY COURSE ADVISOR\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\"\"\n",
    ")\n",
    "for dimension, details in quality_metrics.items():\n",
    "    print(\n",
    "        f\"\"\"\n",
    "{dimension}:\n",
    "  Definition: {details['definition']}\n",
    "  Metric: {details['metric']}\n",
    "  How to measure: {details['measurement']}\n",
    "  Target: {details['target']}\n",
    "  Why important: {details['why_important']}\"\"\"\n",
    "    )\n",
    "print(\"\\n\" + \"â”\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5331fe7c2b23131",
   "metadata": {},
   "source": [
    "### Key Insight: Metrics Must Be Domain-Specific\n",
    "\n",
    "Notice how these metrics are specific to the course advisor domain:\n",
    "\n",
    "**Relevance metric:**\n",
    "- âŒ Generic: \"Cosine similarity > 0.8\"\n",
    "- âœ… Domain-specific: \"Retrieved courses match query intent\"\n",
    "\n",
    "**Completeness metric:**\n",
    "- âŒ Generic: \"Context includes top-5 search results\"\n",
    "- âœ… Domain-specific: \"All prerequisites mentioned when asked\"\n",
    "\n",
    "**Efficiency metric:**\n",
    "- âŒ Generic: \"Minimize tokens\"\n",
    "- âœ… Domain-specific: \"<5,000 tokens (fits our budget)\"\n",
    "\n",
    "**Accuracy metric:**\n",
    "- âŒ Generic: \"95% on MMLU benchmark\"\n",
    "- âœ… Domain-specific: \"Correct course information vs. database\"\n",
    "\n",
    "**Your metrics should reflect YOUR domain's requirements, not generic benchmarks.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9936b76be5dd4f",
   "metadata": {},
   "source": [
    "### Step 2-5: Baseline â†’ Experiment â†’ Measure â†’ Iterate\n",
    "\n",
    "Let's demonstrate the optimization process with a concrete example.\n",
    "\n",
    "**Scenario:** We want to optimize our hybrid approach (catalog overview + RAG) to meet all quality targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ab33636bc9a2a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:29.557841Z",
     "iopub.status.busy": "2025-11-05T13:43:29.557772Z",
     "iopub.status.idle": "2025-11-05T13:43:29.763818Z",
     "shell.execute_reply": "2025-11-05T13:43:29.762880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:43:29 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE (Hybrid Approach):\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "Tokens: 177\n",
      "\n",
      "Context:\n",
      "Redis University Course Catalog Overview:\n",
      "\n",
      "Computer Science Department:\n",
      "- RU101: Introduction to Redis Data Structures (Beginner, 4-6 hours)\n",
      "- RU201: Redis for Python Developers (Intermediate, 6-8 hours)\n",
      "- RU301: Vector Similarity Search with Redis (Advanced, 8-10 hours)\n",
      "\n",
      "Data Science Department:\n",
      "- RU401: Machine Learning with Redis (Intermediate, 10-12 hours)\n",
      "- RU402: Real-Time Analytics with Redis (Advanced, 8-10 hours)\n",
      "\n",
      "\n",
      "Detailed Course Information:\n",
      "CS007: Machine Learning (advanced)\n",
      "Description: Introduction to machine learning algorithms and applications. Supervised and unsupervised learning, neural networks.\n",
      "Prerequisites: None\n",
      "\n",
      "MATH022: Linear Algebra (intermediate)\n",
      "Description: Vector spaces, matrices, eigenvalues, and linear transformations. Essential for data science and engineering.\n",
      "Prerequisites: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Establish Baseline (Hybrid Approach from Part 3)\n",
    "\n",
    "# Sample query\n",
    "test_query = \"What machine learning courses are available for beginners?\"\n",
    "\n",
    "# Hybrid approach: Catalog overview + RAG\n",
    "catalog_overview = \"\"\"Redis University Course Catalog Overview:\n",
    "\n",
    "Computer Science Department:\n",
    "- RU101: Introduction to Redis Data Structures (Beginner, 4-6 hours)\n",
    "- RU201: Redis for Python Developers (Intermediate, 6-8 hours)\n",
    "- RU301: Vector Similarity Search with Redis (Advanced, 8-10 hours)\n",
    "\n",
    "Data Science Department:\n",
    "- RU401: Machine Learning with Redis (Intermediate, 10-12 hours)\n",
    "- RU402: Real-Time Analytics with Redis (Advanced, 8-10 hours)\n",
    "\"\"\"\n",
    "\n",
    "# RAG: Get specific courses\n",
    "rag_results = await course_manager.search_courses(test_query, limit=2)\n",
    "rag_context = \"\\n\\n\".join(\n",
    "    [\n",
    "        f\"\"\"{course.course_code}: {course.title} ({course.difficulty_level.value})\n",
    "Description: {course.description}\n",
    "Prerequisites: {', '.join([p.course_code for p in course.prerequisites]) if course.prerequisites else 'None'}\"\"\"\n",
    "        for course in rag_results\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combined context\n",
    "baseline_context = f\"\"\"{catalog_overview}\n",
    "\n",
    "Detailed Course Information:\n",
    "{rag_context}\"\"\"\n",
    "\n",
    "baseline_tokens = count_tokens(baseline_context)\n",
    "\n",
    "print(\n",
    "    f\"\"\"BASELINE (Hybrid Approach):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Tokens: {baseline_tokens:,}\n",
    "\n",
    "Context:\n",
    "{baseline_context}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9835f424b4bba43e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:29.765996Z",
     "iopub.status.busy": "2025-11-05T13:43:29.765906Z",
     "iopub.status.idle": "2025-11-05T13:43:29.768493Z",
     "shell.execute_reply": "2025-11-05T13:43:29.768004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT (Optimized Hybrid):\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "Tokens: 111\n",
      "\n",
      "Context:\n",
      "Redis University - Relevant Departments:\n",
      "\n",
      "Data Science:\n",
      "- RU401: Machine Learning with Redis (Intermediate)\n",
      "- RU402: Real-Time Analytics (Advanced)\n",
      "\n",
      "Computer Science:\n",
      "- RU301: Vector Search (Advanced)\n",
      "\n",
      "\n",
      "CS007: Machine Learning (advanced)\n",
      "Description: Introduction to machine learning algorithms and applications. Supervised and unsupervised learning, neural networks.\n",
      "Prerequisites: None\n",
      "\n",
      "MATH022: Linear Algebra (intermediate)\n",
      "Description: Vector spaces, matrices, eigenvalues, and linear transformations. Essential for data science and engineering.\n",
      "Prerequisites: None\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "Token Reduction: 66 tokens (37.3% reduction)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Experiment - Try optimized version\n",
    "\n",
    "# Optimization: Reduce catalog overview to just relevant departments\n",
    "optimized_catalog = \"\"\"Redis University - Relevant Departments:\n",
    "\n",
    "Data Science:\n",
    "- RU401: Machine Learning with Redis (Intermediate)\n",
    "- RU402: Real-Time Analytics (Advanced)\n",
    "\n",
    "Computer Science:\n",
    "- RU301: Vector Search (Advanced)\n",
    "\"\"\"\n",
    "\n",
    "optimized_context = f\"\"\"{optimized_catalog}\n",
    "\n",
    "{rag_context}\"\"\"\n",
    "\n",
    "optimized_tokens = count_tokens(optimized_context)\n",
    "\n",
    "print(\n",
    "    f\"\"\"EXPERIMENT (Optimized Hybrid):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Tokens: {optimized_tokens:,}\n",
    "\n",
    "Context:\n",
    "{optimized_context}\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Token Reduction: {baseline_tokens - optimized_tokens:,} tokens ({((baseline_tokens - optimized_tokens) / baseline_tokens * 100):.1f}% reduction)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e9f6bf32872925d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:43:29.769722Z",
     "iopub.status.busy": "2025-11-05T13:43:29.769629Z",
     "iopub.status.idle": "2025-11-05T13:43:37.401544Z",
     "shell.execute_reply": "2025-11-05T13:43:37.400237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:43:34 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:43:37 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE RESPONSE:\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "In the Redis University course catalog, there isn't a specific machine learning course labeled as \"beginner.\" However, if you're interested in machine learning and are looking for a foundational course, you might consider starting with the Computer Science Department's RU101: Introduction to Redis Data Structures. This course is beginner-friendly and can provide a good foundation in understanding data structures, which is essential for working with machine learning algorithms.\n",
      "\n",
      "For a more direct introduction to machine learning concepts, you might want to look outside the Redis University catalog or consider CS007: Machine Learning, which is labeled as advanced but covers introductory topics in machine learning algorithms and applications.\n",
      "\n",
      "OPTIMIZED RESPONSE:\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "At Redis University, we currently do not have any beginner-level machine learning courses. However, if you're interested in machine learning, you might consider starting with foundational courses in related areas, such as linear algebra, which is essential for understanding machine learning concepts. The course MATH022: Linear Algebra (intermediate) could be a good starting point. Once you have a solid foundation, you can explore more advanced machine learning courses like CS007: Machine Learning (advanced) or RU401: Machine Learning with Redis (Intermediate).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Measure - Compare responses\n",
    "\n",
    "# Baseline response\n",
    "messages_baseline = [\n",
    "    SystemMessage(content=f\"You are a Redis University course advisor.\\n\\n{baseline_context}\"),\n",
    "    HumanMessage(content=test_query),\n",
    "]\n",
    "response_baseline = llm.invoke(messages_baseline)\n",
    "\n",
    "# Optimized response\n",
    "messages_optimized = [\n",
    "    SystemMessage(\n",
    "        content=f\"You are a Redis University course advisor.\\n\\n{optimized_context}\"\n",
    "    ),\n",
    "    HumanMessage(content=test_query),\n",
    "]\n",
    "response_optimized = llm.invoke(messages_optimized)\n",
    "\n",
    "print(\n",
    "    f\"\"\"BASELINE RESPONSE:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "{response_baseline.content}\n",
    "\n",
    "OPTIMIZED RESPONSE:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "{response_optimized.content}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73762d987cdf036d",
   "metadata": {},
   "source": [
    "### Step 5: Iterate - Refine Based on Results\n",
    "\n",
    "Based on the measurements:\n",
    "\n",
    "**Quality Assessment:**\n",
    "- âœ… **Relevance:** Both approaches retrieve relevant ML courses\n",
    "- âœ… **Completeness:** Both mention prerequisites and difficulty levels\n",
    "- âœ… **Efficiency:** Optimized version uses fewer tokens ({optimized_tokens} vs {baseline_tokens})\n",
    "- âœ… **Accuracy:** Both provide correct course information\n",
    "\n",
    "**Decision:** The optimized hybrid approach meets all quality targets while reducing token usage.\n",
    "\n",
    "**Next Iteration:** Test with more queries to ensure consistency across different query types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6618ae42e227c5d1",
   "metadata": {},
   "source": [
    "### Key Takeaways: Quality Optimization\n",
    "\n",
    "1. **Define Domain-Specific Metrics** - Don't rely on generic benchmarks\n",
    "2. **Measure Systematically** - Baseline â†’ Experiment â†’ Measure â†’ Iterate\n",
    "3. **Balance Trade-offs** - Relevance vs. Efficiency, Completeness vs. Token Budget\n",
    "4. **Test Before Deployment** - Validate with real queries from your domain\n",
    "5. **Iterate Continuously** - Quality optimization is ongoing, not one-time\n",
    "\n",
    "**The Engineering Mindset:**\n",
    "- Context quality is measurable\n",
    "- Optimization is systematic, not guesswork\n",
    "- Domain-specific metrics matter more than generic benchmarks\n",
    "- Testing and iteration are essential\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76bb463b4f96e72",
   "metadata": {},
   "source": [
    "## ðŸ“ Summary\n",
    "\n",
    "You've mastered practical context engineering:\n",
    "\n",
    "**Part 1: The Engineering Mindset**\n",
    "- âœ… Context is data requiring engineering discipline\n",
    "- âœ… Naive approaches fail in real-world applications\n",
    "- âœ… Engineering mindset: Requirements â†’ Transformation â†’ Quality â†’ Testing\n",
    "\n",
    "**Part 2: Data Engineering Pipeline**\n",
    "- âœ… Extract â†’ Clean â†’ Transform â†’ Optimize â†’ Store\n",
    "- âœ… Concrete examples with course data\n",
    "- âœ… Token optimization techniques\n",
    "\n",
    "**Part 3: Engineering Approaches**\n",
    "- âœ… RAG (Semantic Search)\n",
    "- âœ… Structured Views (Pre-Computed Summaries)\n",
    "- âœ… Hybrid (Best of Both Worlds)\n",
    "- âœ… Decision framework for choosing approaches\n",
    "\n",
    "**Part 4: Chunking Strategies**\n",
    "- âœ… When to chunk (critical first question)\n",
    "- âœ… Four strategies with LangChain integration\n",
    "- âœ… Trade-offs and decision criteria\n",
    "\n",
    "**Part 5: Context Pipeline Architectures**\n",
    "- âœ… Request-Time, Batch, Event-Driven\n",
    "- âœ… Batch processing example with data\n",
    "- âœ… Decision framework for architecture selection\n",
    "\n",
    "**Part 6: Quality Optimization**\n",
    "- âœ… Domain-specific quality metrics\n",
    "- âœ… Systematic optimization process\n",
    "- âœ… Baseline â†’ Experiment â†’ Measure â†’ Iterate\n",
    "\n",
    "**You're now ready to engineer practical context for any domain!** ðŸŽ‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e80246eb4632a",
   "metadata": {},
   "source": [
    "## ðŸš€ What's Next?\n",
    "\n",
    "### Section 3: Memory Systems for Context Engineering\n",
    "\n",
    "Now that you can engineer high-quality retrieved context, you'll learn to manage conversation context:\n",
    "- **Working Memory:** Track conversation history within a session\n",
    "- **Long-term Memory:** Remember user preferences across sessions\n",
    "- **LangGraph Integration:** Manage stateful workflows with checkpointing\n",
    "- **Redis Agent Memory Server:** Automatic memory extraction and retrieval\n",
    "\n",
    "### Section 4: Tool Use and Agents\n",
    "\n",
    "After adding memory, you'll build complete autonomous agents:\n",
    "- **Tool Calling:** Let the AI use functions (search, enroll, check prerequisites)\n",
    "- **LangGraph State Management:** Orchestrate complex multi-step workflows\n",
    "- **Agent Reasoning:** Plan and execute multi-step tasks\n",
    "- **Practical Patterns:** Error handling, retries, and monitoring\n",
    "\n",
    "```\n",
    "Section 1: Context Engineering Fundamentals\n",
    "    â†“\n",
    "Section 2, NB1: RAG Fundamentals\n",
    "    â†“\n",
    "Section 2, NB2: Crafting and Optimizing Context â† You are here\n",
    "    â†“\n",
    "Section 3: Memory Systems for Context Engineering â† Next\n",
    "    â†“\n",
    "Section 4: Tool Use and Agents (Complete System)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "**Chunking Strategies:**\n",
    "- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "- [LlamaIndex Node Parsers](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/)\n",
    "\n",
    "**Data Engineering for LLMs:**\n",
    "- [OpenAI Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "\n",
    "**Vector Databases:**\n",
    "- [Redis Vector Search Documentation](https://redis.io/docs/stack/search/reference/vectors/)\n",
    "- [RedisVL Python Library](https://github.com/RedisVentures/redisvl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682e2447c50f133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
