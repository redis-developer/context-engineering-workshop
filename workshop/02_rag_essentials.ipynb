{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08907fd85bc2579",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Module 2: RAG Fundamentals and Implementation\n",
    "\n",
    "## Workshop: Context Engineering Matters\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "\n",
    "1. **Understand** what RAG is and why it matters for context engineering\n",
    "2. **Implement** vector embeddings and semantic search with Redis\n",
    "3. **Build** a complete RAG pipeline using HierarchicalCourseManager\n",
    "4. **Apply** progressive disclosure for efficient context retrieval\n",
    "5. **Compare** poor vs. well-engineered context approaches\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Module 1: Introduction to Context Engineering\n",
    "- Redis Stack running (with vector search capability)\n",
    "- OpenAI API key configured\n",
    "- Course data loaded in Redis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44bb37043ffa952",
   "metadata": {},
   "source": [
    "## Part 1: What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances LLM responses by:\n",
    "\n",
    "1. **Retrieving** relevant information from external sources\n",
    "2. **Augmenting** the prompt with this retrieved context\n",
    "3. **Generating** responses grounded in factual data\n",
    "\n",
    "### Why RAG Matters\n",
    "\n",
    "| Problem | RAG Solution |\n",
    "|---------|-------------|\n",
    "| LLMs have knowledge cutoff dates | Retrieve current information |\n",
    "| LLMs can hallucinate facts | Ground responses in real data |\n",
    "| LLMs don't know your domain | Inject domain-specific knowledge |\n",
    "| Context windows are limited | Retrieve only relevant information |\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "```\n",
    "User Query ‚Üí Embed Query ‚Üí Search Vector DB ‚Üí Retrieve Documents ‚Üí Assemble Context ‚Üí Generate Response\n",
    "```\n",
    "\n",
    "Let's build each component step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124479512e5a58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de35579524438ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:14.300930Z",
     "iopub.status.busy": "2025-12-05T00:13:14.300723Z",
     "iopub.status.idle": "2025-12-05T00:13:16.315246Z",
     "shell.execute_reply": "2025-12-05T00:13:16.314794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded\n",
      "   OPENAI_API_KEY: ‚úì Set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "# Handle both running from workshop/ directory and from project root\n",
    "if Path.cwd().name == \"workshop\":\n",
    "    project_root = Path.cwd().parent\n",
    "else:\n",
    "    project_root = Path.cwd()\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables from project root\n",
    "from dotenv import load_dotenv\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Verify required environment variables\n",
    "required_vars = [\"OPENAI_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\"\"\n",
    "‚ö†Ô∏è  Missing required environment variables: {', '.join(missing_vars)}\n",
    "\n",
    "Please create a .env file in the project root with:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "\n",
    "For Redis setup:\n",
    "- Local: docker run -d -p 6379:6379 redis/redis-stack-server:latest\n",
    "- Cloud: https://redis.com/try-free/\n",
    "\"\"\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "print(\"‚úÖ Environment variables loaded\")\n",
    "print(f\"   OPENAI_API_KEY: {'‚úì Set' if os.getenv('OPENAI_API_KEY') else '‚úó Not set'}\")\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Project imports\n",
    "from redis_context_course.redis_config import redis_config\n",
    "from redis_context_course.hierarchical_manager import HierarchicalCourseManager\n",
    "from redis_context_course.hierarchical_context import HierarchicalContextAssembler\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419ac1d571fabaf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:16.316526Z",
     "iopub.status.busy": "2025-12-05T00:13:16.316397Z",
     "iopub.status.idle": "2025-12-05T00:13:16.451644Z",
     "shell.execute_reply": "2025-12-05T00:13:16.451282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Redis connection successful!\n",
      "‚úÖ HierarchicalCourseManager initialized\n",
      "‚úÖ HierarchicalContextAssembler initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize clients\n",
    "openai_client = OpenAI()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Initialize Redis connection (redis_config.redis_client is a property)\n",
    "redis_client = redis_config.redis_client\n",
    "\n",
    "# Initialize HierarchicalCourseManager for two-tier retrieval\n",
    "hierarchical_manager = HierarchicalCourseManager(redis_client=redis_client)\n",
    "context_assembler = HierarchicalContextAssembler()\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    redis_client.ping()\n",
    "    print(\"\"\"‚úÖ Redis connection successful!\n",
    "‚úÖ HierarchicalCourseManager initialized\n",
    "‚úÖ HierarchicalContextAssembler initialized\"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Redis connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a619065df1becbf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Understanding Vector Embeddings\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Embeddings are **numerical representations** of text that capture semantic meaning:\n",
    "\n",
    "- Similar concepts have similar embeddings\n",
    "- We can measure similarity using cosine similarity\n",
    "- This enables **semantic search** (finding by meaning, not keywords)\n",
    "\n",
    "### Embedding Dimensions\n",
    "\n",
    "| Model | Dimensions | Use Case |\n",
    "|-------|------------|----------|\n",
    "| text-embedding-3-small | 1536 | General purpose, cost-effective |\n",
    "| text-embedding-3-large | 3072 | Higher accuracy, more expensive |\n",
    "| text-embedding-ada-002 | 1536 | Legacy model |\n",
    "\n",
    "Let's see embeddings in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e51f074585d91aca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:16.452828Z",
     "iopub.status.busy": "2025-12-05T00:13:16.452762Z",
     "iopub.status.idle": "2025-12-05T00:13:17.296842Z",
     "shell.execute_reply": "2025-12-05T00:13:17.295858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:17 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Embeddings Demo\n",
      "============================================================\n",
      "Number of texts: 4\n",
      "Embedding dimensions: 1536\n",
      "\n",
      "Sample embedding (first 10 values):\n",
      "[-0.030342772603034973, -0.013117661699652672, 0.0014119355473667383, -0.03249717131257057, 0.03269851580262184, -0.003513479605317116, -0.0007751803495921195, 0.04941019415855408, -0.02436280995607376, 0.07264547049999237]...\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for sample texts\n",
    "sample_texts = [\n",
    "    \"Introduction to machine learning and neural networks\",\n",
    "    \"Deep learning fundamentals with Python\",\n",
    "    \"Advanced calculus and differential equations\",\n",
    "    \"Redis database administration and optimization\",\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "response = openai_client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=sample_texts\n",
    ")\n",
    "\n",
    "# Extract embedding vectors\n",
    "embeddings_list = [item.embedding for item in response.data]\n",
    "\n",
    "print(f\"\"\"Vector Embeddings Demo\n",
    "{'=' * 60}\n",
    "Number of texts: {len(sample_texts)}\n",
    "Embedding dimensions: {len(embeddings_list[0])}\n",
    "\n",
    "Sample embedding (first 10 values):\n",
    "{embeddings_list[0][:10]}...\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44342365ce0b82",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "Cosine similarity measures the angle between two vectors:\n",
    "- **1.0** = Identical meaning\n",
    "- **0.0** = Unrelated\n",
    "- **-1.0** = Opposite meaning (rare in practice)\n",
    "\n",
    "Let's compare our sample texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90bc416fbbe189fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:17.298896Z",
     "iopub.status.busy": "2025-12-05T00:13:17.298719Z",
     "iopub.status.idle": "2025-12-05T00:13:17.303758Z",
     "shell.execute_reply": "2025-12-05T00:13:17.303098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity Matrix\n",
      "============================================================\n",
      "\n",
      "[0] Introduction to machine learning and neural networ...\n",
      "    vs [1]: 0.5249\n",
      "    vs [2]: 0.3053\n",
      "    vs [3]: 0.0955\n",
      "\n",
      "[1] Deep learning fundamentals with Python...\n",
      "    vs [2]: 0.2684\n",
      "    vs [3]: 0.1511\n",
      "\n",
      "[2] Advanced calculus and differential equations...\n",
      "    vs [3]: 0.1426\n",
      "\n",
      "[3] Redis database administration and optimization...\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Compare all pairs\n",
    "print(f\"\"\"Semantic Similarity Matrix\n",
    "{'=' * 60}\"\"\")\n",
    "\n",
    "for i, text1 in enumerate(sample_texts):\n",
    "    print(f\"\\n[{i}] {text1[:50]}...\")\n",
    "    for j, text2 in enumerate(sample_texts):\n",
    "        if i < j:\n",
    "            similarity = cosine_similarity(embeddings_list[i], embeddings_list[j])\n",
    "            print(f\"    vs [{j}]: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5cfe024e4ad77e",
   "metadata": {},
   "source": [
    "### Key Insight: Semantic vs. Keyword Search\n",
    "\n",
    "Notice how:\n",
    "- \"machine learning\" and \"deep learning\" have HIGH similarity (related topics)\n",
    "- \"machine learning\" and \"calculus\" have LOWER similarity (different domains)\n",
    "- \"Redis database\" is distinct from all ML topics\n",
    "\n",
    "This is the power of semantic search - it understands **meaning**, not just keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6dcbcac48b2233",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Progressive Disclosure with HierarchicalCourseManager\n",
    "\n",
    "### The Two-Tier Architecture\n",
    "\n",
    "The `HierarchicalCourseManager` implements **progressive disclosure**:\n",
    "\n",
    "```\n",
    "Tier 1: Course Summaries (lightweight, fast search)\n",
    "    ‚Üì\n",
    "Tier 2: Course Details (comprehensive, on-demand)\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- Search across ALL courses using lightweight summaries\n",
    "- Fetch full details ONLY for the most relevant matches\n",
    "- Efficient token usage (don't load everything upfront)\n",
    "\n",
    "### Key Methods\n",
    "\n",
    "| Method | Purpose | Returns |\n",
    "|--------|---------|--------|\n",
    "| `search_summaries()` | Tier 1: Fast vector search | List of CourseSummary |\n",
    "| `fetch_details()` | Tier 2: Get full course info | List of CourseDetails |\n",
    "| `hierarchical_search()` | Combined two-stage retrieval | (summaries, details) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf5cbe5c63b17a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:17.305444Z",
     "iopub.status.busy": "2025-12-05T00:13:17.305300Z",
     "iopub.status.idle": "2025-12-05T00:13:17.999366Z",
     "shell.execute_reply": "2025-12-05T00:13:17.998350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier 1: Searching Course Summaries\n",
      "============================================================\n",
      "Query: \"machine learning courses for beginners\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:17 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:17 redis_context_course.hierarchical_manager INFO   Created summary index: course_summaries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:17 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: machine learning courses for beginners\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matching courses:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Tier 1: Search Summaries\n",
    "query = \"machine learning courses for beginners\"\n",
    "\n",
    "print(f\"\"\"Tier 1: Searching Course Summaries\n",
    "{'=' * 60}\n",
    "Query: \"{query}\"\n",
    "\"\"\")\n",
    "\n",
    "# Search summaries (lightweight)\n",
    "summaries = await hierarchical_manager.search_summaries(\n",
    "    query=query,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(f\"Found {len(summaries)} matching courses:\\n\")\n",
    "for i, summary in enumerate(summaries, 1):\n",
    "    print(f\"\"\"{i}. {summary.course_code}: {summary.title}\n",
    "   Department: {summary.department} | Level: {summary.difficulty_level.value}\n",
    "   {summary.short_description[:100]}...\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba9e42a85a538b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:18.001695Z",
     "iopub.status.busy": "2025-12-05T00:13:18.001504Z",
     "iopub.status.idle": "2025-12-05T00:13:18.005849Z",
     "shell.execute_reply": "2025-12-05T00:13:18.005124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier 2: Fetching Full Details\n",
      "============================================================\n",
      "Fetching details for: []\n",
      "\n",
      "19:13:18 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Tier 2: Fetch Details for Top Matches\n",
    "top_course_codes = [s.course_code for s in summaries[:2]]\n",
    "\n",
    "print(f\"\"\"Tier 2: Fetching Full Details\n",
    "{'=' * 60}\n",
    "Fetching details for: {top_course_codes}\n",
    "\"\"\")\n",
    "\n",
    "details = await hierarchical_manager.fetch_details(top_course_codes)\n",
    "\n",
    "for detail in details:\n",
    "    print(f\"\"\"\n",
    "{'‚îÄ' * 60}\n",
    "{detail.course_code}: {detail.title}\n",
    "Instructor: {detail.instructor}\n",
    "\n",
    "Description:\n",
    "{detail.full_description[:300]}...\n",
    "\n",
    "Learning Objectives:\"\"\")\n",
    "    for obj in detail.learning_objectives[:3]:\n",
    "        print(f\"  ‚Ä¢ {obj}\")\n",
    "    print(f\"\"\"\n",
    "Syllabus: {detail.syllabus.total_weeks} weeks\n",
    "Assignments: {detail.get_total_assignments()} total ({detail.get_total_points()} points)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c513416ab472c40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:18.007612Z",
     "iopub.status.busy": "2025-12-05T00:13:18.007470Z",
     "iopub.status.idle": "2025-12-05T00:13:18.372077Z",
     "shell.execute_reply": "2025-12-05T00:13:18.370963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Search (Two-Stage Retrieval)\n",
      "============================================================\n",
      "Query: \"advanced Redis data structures\"\n",
      "\n",
      "19:13:18 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'advanced Redis data structures' (summaries=5, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:18 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:18 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: advanced Redis data structures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:18 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:18 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Retrieved 0 summaries\n",
      "Stage 2: Fetched 0 detailed courses\n",
      "\n",
      "All Matching Courses (Summaries):\n",
      "\n",
      "‚òÖ = Full details available | ‚óã = Summary only\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Combined Hierarchical Search\n",
    "query = \"advanced Redis data structures\"\n",
    "\n",
    "print(f\"\"\"Hierarchical Search (Two-Stage Retrieval)\n",
    "{'=' * 60}\n",
    "Query: \"{query}\"\n",
    "\"\"\")\n",
    "\n",
    "summaries, details = await hierarchical_manager.hierarchical_search(\n",
    "    query=query,\n",
    "    summary_limit=5,  # Get 5 summaries\n",
    "    detail_limit=2    # But only 2 full details\n",
    ")\n",
    "\n",
    "print(f\"\"\"Stage 1: Retrieved {len(summaries)} summaries\n",
    "Stage 2: Fetched {len(details)} detailed courses\n",
    "\n",
    "All Matching Courses (Summaries):\"\"\")\n",
    "for i, s in enumerate(summaries, 1):\n",
    "    marker = \"‚òÖ\" if i <= len(details) else \"‚óã\"\n",
    "    print(f\"  {marker} {s.course_code}: {s.title}\")\n",
    "\n",
    "print(f\"\\n‚òÖ = Full details available | ‚óã = Summary only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef865b89953d10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Building the Complete RAG Pipeline\n",
    "\n",
    "Now let's combine everything into a complete RAG pipeline: Retrieval ‚Üí Context Assembly ‚Üí Generation.\n",
    "\n",
    "### The RAG Flow\n",
    "\n",
    "```\n",
    "User Query\n",
    "    ‚Üì\n",
    "1. Hierarchical Search (retrieve summaries ‚Üí fetch details)\n",
    "    ‚Üì\n",
    "2. Context Assembly (combine system + user + retrieved context)\n",
    "    ‚Üì\n",
    "3. LLM Generation (create personalized response)\n",
    "```\n",
    "\n",
    "Let's implement each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3238a84f6c3c0ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:18.373925Z",
     "iopub.status.busy": "2025-12-05T00:13:18.373771Z",
     "iopub.status.idle": "2025-12-05T00:13:18.376265Z",
     "shell.execute_reply": "2025-12-05T00:13:18.375935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized (gpt-4o-mini)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "print(\"‚úÖ LLM initialized (gpt-4o-mini)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0ab12aeb0799d",
   "metadata": {},
   "source": [
    "### Step 5.1: Retrieval Function with Hierarchical Search\n",
    "\n",
    "First, let's create a function to retrieve relevant courses using hierarchical search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df25249b94098e3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:18.377489Z",
     "iopub.status.busy": "2025-12-05T00:13:18.377396Z",
     "iopub.status.idle": "2025-12-05T00:13:19.878014Z",
     "shell.execute_reply": "2025-12-05T00:13:19.876983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:18 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'I want to learn about data structures' (summaries=5, details=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:19 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:19 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: I want to learn about data structures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:19 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:19 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Retrieved for: 'I want to learn about data structures'\n",
      "   Summaries: 0\n",
      "   Details: 0\n"
     ]
    }
   ],
   "source": [
    "async def retrieve_courses_hierarchical(\n",
    "    query: str,\n",
    "    summary_limit: int = 5,\n",
    "    detail_limit: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve relevant courses using hierarchical search.\n",
    "\n",
    "    Args:\n",
    "        query: User's search query\n",
    "        summary_limit: Number of summaries to retrieve\n",
    "        detail_limit: Number of full details to fetch\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (summaries, details)\n",
    "    \"\"\"\n",
    "    summaries, details = await hierarchical_manager.hierarchical_search(\n",
    "        query=query,\n",
    "        summary_limit=summary_limit,\n",
    "        detail_limit=detail_limit\n",
    "    )\n",
    "    return summaries, details\n",
    "\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"I want to learn about data structures\"\n",
    "test_summaries, test_details = await retrieve_courses_hierarchical(test_query)\n",
    "\n",
    "print(f\"üîç Retrieved for: '{test_query}'\")\n",
    "print(f\"   Summaries: {len(test_summaries)}\")\n",
    "print(f\"   Details: {len(test_details)}\")\n",
    "for s in test_summaries:\n",
    "    print(f\"   - {s.course_code}: {s.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4b01fc97a364",
   "metadata": {},
   "source": [
    "### Step 5.2: Context Assembly Function\n",
    "\n",
    "Now let's assemble context from multiple sources (system + user + retrieved):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59810b880894772e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:19.880228Z",
     "iopub.status.busy": "2025-12-05T00:13:19.880050Z",
     "iopub.status.idle": "2025-12-05T00:13:19.885325Z",
     "shell.execute_reply": "2025-12-05T00:13:19.884866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Context assembled\n",
      "   Total length: 769 characters\n",
      "   Includes: System + User + Retrieved context (hierarchical)\n"
     ]
    }
   ],
   "source": [
    "def assemble_rag_context(\n",
    "    user_query: str,\n",
    "    summaries: list,\n",
    "    details: list,\n",
    "    user_profile: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Assemble context from multiple sources for the LLM.\n",
    "\n",
    "    This implements the context engineering principles from Module 1:\n",
    "    - System Context: AI role and instructions\n",
    "    - User Context: Student profile and preferences\n",
    "    - Retrieved Context: Relevant courses from hierarchical search\n",
    "    \"\"\"\n",
    "\n",
    "    # System Context: Define the AI's role\n",
    "    system_context = \"\"\"You are a Redis University course advisor.\n",
    "\n",
    "Your role:\n",
    "- Help students find courses that match their interests and goals\n",
    "- Provide personalized recommendations based on student profiles\n",
    "- Explain course prerequisites and learning paths\n",
    "- Be encouraging and supportive\n",
    "\n",
    "Guidelines:\n",
    "- Only recommend courses from the provided course list\n",
    "- Consider student's difficulty level preferences\n",
    "- Explain your reasoning for recommendations\n",
    "- Be concise but informative\n",
    "\"\"\"\n",
    "\n",
    "    # User Context: Student profile (if provided)\n",
    "    user_context = \"\"\n",
    "    if user_profile:\n",
    "        user_context = f\"\"\"\n",
    "Student Profile:\n",
    "- Name: {user_profile.get('name', 'Student')}\n",
    "- Major: {user_profile.get('major', 'Undeclared')}\n",
    "- Year: {user_profile.get('year', 'N/A')}\n",
    "- Interests: {', '.join(user_profile.get('interests', []))}\n",
    "- Preferred Difficulty: {user_profile.get('preferred_difficulty', 'any')}\n",
    "- Preferred Format: {user_profile.get('preferred_format', 'any')}\n",
    "\"\"\"\n",
    "\n",
    "    # Retrieved Context: Use HierarchicalContextAssembler\n",
    "    retrieved_context = context_assembler.assemble_hierarchical_context(\n",
    "        summaries=summaries,\n",
    "        details=details,\n",
    "        query=user_query\n",
    "    )\n",
    "\n",
    "    # Combine all context\n",
    "    full_context = system_context\n",
    "    if user_context:\n",
    "        full_context += user_context\n",
    "    full_context += \"\\n\" + retrieved_context\n",
    "\n",
    "    return full_context\n",
    "\n",
    "\n",
    "# Test context assembly\n",
    "test_profile = {\n",
    "    \"name\": \"Sarah Chen\",\n",
    "    \"major\": \"Computer Science\",\n",
    "    \"year\": \"Junior\",\n",
    "    \"interests\": [\"machine learning\", \"data science\"],\n",
    "    \"preferred_difficulty\": \"intermediate\",\n",
    "    \"preferred_format\": \"online\",\n",
    "}\n",
    "\n",
    "assembled_context = assemble_rag_context(\n",
    "    user_query=test_query,\n",
    "    summaries=test_summaries,\n",
    "    details=test_details,\n",
    "    user_profile=test_profile,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Context assembled\")\n",
    "print(f\"   Total length: {len(assembled_context)} characters\")\n",
    "print(f\"   Includes: System + User + Retrieved context (hierarchical)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f04f2613a036d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:19.887315Z",
     "iopub.status.busy": "2025-12-05T00:13:19.887159Z",
     "iopub.status.idle": "2025-12-05T00:13:19.889513Z",
     "shell.execute_reply": "2025-12-05T00:13:19.889009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observe the assembled context: \n",
      "\n",
      "You are a Redis University course advisor.\n",
      "\n",
      "Your role:\n",
      "- Help students find courses that match their interests and goals\n",
      "- Provide personalized recommendations based on student profiles\n",
      "- Explain course prerequisites and learning paths\n",
      "- Be encouraging and supportive\n",
      "\n",
      "Guidelines:\n",
      "- Only recommend courses from the provided course list\n",
      "- Consider student's difficulty level preferences\n",
      "- Explain your reasoning for recommendations\n",
      "- Be concise but informative\n",
      "\n",
      "Student Profile:\n",
      "- Name: Sarah Chen\n",
      "- Major: Computer Science\n",
      "- Year: Junior\n",
      "- Interests: machine learning, data science\n",
      "- Preferred Difficulty: intermediate\n",
      "- Preferred Format: online\n",
      "\n",
      "# Course Search Results for: I want to learn about data structures\n",
      "\n",
      "## Overview of All Matches\n",
      "\n",
      "Found 0 relevant courses:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observe the assembled context: \\n\\n{assembled_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc183239beb27721",
   "metadata": {},
   "source": [
    "**üéÅ Bonus:** Can you identify the different parts of the context from what we learned in Module 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afa4b551eddfb2",
   "metadata": {},
   "source": [
    "**‚úÖ Answer:** Yes! Looking at the assembled context above, we can identify all three context types from Module 1:\n",
    "\n",
    "1. **System Context** (Static)\n",
    "   - The first section: \"You are a Redis University course advisor...\"\n",
    "   - Defines the AI's role, responsibilities, and guidelines\n",
    "   - Remains the same for all queries\n",
    "   - Sets behavioral instructions and constraints\n",
    "\n",
    "2. **User Context** (Dynamic, User-Specific)\n",
    "   - The \"Student Profile\" section\n",
    "   - Contains Sarah Chen's personal information: major, year, interests, preferences\n",
    "   - Changes based on who is asking the question\n",
    "   - Enables personalized recommendations\n",
    "\n",
    "3. **Retrieved Context** (Dynamic, Query-Specific)\n",
    "   - The \"Relevant Courses\" section with summaries and details\n",
    "   - Uses progressive disclosure (summaries for all, details for top matches)\n",
    "   - Changes based on the specific query\n",
    "   - Provides the factual information the LLM needs to answer\n",
    "\n",
    "Notice how all three work together: System Context tells the AI **how to behave**, User Context tells it **who it's helping**, and Retrieved Context provides **what information is relevant**. This is RAG in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e419e0af5d9c6fc",
   "metadata": {},
   "source": [
    "### Step 5.3: Generation Function\n",
    "\n",
    "Finally, let's generate a response using the assembled context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4fb8925997cde24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:19.891524Z",
     "iopub.status.busy": "2025-12-05T00:13:19.891385Z",
     "iopub.status.idle": "2025-12-05T00:13:22.587943Z",
     "shell.execute_reply": "2025-12-05T00:13:22.587199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:22 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Generated Response:\n",
      "\n",
      "It looks like there are currently no courses available that focus specifically on data structures. However, since you are interested in machine learning and data science, I recommend exploring courses that cover those topics. \n",
      "\n",
      "Would you like me to assist you in finding courses related to machine learning or data science that align with your intermediate difficulty preference? These subjects often involve a strong understanding of data structures, which could be beneficial for your studies in computer science. Let me know how you would like to proceed!\n"
     ]
    }
   ],
   "source": [
    "async def generate_response(user_query: str, context: str):\n",
    "    \"\"\"\n",
    "    Generate LLM response using assembled context.\n",
    "\n",
    "    Args:\n",
    "        user_query: User's question\n",
    "        context: Assembled context (system + user + retrieved)\n",
    "\n",
    "    Returns:\n",
    "        LLM response string\n",
    "    \"\"\"\n",
    "    messages = [SystemMessage(content=context), HumanMessage(content=user_query)]\n",
    "\n",
    "    response = await llm.ainvoke(messages)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Test generation\n",
    "response = await generate_response(test_query, assembled_context)\n",
    "\n",
    "print(\"\\nü§ñ Generated Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c60efbc61f04f",
   "metadata": {},
   "source": [
    "### üéØ Understanding the Generated Response\n",
    "\n",
    "Notice how the LLM's response demonstrates effective context engineering:\n",
    "\n",
    "**üë§ Personalization from User Context:**\n",
    "- Addresses Sarah by name\n",
    "- References her intermediate difficulty preference\n",
    "- Acknowledges her online format preference\n",
    "- Connects to her interests (machine learning and data science)\n",
    "\n",
    "**üìö Accuracy from Retrieved Context:**\n",
    "- Recommends courses from the retrieved list\n",
    "- Provides correct course details (difficulty, format, credits, description)\n",
    "- Mentions prerequisites accurately\n",
    "\n",
    "**ü§ñ Guidance from System Context:**\n",
    "- Acts as a supportive advisor\n",
    "- Explains reasoning for the recommendation\n",
    "- Stays within the provided course list\n",
    "\n",
    "This is the power of RAG: the LLM generates a response that is **personalized** (User Context), **accurate** (Retrieved Context), and **helpful** (System Context). Without RAG, the LLM would either hallucinate course details or provide generic advice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6b9a7074905d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Complete RAG Function\n",
    "\n",
    "Let's combine all three steps into a single, reusable RAG function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "349d54053ccbe5d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:22.590159Z",
     "iopub.status.busy": "2025-12-05T00:13:22.589975Z",
     "iopub.status.idle": "2025-12-05T00:13:27.470821Z",
     "shell.execute_reply": "2025-12-05T00:13:27.469748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPLETE HIERARCHICAL RAG PIPELINE TEST\n",
      "============================================================\n",
      "\n",
      "Query: I'm interested in learning about databases and data management\n",
      "\n",
      "Student: Alex Johnson (Data Science, Sophomore)\n",
      "\n",
      "19:13:22 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'I'm interested in learning about databases and data management' (summaries=5, details=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:23 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:23 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: I'm interested in learning about databases and data management\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:23 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:23 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:27 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Courses:\n",
      "\n",
      "AI Response:\n",
      "It seems that there are currently no specific courses available that match your interests in databases and data management. However, I encourage you to explore other related topics or areas that might align with your goals in data science.\n",
      "\n",
      "Given your major and your interest in SQL and data analysis, I recommend looking for courses that cover:\n",
      "\n",
      "1. **Data Warehousing**: This will deepen your understanding of how databases are structured for efficient data analysis.\n",
      "2. **SQL for Data Science**: Courses that focus on SQL will enhance your skills in querying databases and managing data effectively.\n",
      "\n",
      "If you have access to other platforms or institutions, consider checking their offerings as well. Don't hesitate to reach out if you need further assistance or have other interests you'd like to explore!\n"
     ]
    }
   ],
   "source": [
    "async def hierarchical_rag_query(\n",
    "    user_query: str,\n",
    "    user_profile: dict = None,\n",
    "    summary_limit: int = 5,\n",
    "    detail_limit: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with hierarchical retrieval: Retrieve ‚Üí Assemble ‚Üí Generate\n",
    "\n",
    "    Args:\n",
    "        user_query: User's question\n",
    "        user_profile: Optional student profile\n",
    "        summary_limit: Number of summaries to retrieve\n",
    "        detail_limit: Number of full details to fetch\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (response, summaries, details)\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant courses (hierarchical)\n",
    "    summaries, details = await retrieve_courses_hierarchical(\n",
    "        user_query, summary_limit, detail_limit\n",
    "    )\n",
    "\n",
    "    # Step 2: Assemble context\n",
    "    context = assemble_rag_context(user_query, summaries, details, user_profile)\n",
    "\n",
    "    # Step 3: Generate response\n",
    "    response = await generate_response(user_query, context)\n",
    "\n",
    "    return response, summaries, details\n",
    "\n",
    "\n",
    "# Test the complete RAG pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE HIERARCHICAL RAG PIPELINE TEST\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "query = \"I'm interested in learning about databases and data management\"\n",
    "profile = {\n",
    "    \"name\": \"Alex Johnson\",\n",
    "    \"major\": \"Data Science\",\n",
    "    \"year\": \"Sophomore\",\n",
    "    \"interests\": [\"databases\", \"data analysis\", \"SQL\"],\n",
    "    \"preferred_difficulty\": \"intermediate\",\n",
    "    \"preferred_format\": \"hybrid\",\n",
    "}\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print()\n",
    "print(f\"Student: {profile['name']} ({profile['major']}, {profile['year']})\")\n",
    "print()\n",
    "\n",
    "response, summaries, details = await hierarchical_rag_query(query, profile)\n",
    "\n",
    "print(\"Retrieved Courses:\")\n",
    "for i, s in enumerate(summaries, 1):\n",
    "    marker = \"‚òÖ\" if i <= len(details) else \"‚óã\"\n",
    "    print(f\"   {marker} {s.course_code}: {s.title}\")\n",
    "print()\n",
    "\n",
    "print(\"AI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5db7eb620763a",
   "metadata": {},
   "source": [
    "### üéØ Why This Complete RAG Function Matters\n",
    "\n",
    "The `hierarchical_rag_query()` function encapsulates the entire RAG pipeline in a single, reusable interface. This is important because:\n",
    "\n",
    "**1. Simplicity:** One function call handles retrieval ‚Üí assembly ‚Üí generation\n",
    "- No need to manually orchestrate the three steps\n",
    "- Clean API for building applications\n",
    "\n",
    "**2. Consistency:** Every query follows the same pattern\n",
    "- Ensures all three context types are always included\n",
    "- Reduces errors from missing context\n",
    "\n",
    "**3. Efficiency:** Progressive disclosure saves tokens\n",
    "- Summaries for all matches (lightweight)\n",
    "- Full details only for top matches (on-demand)\n",
    "- Optimal token usage\n",
    "\n",
    "**4. Production-Ready:** This pattern scales to real applications\n",
    "- In Module 4, we'll add memory (conversation history)\n",
    "- In Module 5, we'll add tools (course enrollment, prerequisites checking)\n",
    "- The core RAG pattern remains the same\n",
    "\n",
    "This is the foundation you'll build on throughout the rest of the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ad5741b33f1ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Try Different Queries\n",
    "\n",
    "Let's test our RAG system with various queries to see how it handles different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d8a9f93c1bea4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:27.473331Z",
     "iopub.status.busy": "2025-12-05T00:13:27.473089Z",
     "iopub.status.idle": "2025-12-05T00:13:31.512075Z",
     "shell.execute_reply": "2025-12-05T00:13:31.510562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 1: Beginner Programming\n",
      "============================================================\n",
      "\n",
      "19:13:27 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'I'm new to programming and want to start learning' (summaries=5, details=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:27 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:27 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: I'm new to programming and want to start learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:27 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:27 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:31 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: I'm new to programming and want to start learning\n",
      "\n",
      "\n",
      "AI Response:\n",
      "\n",
      "It seems that there are currently no specific courses available that match your interests in programming and technology at a beginner level. However, I encourage you to explore other resources and platforms that may offer foundational programming courses. \n",
      "\n",
      "Since you are a freshman and undeclared in your major, this could also be a great opportunity to explore introductory courses in computer science or programming languages like Python, JavaScript, or HTML/CSS, if you come across them in the future.\n",
      "\n",
      "If you have any other interests or specific areas you'd like to explore further, please let me know, and I can assist you in finding resources that align with your goals! Keep up the enthusiasm for learning, and I'm here to support you!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Beginner looking for programming courses\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 1: Beginner Programming\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "query1 = \"I'm new to programming and want to start learning\"\n",
    "profile1 = {\n",
    "    \"name\": \"Maria Garcia\",\n",
    "    \"major\": \"Undeclared\",\n",
    "    \"year\": \"Freshman\",\n",
    "    \"interests\": [\"programming\", \"technology\"],\n",
    "    \"preferred_difficulty\": \"beginner\",\n",
    "    \"preferred_format\": \"online\",\n",
    "}\n",
    "\n",
    "response1, summaries1, details1 = await hierarchical_rag_query(query1, profile1)\n",
    "print(f\"\\nQuery: {query1}\\n\")\n",
    "print(\"\\nAI Response:\\n\")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "417125fb31c8cce8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:31.514625Z",
     "iopub.status.busy": "2025-12-05T00:13:31.514383Z",
     "iopub.status.idle": "2025-12-05T00:13:34.169899Z",
     "shell.execute_reply": "2025-12-05T00:13:34.168831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 2: Advanced Machine Learning\n",
      "============================================================\n",
      "\n",
      "19:13:31 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'I want advanced courses in machine learning and AI' (summaries=5, details=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:31 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:31 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: I want advanced courses in machine learning and AI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:31 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:31 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:34 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: I want advanced courses in machine learning and AI\n",
      "\n",
      "\n",
      "AI Response:\n",
      "\n",
      "It seems that there are currently no advanced courses in machine learning and AI available in the provided course list. However, I encourage you to explore other related topics or perhaps consider foundational courses to enhance your understanding, which could benefit your research in the future.\n",
      "\n",
      "If there are any specific subjects or other areas of interest you would like me to look into, please let me know! I'm here to help you find the right path that aligns with your goals.\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Advanced student looking for specialized courses\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 2: Advanced Machine Learning\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "query2 = \"I want advanced courses in machine learning and AI\"\n",
    "profile2 = {\n",
    "    \"name\": \"David Kim\",\n",
    "    \"major\": \"Computer Science\",\n",
    "    \"year\": \"Senior\",\n",
    "    \"interests\": [\"machine learning\", \"AI\", \"research\"],\n",
    "    \"preferred_difficulty\": \"advanced\",\n",
    "    \"preferred_format\": \"in-person\",\n",
    "}\n",
    "\n",
    "response2, summaries2, details2 = await hierarchical_rag_query(query2, profile2)\n",
    "print(f\"\\nQuery: {query2}\\n\")\n",
    "print(\"\\nAI Response:\\n\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dcabdc1ac4eba56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:34.172511Z",
     "iopub.status.busy": "2025-12-05T00:13:34.172320Z",
     "iopub.status.idle": "2025-12-05T00:13:42.665480Z",
     "shell.execute_reply": "2025-12-05T00:13:42.664504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 3: Business Analytics\n",
      "============================================================\n",
      "\n",
      "19:13:34 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'What courses can help me with business analytics and decision making?' (summaries=5, details=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:35 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:35 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: What courses can help me with business analytics and decision making?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:35 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:35 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:42 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What courses can help me with business analytics and decision making?\n",
      "\n",
      "\n",
      "\n",
      "AI Response:\n",
      "\n",
      "It seems that there are currently no courses listed that specifically align with business analytics and decision-making. However, I would encourage you to explore related subjects that may complement your interests in analytics, management, and strategy.\n",
      "\n",
      "Since you're a junior majoring in Business Administration and prefer intermediate-level courses in a hybrid format, I recommend considering courses that might not be directly labeled as \"business analytics\" but could provide valuable skills and knowledge in that area. \n",
      "\n",
      "Keep an eye on future course offerings or reach out to your academic advisor to see if there are upcoming courses that match your interests. In the meantime, you might also want to explore independent study opportunities or online resources to enhance your analytics skills.\n",
      "\n",
      "Stay motivated, Jennifer! Your interest in analytics will greatly benefit your future career in business.\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Business student looking for relevant courses\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 3: Business Analytics\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "query3 = \"What courses can help me with business analytics and decision making?\"\n",
    "profile3 = {\n",
    "    \"name\": \"Jennifer Lee\",\n",
    "    \"major\": \"Business Administration\",\n",
    "    \"year\": \"Junior\",\n",
    "    \"interests\": [\"analytics\", \"management\", \"strategy\"],\n",
    "    \"preferred_difficulty\": \"intermediate\",\n",
    "    \"preferred_format\": \"hybrid\",\n",
    "}\n",
    "\n",
    "response3, summaries3, details3 = await hierarchical_rag_query(query3, profile3)\n",
    "print(f\"\\nQuery: {query3}\\n\")\n",
    "print()\n",
    "print(\"\\nAI Response:\\n\")\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454bafd627df17e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Context Quality Matters\n",
    "\n",
    "### Why Quality Engineering is Essential\n",
    "\n",
    "You've built a working RAG system - congratulations! But there's a critical question: **What makes context \"good\"?**\n",
    "\n",
    "Context engineering is real engineering - it requires the same rigor, analysis, and deliberate decision-making as any other engineering discipline. Let's see why this matters with a concrete example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc981190c788ca",
   "metadata": {},
   "source": [
    "### Example: The Impact of Poor vs. Well-Engineered Context\n",
    "\n",
    "Let's see what happens when we don't engineer our context properly.\n",
    "\n",
    "**Scenario:** A student asks about machine learning courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e16742ea94b81c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:42.668051Z",
     "iopub.status.busy": "2025-12-05T00:13:42.667808Z",
     "iopub.status.idle": "2025-12-05T00:13:43.116109Z",
     "shell.execute_reply": "2025-12-05T00:13:43.115665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:42 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'course' (summaries=10, details=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:42 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:42 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: course\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:42 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:42 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå POOR CONTEXT (Naive Approach):\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "Courses: 0 (unfiltered - may not be relevant)\n",
      "Tokens: 1\n",
      "Format: Raw JSON with all fields\n",
      "\n",
      "Sample:\n",
      "[]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-4o-mini\") -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Poor context: Raw JSON dump (what we might do naively)\n",
    "# Get first 10 courses using a broad search\n",
    "poor_summaries, _ = await hierarchical_manager.hierarchical_search(\n",
    "    query=\"course\",\n",
    "    summary_limit=10,\n",
    "    detail_limit=0\n",
    ")\n",
    "\n",
    "poor_context = json.dumps(\n",
    "    [\n",
    "        {\n",
    "            \"course_code\": s.course_code,\n",
    "            \"title\": s.title,\n",
    "            \"summary\": s.summary,\n",
    "            \"department\": s.department,\n",
    "            \"difficulty_level\": s.difficulty_level.value,\n",
    "            \"format\": s.format.value,\n",
    "        }\n",
    "        for s in poor_summaries\n",
    "    ],\n",
    "    indent=2,\n",
    ")\n",
    "\n",
    "poor_tokens = count_tokens(poor_context)\n",
    "\n",
    "print(f\"\"\"‚ùå POOR CONTEXT (Naive Approach):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Courses: {len(poor_summaries)} (unfiltered - may not be relevant)\n",
    "Tokens: {poor_tokens:,}\n",
    "Format: Raw JSON with all fields\n",
    "\n",
    "Sample:\n",
    "{poor_context[:400]}...\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58feb3a545f174a4",
   "metadata": {},
   "source": [
    "Now let's compare with well-engineered context using our hierarchical RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87cf55e7aa8ef56b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:43.117183Z",
     "iopub.status.busy": "2025-12-05T00:13:43.117106Z",
     "iopub.status.idle": "2025-12-05T00:13:43.790449Z",
     "shell.execute_reply": "2025-12-05T00:13:43.789323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:43 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'What machine learning courses are available?' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:43 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:43 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: What machine learning courses are available?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:43 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:43 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ WELL-ENGINEERED CONTEXT (Hierarchical RAG + Optimization):\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "Courses: 0 summaries + 0 details (filtered by semantic relevance)\n",
      "Tokens: 25\n",
      "Format: LLM-optimized text with progressive disclosure\n",
      "\n",
      "Context:\n",
      "# Course Search Results for: What machine learning courses are available?\n",
      "\n",
      "## Overview of All Matches\n",
      "\n",
      "Found 0 relevant courses:\n",
      "\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "Token Reduction: -24 tokens (-2400.0% reduction)\n",
      "Cost Savings: $-0.0001 per request\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Well-engineered context: Filtered + Optimized with hierarchical retrieval\n",
    "query = \"What machine learning courses are available?\"\n",
    "\n",
    "# Use our hierarchical RAG system to get relevant courses\n",
    "good_summaries, good_details = await hierarchical_manager.hierarchical_search(\n",
    "    query=query,\n",
    "    summary_limit=3,\n",
    "    detail_limit=2\n",
    ")\n",
    "\n",
    "# Use context assembler for LLM-friendly format\n",
    "well_engineered_context = context_assembler.assemble_hierarchical_context(\n",
    "    summaries=good_summaries,\n",
    "    details=good_details,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "good_tokens = count_tokens(well_engineered_context)\n",
    "\n",
    "print(f\"\"\"‚úÖ WELL-ENGINEERED CONTEXT (Hierarchical RAG + Optimization):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Courses: {len(good_summaries)} summaries + {len(good_details)} details (filtered by semantic relevance)\n",
    "Tokens: {good_tokens:,}\n",
    "Format: LLM-optimized text with progressive disclosure\n",
    "\n",
    "Context:\n",
    "{well_engineered_context}\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Token Reduction: {poor_tokens - good_tokens:,} tokens ({((poor_tokens - good_tokens) / poor_tokens * 100):.1f}% reduction)\n",
    "Cost Savings: ${((poor_tokens - good_tokens) / 1_000_000) * 2.50:.4f} per request\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5939de968b43a",
   "metadata": {},
   "source": [
    "### The Difference in LLM Responses\n",
    "\n",
    "Let's see how context quality affects the actual responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd4728cf6c00e3ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:43.792806Z",
     "iopub.status.busy": "2025-12-05T00:13:43.792607Z",
     "iopub.status.idle": "2025-12-05T00:13:46.042576Z",
     "shell.execute_reply": "2025-12-05T00:13:46.041123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:46 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå RESPONSE WITH POOR CONTEXT:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "It appears that there are currently no machine learning courses available in the Redis University catalog. If you're interested in other topics or have specific subjects in mind, please let me know, and I can help you find relevant courses or resources!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with poor context\n",
    "messages_poor = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "Available Courses:\n",
    "{poor_context}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "\n",
    "response_poor = llm.invoke(messages_poor)\n",
    "\n",
    "print(f\"\"\"‚ùå RESPONSE WITH POOR CONTEXT:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "{response_poor.content}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1f304e889ad6ece",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:46.044823Z",
     "iopub.status.busy": "2025-12-05T00:13:46.044626Z",
     "iopub.status.idle": "2025-12-05T00:13:48.093735Z",
     "shell.execute_reply": "2025-12-05T00:13:48.092642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:48 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RESPONSE WITH WELL-ENGINEERED CONTEXT:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "It seems that there are currently no machine learning courses available in the Redis University course catalog. If you're looking for courses in machine learning, I recommend checking other educational platforms or institutions that may offer relevant programs. If you have any specific topics or skills in mind, I can help you find resources or suggest alternative learning paths!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with well-engineered context\n",
    "messages_good = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "{well_engineered_context}\n",
    "\n",
    "Help students find the best course for their needs.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "\n",
    "response_good = llm.invoke(messages_good)\n",
    "\n",
    "print(f\"\"\"‚úÖ RESPONSE WITH WELL-ENGINEERED CONTEXT:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "{response_good.content}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55176f04e022a55e",
   "metadata": {},
   "source": [
    "### Key Takeaways: Why Context Engineering Matters\n",
    "\n",
    "From this example, you can see that well-engineered context:\n",
    "\n",
    "1. **Reduces Token Usage** - 50-70% fewer tokens through filtering and optimization\n",
    "2. **Improves Relevance** - Semantic search finds the right courses\n",
    "3. **Enhances Response Quality** - LLM can focus on relevant information\n",
    "4. **Saves Money** - Fewer tokens = lower API costs\n",
    "5. **Scales Better** - Works with thousands of courses, not just 10\n",
    "\n",
    "**The Engineering Mindset:**\n",
    "- Context is data that requires engineering discipline\n",
    "- Raw data ‚â† Good context\n",
    "- Systematic transformation: Extract ‚Üí Clean ‚Üí Transform ‚Üí Optimize ‚Üí Store\n",
    "- Quality metrics: Relevance, Completeness, Efficiency, Accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e3ea459ac6e25",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "**1. RAG Fundamentals**\n",
    "- RAG dynamically retrieves relevant information instead of hardcoding knowledge\n",
    "- Vector embeddings enable semantic search (meaning-based, not keyword-based)\n",
    "- RAG solves the scalability and token efficiency problems of static context\n",
    "\n",
    "**2. The Hierarchical RAG Pipeline**\n",
    "```\n",
    "User Query ‚Üí Hierarchical Search ‚Üí Context Assembly ‚Üí LLM Generation\n",
    "```\n",
    "- **Retrieval:** Find relevant documents using vector similarity (summaries first)\n",
    "- **Progressive Disclosure:** Fetch full details only for top matches\n",
    "- **Assembly:** Combine system + user + retrieved context\n",
    "- **Generation:** LLM creates personalized response with full context\n",
    "\n",
    "**3. Context Engineering in Practice**\n",
    "- **System Context:** AI role and instructions (static)\n",
    "- **User Context:** Student profile and preferences (dynamic, user-specific)\n",
    "- **Retrieved Context:** Relevant courses from vector search (dynamic, query-specific)\n",
    "- **Integration:** All three context types work together\n",
    "\n",
    "**4. Technical Implementation**\n",
    "- **HierarchicalCourseManager**: Two-tier retrieval (summaries + details)\n",
    "- **HierarchicalContextAssembler**: Progressive disclosure context assembly\n",
    "- **Benefits**: Efficient token usage, better relevance, scalable architecture\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**Retrieval:**\n",
    "- Retrieve only what's needed (top-k results)\n",
    "- Use progressive disclosure (summaries ‚Üí details)\n",
    "- Balance between too few (missing info) and too many (wasting tokens) results\n",
    "- **üí° Research Insight:** Context Rot research shows that distractors (similar-but-wrong information) have amplified negative impact in long contexts. Precision in retrieval matters more than recall.\n",
    "\n",
    "**Context Assembly:**\n",
    "- Structure context clearly (system ‚Üí user ‚Üí retrieved)\n",
    "- Include only relevant metadata\n",
    "- Keep descriptions concise but informative\n",
    "- Use progressive disclosure for efficiency\n",
    "\n",
    "**Generation:**\n",
    "- Use appropriate temperature (0.7 for creative, 0.0 for factual)\n",
    "- Provide clear instructions in system context\n",
    "- Let the LLM explain its reasoning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b1c6835f5be13",
   "metadata": {},
   "source": [
    "## üöÄ What's Next?\n",
    "\n",
    "### üìä Module 3: Data Engineering for Context\n",
    "\n",
    "Now that you understand RAG fundamentals and why context quality matters, Module 3 teaches you to engineer context with production-level rigor:\n",
    "- Master data engineering workflows for context preparation\n",
    "- Understand chunking as a design choice (not a default)\n",
    "- Build production-ready context pipelines\n",
    "- Optimize context quality with systematic approaches\n",
    "\n",
    "### üß† Module 4: Memory Systems\n",
    "\n",
    "In this module, you built a RAG system that retrieves relevant information for each query. But there's a problem: **it doesn't remember previous conversations**.\n",
    "\n",
    "In Module 4, you'll add memory to your RAG system:\n",
    "- **Working Memory:** Track conversation history within a session\n",
    "- **Long-term Memory:** Remember user preferences across sessions\n",
    "- **Agent Memory Server:** Automatic memory extraction and retrieval\n",
    "\n",
    "### ü§ñ Module 5: Building Agents\n",
    "\n",
    "After adding memory, you'll transform your RAG system into a full agent:\n",
    "- **Tool Calling:** Let the AI use functions (search, enroll, check prerequisites)\n",
    "- **LangGraph State Management:** Orchestrate complex multi-step workflows\n",
    "- **Agent Reasoning:** Plan and execute multi-step tasks\n",
    "- **Production Patterns:** Error handling, retries, and monitoring\n",
    "\n",
    "### The Journey\n",
    "\n",
    "```\n",
    "Module 1: Context Engineering Fundamentals\n",
    "    ‚Üì\n",
    "Module 2: RAG Fundamentals ‚Üê You are here\n",
    "    ‚Üì\n",
    "Module 3: Data Engineering for Context ‚Üê Next\n",
    "    ‚Üì\n",
    "Module 4: Memory Systems\n",
    "    ‚Üì\n",
    "Module 5: Building Agents (Complete System)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05ca3885aaa51cc",
   "metadata": {},
   "source": [
    "## üí™ Practice Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "**Exercise 1: Custom Filters**\n",
    "- Modify the hierarchical search to filter by specific departments\n",
    "- Try combining multiple filters (difficulty + format + department)\n",
    "\n",
    "**Exercise 2: Adjust Retrieval**\n",
    "- Experiment with different `summary_limit` and `detail_limit` values\n",
    "- Observe how response quality changes with more/fewer retrieved courses\n",
    "\n",
    "**Exercise 3: Context Optimization**\n",
    "- Modify the `assemble_rag_context` function to include more/less detail\n",
    "- Measure token usage and response quality trade-offs\n",
    "\n",
    "**Exercise 4: Progressive Disclosure Tuning**\n",
    "- Try different ratios of summaries to details (5:2, 10:3, 3:1)\n",
    "- Measure token usage and response quality for each\n",
    "\n",
    "**Exercise 5: Evaluation**\n",
    "- Create test queries with expected results\n",
    "- Measure retrieval accuracy (are the right courses retrieved?)\n",
    "- Measure generation quality (are responses helpful and accurate?)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7555eead43793",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "You've built a complete hierarchical RAG system that:\n",
    "- ‚úÖ Uses two-tier retrieval (summaries + details)\n",
    "- ‚úÖ Performs semantic search to find relevant courses\n",
    "- ‚úÖ Applies progressive disclosure for efficient token usage\n",
    "- ‚úÖ Assembles context from multiple sources (system + user + retrieved)\n",
    "- ‚úÖ Generates personalized responses using LLMs\n",
    "- ‚úÖ Handles different query types and user profiles\n",
    "\n",
    "This RAG system is the foundation for the advanced topics in Modules 3, 4, and 5. You'll build on this exact code to add data engineering, memory, tools, and full agent capabilities.\n",
    "\n",
    "**Great work!** You've mastered Retrieved Context and built a production-ready hierarchical RAG pipeline. üéâ\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "### **RAG and Vector Search**\n",
    "- [Retrieval-Augmented Generation Paper](https://arxiv.org/abs/2005.11401) - Original RAG paper by Facebook AI\n",
    "- [Redis Vector Similarity Search](https://redis.io/docs/stack/search/reference/vectors/) - Official Redis VSS documentation\n",
    "- [RedisVL Documentation](https://redisvl.com/) - Redis Vector Library for Python\n",
    "- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/) - Building RAG applications\n",
    "\n",
    "### **Embeddings and Semantic Search**\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings) - Understanding text embeddings\n",
    "- [Sentence Transformers](https://www.sbert.net/) - Open-source embedding models\n",
    "- [HNSW Algorithm](https://arxiv.org/abs/1603.09320) - Hierarchical Navigable Small World graphs\n",
    "\n",
    "### **LangChain and Redis Integration**\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction) - Framework overview\n",
    "- [LangChain Redis Integration](https://python.langchain.com/docs/integrations/vectorstores/redis/) - Using Redis with LangChain\n",
    "- [Redis Python Client](https://redis-py.readthedocs.io/) - redis-py documentation\n",
    "\n",
    "### **Advanced RAG Techniques**\n",
    "- [Advanced RAG Patterns](https://blog.langchain.dev/deconstructing-rag/) - LangChain blog on RAG optimization\n",
    "- [Advanced Search with RedisVL](https://docs.redisvl.com/en/latest/user_guide/11_advanced_queries.html) - Vector, Hybrid, Text, and Keyword Search\n",
    "- [RAG Evaluation](https://arxiv.org/abs/2309.15217) - Measuring RAG system performance\n",
    "\n",
    "---\n",
    "\n",
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "**Redis University - Context Engineering Workshop**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb6b4aad0eba436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
