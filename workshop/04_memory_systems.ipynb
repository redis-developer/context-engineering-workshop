{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0bfa9cdcafeff8",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Module 4: Memory Systems\n",
    "\n",
    "**‚è±Ô∏è Estimated Time:** 75-90 minutes\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "\n",
    "1. **Understand** why memory is essential for context engineering\n",
    "2. **Implement** working memory for conversation continuity\n",
    "3. **Use** long-term memory for persistent user knowledge\n",
    "4. **Integrate** memory with your Module 2 RAG system\n",
    "5. **Build** a complete memory-enhanced course advisor\n",
    "6. **Combine** all four context types in a unified system\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Recap\n",
    "\n",
    "### **Module 1: The Four Context Types**\n",
    "\n",
    "Recall the four context types from Module 1:\n",
    "\n",
    "1. **System Context** (Static) - Role, instructions, guidelines\n",
    "2. **User Context** (Dynamic, User-Specific) - Profile, preferences, goals\n",
    "3. **Conversation Context** (Dynamic, Session-Specific) - **‚Üê Memory enables this!**\n",
    "4. **Retrieved Context** (Dynamic, Query-Specific) - RAG results\n",
    "\n",
    "### **Module 2: Stateless RAG**\n",
    "\n",
    "Your Module 2 RAG system was **stateless**:\n",
    "\n",
    "```python\n",
    "async def rag_query(query, student_profile):\n",
    "    # 1. Search courses (Retrieved Context)\n",
    "    courses = await course_manager.search_courses(query)\n",
    "\n",
    "    # 2. Assemble context (System + User + Retrieved)\n",
    "    context = assemble_context(system_prompt, student_profile, courses)\n",
    "\n",
    "    # 3. Generate response\n",
    "    response = llm.invoke(context)\n",
    "\n",
    "    # ‚ùå No conversation history stored\n",
    "    # ‚ùå Each query is independent\n",
    "    # ‚ùå Can't reference previous messages\n",
    "```\n",
    "\n",
    "**The Problem:** Every query starts from scratch. No conversation continuity.\n",
    "\n",
    "---\n",
    "\n",
    "## üö® Why Agents Need Memory: The Grounding Problem\n",
    "\n",
    "Before diving into implementation, let's understand the fundamental problem that memory solves.\n",
    "\n",
    "**Grounding** means understanding what users are referring to. Natural conversation is full of references:\n",
    "\n",
    "### **Without Memory:**\n",
    "\n",
    "```\n",
    "User: \"Tell me about CS401\"\n",
    "Agent: \"CS401 is Machine Learning. It covers supervised learning...\"\n",
    "\n",
    "User: \"What are its prerequisites?\"\n",
    "Agent: ‚ùå \"What does 'it' refer to? Please specify which course.\"\n",
    "\n",
    "User: \"The course we just discussed!\"\n",
    "Agent: ‚ùå \"I don't have access to previous messages. Which course?\"\n",
    "```\n",
    "\n",
    "**This is a terrible user experience.**\n",
    "\n",
    "### Types of References That Need Grounding\n",
    "\n",
    "**Pronouns:**\n",
    "- \"it\", \"that course\", \"those\", \"this one\"\n",
    "- \"he\", \"she\", \"they\" (referring to people)\n",
    "\n",
    "**Descriptions:**\n",
    "- \"the easy one\", \"the online course\"\n",
    "- \"my advisor\", \"that professor\"\n",
    "\n",
    "**Implicit context:**\n",
    "- \"Can I take it?\" ‚Üí Take what?\n",
    "- \"When does it start?\" ‚Üí What starts?\n",
    "\n",
    "**Temporal references:**\n",
    "- \"you mentioned\", \"earlier\", \"last time\"\n",
    "\n",
    "### **With Memory:**\n",
    "\n",
    "```\n",
    "User: \"Tell me about CS401\"\n",
    "Agent: \"CS401 is Machine Learning. It covers...\"\n",
    "[Stores: User asked about CS401]\n",
    "\n",
    "User: \"What are its prerequisites?\"\n",
    "Agent: [Checks memory: \"its\" = CS401]\n",
    "Agent: ‚úÖ \"CS401 requires CS201 and MATH301\"\n",
    "\n",
    "User: \"Can I take it?\"\n",
    "Agent: [Checks memory: \"it\" = CS401, checks student transcript]\n",
    "Agent: ‚úÖ \"You've completed CS201 but still need MATH301\"\n",
    "```\n",
    "\n",
    "**Now the conversation flows naturally!**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Two Types of Memory\n",
    "\n",
    "### **1. Working Memory (Session-Scoped)**\n",
    "\n",
    " - **What:** Conversation messages from the current session\n",
    " - **Purpose:** Reference resolution, conversation continuity\n",
    " - **Lifetime:** Persists for the session\n",
    " - **Storage:** Conversation remains accessible when you return to the same session\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Session: session_123\n",
    "Messages:\n",
    "  1. User: \"Tell me about CS401\"\n",
    "  2. Agent: \"CS401 is Machine Learning...\"\n",
    "  3. User: \"What are its prerequisites?\"\n",
    "  4. Agent: \"CS401 requires CS201 and MATH301\"\n",
    "```\n",
    "\n",
    "**Key Point:** Just like ChatGPT or Claude, when you return to a conversation, the working memory is still there. The conversation doesn't disappear!\n",
    "\n",
    "### **2. Long-term Memory (Cross-Session)**\n",
    "\n",
    " - **What:** Persistent knowledge (user preferences, domain facts, business rules)\n",
    " - **Purpose:** Personalization AND consistent application behavior across sessions\n",
    " - **Lifetime:** Permanent (until explicitly deleted)\n",
    " - **Scope:** Can be user-specific OR application-wide\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "**User-Scoped (Personalization):**\n",
    "```\n",
    "User: student_sarah\n",
    "  - \"Prefers online courses over in-person\"\n",
    "  - \"Major: Computer Science, focus on AI/ML\"\n",
    "  - \"Goal: Graduate Spring 2026\"\n",
    "  - \"Completed: CS101, CS201, MATH301\"\n",
    "```\n",
    "\n",
    "**Application-Scoped (Domain Knowledge):**\n",
    "```\n",
    "Domain: course_requirements\n",
    "  - \"CS401 requires CS201 as prerequisite\"\n",
    "  - \"Maximum course load is 18 credits per semester\"\n",
    "  - \"Registration opens 2 weeks before semester start\"\n",
    "  - \"Lab courses require campus attendance\"\n",
    "```\n",
    "\n",
    "### **Comparison: Working vs. Long-term Memory**\n",
    "\n",
    "| Working Memory | Long-term Memory |\n",
    "|----------------|------------------|\n",
    "| **Session-scoped** | **User-scoped OR Application-scoped** |\n",
    "| Current conversation | Important facts, rules, knowledge |\n",
    "| Persists for session | Persists across sessions |\n",
    "| Full message history | Extracted knowledge (user + domain) |\n",
    "| Loaded/saved each turn | Searched when needed |\n",
    "| **Challenge:** Context window limits | **Challenge:** Storage growth |\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Setup and Environment\n",
    "\n",
    "Let's set up our environment with the necessary dependencies and connections. We'll build on Module 2's RAG foundation and add memory capabilities.\n",
    "\n",
    "### ‚ö†Ô∏è Prerequisites\n",
    "\n",
    "**Before running this notebook, make sure you have:**\n",
    "\n",
    "1. **Docker Desktop running** - Required for Redis and Agent Memory Server\n",
    "\n",
    "2. **Environment variables** - Create a `.env` file in the `reference-agent` directory:\n",
    "   ```bash\n",
    "   # Copy the example file\n",
    "   cd ../reference-agent\n",
    "   cp .env.example .env\n",
    "\n",
    "   # Edit .env and add your OpenAI API key\n",
    "   # OPENAI_API_KEY=your_actual_openai_api_key_here\n",
    "   ```\n",
    "\n",
    "3. **Run the setup script** - This will automatically start Redis and Agent Memory Server:\n",
    "   ```bash\n",
    "   cd ../reference-agent\n",
    "   python setup_agent_memory_server.py\n",
    "   ```\n",
    "\n",
    "**Note:** The setup script will:\n",
    "- ‚úÖ Check if Docker is running\n",
    "- ‚úÖ Start Redis if not running (port 6379)\n",
    "- ‚úÖ Start Agent Memory Server if not running (port 8088)\n",
    "- ‚úÖ Verify Redis connection is working\n",
    "- ‚úÖ Handle any configuration issues automatically\n",
    "\n",
    "If the Memory Server is not available, the notebook will skip memory-related demos but will still run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133103c1578afd1f",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf596157ac1dd8",
   "metadata": {},
   "source": [
    "### Automated Setup Check\n",
    "\n",
    "Let's run the setup script to ensure all services are running properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd4ba605c55ba49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:16:59.332801Z",
     "iopub.status.busy": "2025-12-05T00:16:59.332519Z",
     "iopub.status.idle": "2025-12-05T00:16:59.343711Z",
     "shell.execute_reply": "2025-12-05T00:16:59.342993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Setup script not found. Please ensure services are running manually.\n"
     ]
    }
   ],
   "source": [
    "# Run the setup script to ensure Redis and Agent Memory Server are running\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to setup script\n",
    "setup_script = Path(\"../reference-agent/setup_agent_memory_server.py\")\n",
    "\n",
    "if setup_script.exists():\n",
    "    print(\"Running automated setup check...\\n\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(setup_script)], capture_output=True, text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(\"‚ö†Ô∏è  Setup check failed. Please review the output above.\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All services are ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Setup script not found. Please ensure services are running manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56417b09adcbba74",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40618e4a602328df",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "If you haven't already installed the reference-agent package, uncomment and run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6af962b1933f405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:16:59.345662Z",
     "iopub.status.busy": "2025-12-05T00:16:59.345489Z",
     "iopub.status.idle": "2025-12-05T00:16:59.347941Z",
     "shell.execute_reply": "2025-12-05T00:16:59.347319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to install reference-agent package\n",
    "# %pip install -q -e ../reference-agent\n",
    "\n",
    "# Uncomment to install agent-memory-client\n",
    "# %pip install -q agent-memory-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0c713c97986b5",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n",
    "\n",
    "We'll load environment variables from the `.env` file in the `reference-agent` directory.\n",
    "\n",
    "**Required variables:**\n",
    "- `OPENAI_API_KEY` - Your OpenAI API key\n",
    "- `REDIS_URL` - Redis connection URL (default: redis://localhost:6379)\n",
    "- `AGENT_MEMORY_URL` - Agent Memory Server URL (default: http://localhost:8088)\n",
    "\n",
    "If you haven't created the `.env` file yet, copy `.env.example` and add your OpenAI API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3553025fb912d807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:16:59.349663Z",
     "iopub.status.busy": "2025-12-05T00:16:59.349530Z",
     "iopub.status.idle": "2025-12-05T00:16:59.358886Z",
     "shell.execute_reply": "2025-12-05T00:16:59.358322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded\n",
      "   REDIS_URL: redis://localhost:6379\n",
      "   AGENT_MEMORY_URL: http://localhost:8088\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Handle both running from workshop/ directory and from project root\n",
    "if Path.cwd().name == \"workshop\":\n",
    "    project_root = Path.cwd().parent\n",
    "else:\n",
    "    project_root = Path.cwd()\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables from project root first, then reference-agent\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Also try reference-agent .env for memory-specific settings\n",
    "ref_agent_env = project_root / \"reference-agent\" / \".env\"\n",
    "if ref_agent_env.exists():\n",
    "    load_dotenv(dotenv_path=ref_agent_env, override=False)\n",
    "\n",
    "# Verify required environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "AGENT_MEMORY_URL = os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\n",
    "        f\"\"\"‚ùå OPENAI_API_KEY not found!\n",
    "\n",
    "    Please create a .env file at: {env_path.absolute()}\n",
    "\n",
    "    With the following content:\n",
    "    OPENAI_API_KEY=your_openai_api_key\n",
    "    REDIS_URL=redis://localhost:6379\n",
    "    AGENT_MEMORY_URL=http://localhost:8088\n",
    "    \"\"\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"\"\"‚úÖ Environment variables loaded\n",
    "   REDIS_URL: {REDIS_URL}\n",
    "   AGENT_MEMORY_URL: {AGENT_MEMORY_URL}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab040509ba57335",
   "metadata": {},
   "source": [
    "### Import Core Libraries\n",
    "\n",
    "We'll import standard Python libraries and async support for our memory operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a8fa629dc6e060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:16:59.360354Z",
     "iopub.status.busy": "2025-12-05T00:16:59.360233Z",
     "iopub.status.idle": "2025-12-05T00:16:59.363724Z",
     "shell.execute_reply": "2025-12-05T00:16:59.363173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core libraries imported\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested event loops (required for Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"‚úÖ Core libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170863b81a0d631c",
   "metadata": {},
   "source": [
    "### Import Module 2 Components\n",
    "\n",
    "We're building on Module 2's RAG foundation, so we'll reuse the same components:\n",
    "- `redis_config` - Redis connection and configuration\n",
    "- `HierarchicalCourseManager` - Two-tier course search (summaries + details)\n",
    "- `HierarchicalContextAssembler` - Progressive disclosure context assembly\n",
    "- `StudentProfile` and other models - Data structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44dd307e2471f20f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:16:59.365004Z",
     "iopub.status.busy": "2025-12-05T00:16:59.364919Z",
     "iopub.status.idle": "2025-12-05T00:17:01.555821Z",
     "shell.execute_reply": "2025-12-05T00:17:01.555409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Module 2 components imported\n",
      "   HierarchicalCourseManager: Available\n",
      "   HierarchicalContextAssembler: Available\n",
      "   Redis Config: Available\n",
      "   Models: Course, StudentProfile, etc.\n"
     ]
    }
   ],
   "source": [
    "from redis_context_course.hierarchical_context import HierarchicalContextAssembler\n",
    "from redis_context_course.hierarchical_manager import HierarchicalCourseManager\n",
    "from redis_context_course.models import (\n",
    "    Course,\n",
    "    CourseFormat,\n",
    "    DifficultyLevel,\n",
    "    Semester,\n",
    "    StudentProfile,\n",
    ")\n",
    "\n",
    "# Import Redis configuration from reference-agent\n",
    "from redis_context_course.redis_config import redis_config\n",
    "\n",
    "print(\"\"\"‚úÖ Module 2 components imported\n",
    "   HierarchicalCourseManager: Available\n",
    "   HierarchicalContextAssembler: Available\n",
    "   Redis Config: Available\n",
    "   Models: Course, StudentProfile, etc.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87d8e7727dfad4",
   "metadata": {},
   "source": [
    "### Import LangChain Components\n",
    "\n",
    "We'll use LangChain for LLM interaction and message handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93f219d77f6c72e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:01.557129Z",
     "iopub.status.busy": "2025-12-05T00:17:01.557003Z",
     "iopub.status.idle": "2025-12-05T00:17:01.558994Z",
     "shell.execute_reply": "2025-12-05T00:17:01.558522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain components imported\n",
      "   ChatOpenAI: Available\n",
      "   Message types: HumanMessage, SystemMessage, AIMessage\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "print(\"\"\"‚úÖ LangChain components imported\n",
    "   ChatOpenAI: Available\n",
    "   Message types: HumanMessage, SystemMessage, AIMessage\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75782c53d70e86",
   "metadata": {},
   "source": [
    "### Import Agent Memory Server Client\n",
    "\n",
    "The Agent Memory Server provides production-ready memory management. If it's not available, we'll note that and continue with limited functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3964be15af5b99a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:01.560010Z",
     "iopub.status.busy": "2025-12-05T00:17:01.559935Z",
     "iopub.status.idle": "2025-12-05T00:17:01.561908Z",
     "shell.execute_reply": "2025-12-05T00:17:01.561603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent Memory Server client available\n",
      "   MemoryAPIClient: Ready\n",
      "   Memory models: WorkingMemory, MemoryMessage, ClientMemoryRecord\n"
     ]
    }
   ],
   "source": [
    "# Import Agent Memory Server client\n",
    "try:\n",
    "    from agent_memory_client import MemoryAPIClient, MemoryClientConfig\n",
    "    from agent_memory_client.models import (\n",
    "        ClientMemoryRecord,\n",
    "        MemoryMessage,\n",
    "        WorkingMemory,\n",
    "    )\n",
    "\n",
    "    MEMORY_SERVER_AVAILABLE = True\n",
    "    print(\"\"\"‚úÖ Agent Memory Server client available\n",
    "   MemoryAPIClient: Ready\n",
    "   Memory models: WorkingMemory, MemoryMessage, ClientMemoryRecord\"\"\")\n",
    "except ImportError:\n",
    "    MEMORY_SERVER_AVAILABLE = False\n",
    "    print(\"\"\"‚ö†Ô∏è  Agent Memory Server not available\n",
    "   Install with: pip install agent-memory-client\n",
    "   Start server: See reference-agent/README.md\n",
    "   Note: Some demos will be skipped\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598aa34146b409fe",
   "metadata": {},
   "source": [
    "### What We Just Did\n",
    "\n",
    "We've successfully set up our environment with all the necessary components:\n",
    "\n",
    "**Imported:**\n",
    "- ‚úÖ Module 2 RAG components (`HierarchicalCourseManager`, `HierarchicalContextAssembler`, `redis_config`, models)\n",
    "- ‚úÖ LangChain for LLM interaction\n",
    "- ‚úÖ Agent Memory Server client (if available)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Building on Module 2's foundation (not starting from scratch)\n",
    "- Using progressive disclosure pattern (summaries ‚Üí details)\n",
    "- Agent Memory Server provides scalable, persistent memory\n",
    "- Same Redis University domain for consistency\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Initialize Components\n",
    "\n",
    "Now let's initialize the components we'll use throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdd177e25e295d",
   "metadata": {},
   "source": [
    "### Initialize Redis Connection\n",
    "\n",
    "First, let's connect to Redis using the same configuration from Module 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba657dd4a1d0857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:01.563001Z",
     "iopub.status.busy": "2025-12-05T00:17:01.562931Z",
     "iopub.status.idle": "2025-12-05T00:17:01.564599Z",
     "shell.execute_reply": "2025-12-05T00:17:01.564234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Redis connection established\n",
      "   URL: redis://localhost:6379\n",
      "   Ready for vector operations\n"
     ]
    }
   ],
   "source": [
    "# Initialize Redis connection (redis_config.redis_client is a property)\n",
    "redis_client = redis_config.redis_client\n",
    "\n",
    "print(f\"\"\"‚úÖ Redis connection established\n",
    "   URL: {REDIS_URL}\n",
    "   Ready for vector operations\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0f52a01d8223c",
   "metadata": {},
   "source": [
    "### Initialize Hierarchical Course Manager\n",
    "\n",
    "The `HierarchicalCourseManager` provides two-tier retrieval:\n",
    "- **Tier 1:** Course summaries (lightweight, for search)\n",
    "- **Tier 2:** Full course details (on-demand)\n",
    "\n",
    "This is the same progressive disclosure pattern from Module 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5ec5b919d7cb17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:01.565422Z",
     "iopub.status.busy": "2025-12-05T00:17:01.565357Z",
     "iopub.status.idle": "2025-12-05T00:17:01.567135Z",
     "shell.execute_reply": "2025-12-05T00:17:01.566798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hierarchical Course Manager initialized\n",
      "   Two-tier retrieval: summaries ‚Üí details\n",
      "   Progressive disclosure pattern ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize Hierarchical Course Manager\n",
    "hierarchical_manager = HierarchicalCourseManager(redis_client=redis_client)\n",
    "context_assembler = HierarchicalContextAssembler()\n",
    "\n",
    "print(\"\"\"‚úÖ Hierarchical Course Manager initialized\n",
    "   Two-tier retrieval: summaries ‚Üí details\n",
    "   Progressive disclosure pattern ready\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a268c8a79e1b8",
   "metadata": {},
   "source": [
    "### Initialize LLM\n",
    "\n",
    "We'll use GPT-4o with temperature=0.0 for consistent, deterministic responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1191791c5443e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:01.567999Z",
     "iopub.status.busy": "2025-12-05T00:17:01.567936Z",
     "iopub.status.idle": "2025-12-05T00:17:01.688917Z",
     "shell.execute_reply": "2025-12-05T00:17:01.688546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized (GPT-4o)\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "\n",
    "print(\"‚úÖ LLM initialized (GPT-4o)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f22bb2fcb03f7",
   "metadata": {},
   "source": [
    "### Initialize Memory Client\n",
    "\n",
    "If the Agent Memory Server is available, we'll initialize the memory client. This client handles both working memory (conversation history) and long-term memory (persistent facts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e59ce92b366fb180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:01.690044Z",
     "iopub.status.busy": "2025-12-05T00:17:01.689969Z",
     "iopub.status.idle": "2025-12-05T00:17:01.695040Z",
     "shell.execute_reply": "2025-12-05T00:17:01.694661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory Client initialized\n",
      "   Base URL: http://localhost:8088\n",
      "   Namespace: redis_university\n",
      "   Ready for working memory and long-term memory operations\n"
     ]
    }
   ],
   "source": [
    "# Initialize Memory Client\n",
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    config = MemoryClientConfig(\n",
    "        base_url=AGENT_MEMORY_URL, default_namespace=\"redis_university\"\n",
    "    )\n",
    "    memory_client = MemoryAPIClient(config=config)\n",
    "    print(f\"\"\"‚úÖ Memory Client initialized\n",
    "   Base URL: {config.base_url}\n",
    "   Namespace: {config.default_namespace}\n",
    "   Ready for working memory and long-term memory operations\"\"\")\n",
    "else:\n",
    "    memory_client = None\n",
    "    print(\"\"\"‚ö†Ô∏è  Memory Server not available\n",
    "   Running with limited functionality\n",
    "   Some demos will be skipped\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2dbb1498e13fc",
   "metadata": {},
   "source": [
    "### Create Sample Student Profile\n",
    "\n",
    "We'll create a sample student profile to use throughout our demos. This follows the same pattern from Module 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe37f77d536494e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:01.696003Z",
     "iopub.status.busy": "2025-12-05T00:17:01.695928Z",
     "iopub.status.idle": "2025-12-05T00:17:01.699634Z",
     "shell.execute_reply": "2025-12-05T00:17:01.699203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Student profile created\n",
      "   Name: Sarah Chen\n",
      "   Major: Computer Science\n",
      "   Year: 2\n",
      "   Interests: machine learning, data science, algorithms\n",
      "   Completed: CS101, CS201\n",
      "   Preferred Format: online\n"
     ]
    }
   ],
   "source": [
    "# Create sample student profile\n",
    "sarah = StudentProfile(\n",
    "    name=\"Sarah Chen\",\n",
    "    email=\"sarah.chen@university.edu\",\n",
    "    major=\"Computer Science\",\n",
    "    year=2,\n",
    "    interests=[\"machine learning\", \"data science\", \"algorithms\"],\n",
    "    completed_courses=[\"CS101\", \"CS201\"],\n",
    "    current_courses=[\"MATH301\"],\n",
    "    preferred_format=CourseFormat.ONLINE,\n",
    "    preferred_difficulty=DifficultyLevel.INTERMEDIATE,\n",
    ")\n",
    "\n",
    "print(f\"\"\"‚úÖ Student profile created\n",
    "   Name: {sarah.name}\n",
    "   Major: {sarah.major}\n",
    "   Year: {sarah.year}\n",
    "   Interests: {', '.join(sarah.interests)}\n",
    "   Completed: {', '.join(sarah.completed_courses)}\n",
    "   Preferred Format: {sarah.preferred_format.value}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa912e1bd2e6235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:01.700591Z",
     "iopub.status.busy": "2025-12-05T00:17:01.700525Z",
     "iopub.status.idle": "2025-12-05T00:17:01.702403Z",
     "shell.execute_reply": "2025-12-05T00:17:01.702031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ INITIALIZATION SUMMARY\n",
      "\n",
      "‚úÖ Redis Connection: Ready\n",
      "‚úÖ Hierarchical Course Manager: Ready (two-tier retrieval)\n",
      "‚úÖ Context Assembler: Ready (progressive disclosure)\n",
      "‚úÖ LLM (GPT-4o): Ready\n",
      "‚úÖ Memory Client: Ready\n",
      "‚úÖ Student Profile: Sarah Chen\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"üéØ INITIALIZATION SUMMARY\n",
    "\n",
    "‚úÖ Redis Connection: Ready\n",
    "‚úÖ Hierarchical Course Manager: Ready (two-tier retrieval)\n",
    "‚úÖ Context Assembler: Ready (progressive disclosure)\n",
    "‚úÖ LLM (GPT-4o): Ready\n",
    "{'‚úÖ' if MEMORY_SERVER_AVAILABLE else '‚ö†Ô∏è '} Memory Client: {'Ready' if MEMORY_SERVER_AVAILABLE else 'Not Available'}\n",
    "‚úÖ Student Profile: {sarah.name}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59105c5db8d2eade",
   "metadata": {},
   "source": [
    "### Initialization Done\n",
    "\n",
    "üìã **What We're Building On:**\n",
    "- Module 2's RAG foundation (`HierarchicalCourseManager`, `redis_config`)\n",
    "- Same `StudentProfile` model\n",
    "- Same Redis configuration\n",
    "- Progressive disclosure pattern (summaries ‚Üí details)\n",
    "\n",
    "‚ú® **What We're Adding:**\n",
    "- Memory Client for conversation history\n",
    "- Working Memory for session context\n",
    "- Long-term Memory for persistent knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe569a41967dd57d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Part 1: Working Memory Fundamentals\n",
    "\n",
    "### **What is Working Memory?**\n",
    "\n",
    "Working memory stores **conversation messages** for the current session. It enables:\n",
    "\n",
    "- ‚úÖ **Reference resolution** - \"it\", \"that course\", \"the one you mentioned\"\n",
    "- ‚úÖ **Context continuity** - Each message builds on previous messages\n",
    "- ‚úÖ **Natural conversations** - Users don't repeat themselves\n",
    "\n",
    "### **How It Works:**\n",
    "\n",
    "```\n",
    "Turn 1: Load working memory (empty) ‚Üí Process query ‚Üí Save messages\n",
    "Turn 2: Load working memory (1 exchange) ‚Üí Process query ‚Üí Save messages\n",
    "Turn 3: Load working memory (2 exchanges) ‚Üí Process query ‚Üí Save messages\n",
    "```\n",
    "\n",
    "Each turn has access to all previous messages in the session.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Hands-On: Working Memory in Action\n",
    "\n",
    "Let's simulate a multi-turn conversation with working memory. We'll break this down step-by-step to see how working memory enables natural conversation flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda6d53263e2372",
   "metadata": {},
   "source": [
    "### Setup: Create Session and Student IDs\n",
    "\n",
    "Now that we have our components initialized, let's create session and student identifiers for our working memory demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "473224677f6d5a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:01.703218Z",
     "iopub.status.busy": "2025-12-05T00:17:01.703158Z",
     "iopub.status.idle": "2025-12-05T00:17:01.704953Z",
     "shell.execute_reply": "2025-12-05T00:17:01.704575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Working Memory Demo Setup\n",
      "   Student ID: sarah.chen\n",
      "   Session ID: session_sarah.chen_demo\n",
      "   Ready to demonstrate multi-turn conversation\n"
     ]
    }
   ],
   "source": [
    "# Setup for working memory demo\n",
    "student_id = sarah.email.split(\"@\")[0]  # \"sarah.chen\"\n",
    "session_id = f\"session_{student_id}_demo\"\n",
    "\n",
    "print(f\"\"\"üéØ Working Memory Demo Setup\n",
    "   Student ID: {student_id}\n",
    "   Session ID: {session_id}\n",
    "   Ready to demonstrate multi-turn conversation\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833f8d7a24e241b",
   "metadata": {},
   "source": [
    "### Turn 1: Initial Query\n",
    "\n",
    "Let's start with a simple query about a course. This is the first turn, so working memory will be empty.\n",
    "\n",
    "We'll break this down into clear steps:\n",
    "1. Load working memory (will be empty on first turn)\n",
    "2. Search for courses using hierarchical retrieval\n",
    "3. Generate a response\n",
    "4. Save the conversation to working memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deed83ce5cd46fe",
   "metadata": {},
   "source": [
    "#### Step 1: Set up the user query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7eb329361c4dead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:01.705814Z",
     "iopub.status.busy": "2025-12-05T00:17:01.705755Z",
     "iopub.status.idle": "2025-12-05T00:17:01.707387Z",
     "shell.execute_reply": "2025-12-05T00:17:01.706983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìç TURN 1: User asks about a course\n",
      "================================================================================\n",
      "\n",
      "üë§ User: Tell me about machine learning courses\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìç TURN 1: User asks about a course\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define the user's query\n",
    "turn1_query = \"Tell me about machine learning courses\"\n",
    "print(f\"\\nüë§ User: {turn1_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7f40c8923de74",
   "metadata": {},
   "source": [
    "#### Step 2: Load working memory\n",
    "\n",
    "On the first turn, working memory will be empty since this is a new session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5d6fbb76fd35682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:01.708254Z",
     "iopub.status.busy": "2025-12-05T00:17:01.708161Z",
     "iopub.status.idle": "2025-12-05T00:17:02.689841Z",
     "shell.execute_reply": "2025-12-05T00:17:02.688575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:02 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Working Memory Status:\n",
      "   Messages in memory: 6\n",
      "   Status: Has history\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    # Load working memory (empty for first turn)\n",
    "    _, turn1_working_memory = await memory_client.get_or_create_working_memory(\n",
    "        session_id=session_id, user_id=student_id, model_name=\"gpt-4o\"\n",
    "    )\n",
    "\n",
    "    print(f\"\"\"üìä Working Memory Status:\n",
    "   Messages in memory: {len(turn1_working_memory.messages)}\n",
    "   Status: {'Empty (first turn)' if len(turn1_working_memory.messages) == 0 else 'Has history'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9cdf56f6d3481d",
   "metadata": {},
   "source": [
    "#### Step 3: Search for courses using hierarchical retrieval\n",
    "\n",
    "Use the hierarchical manager to search for courses. This uses the progressive disclosure pattern:\n",
    "- First, get summaries (lightweight)\n",
    "- Then, fetch details for top results (on-demand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9b5f90d38dc581b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:02.692114Z",
     "iopub.status.busy": "2025-12-05T00:17:02.691993Z",
     "iopub.status.idle": "2025-12-05T00:17:03.279262Z",
     "shell.execute_reply": "2025-12-05T00:17:03.278346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for courses using hierarchical retrieval...\n",
      "19:17:02 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'Tell me about machine learning courses' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:03 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:03 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:03 redis_context_course.hierarchical_manager INFO   Created summary index: course_summaries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:03 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: Tell me about machine learning courses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:03 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:03 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 0 summaries, fetched 0 details\n",
      "   Progressive disclosure: summaries first, details on-demand\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Searching for courses using hierarchical retrieval...\")\n",
    "\n",
    "# Use hierarchical search (summaries + details)\n",
    "turn1_summaries, turn1_details = await hierarchical_manager.hierarchical_search(\n",
    "    query=turn1_query,\n",
    "    summary_limit=3,\n",
    "    detail_limit=2\n",
    ")\n",
    "\n",
    "print(f\"\"\"   Found {len(turn1_summaries)} summaries, fetched {len(turn1_details)} details\n",
    "   Progressive disclosure: summaries first, details on-demand\"\"\")\n",
    "\n",
    "# Show what we found\n",
    "if turn1_summaries:\n",
    "    print(\"\\n   üìã Course Summaries:\")\n",
    "    for i, summary in enumerate(turn1_summaries[:3], 1):\n",
    "        print(f\"      {i}. {summary.course_code}: {summary.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd73a8a3051d5ac",
   "metadata": {},
   "source": [
    "#### Step 4: Assemble context and generate response\n",
    "\n",
    "Use the context assembler to build context with progressive disclosure, then generate a response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bce7ff089f5f62b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:03.281193Z",
     "iopub.status.busy": "2025-12-05T00:17:03.281047Z",
     "iopub.status.idle": "2025-12-05T00:17:05.611675Z",
     "shell.execute_reply": "2025-12-05T00:17:05.610903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìù Context assembled (123 characters)\n",
      "\n",
      "üí≠ Generating response using LLM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:05 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: It seems there are no specific machine learning courses listed in the search results you provided. However, generally speaking, machine learning courses typically cover topics such as supervised and unsupervised learning, neural networks, deep learning, and data preprocessing. They often include practical components where students work on projects using programming languages like Python and tools such as TensorFlow or PyTorch. If you're interested in machine learning, you might want to explore online platforms like Coursera, edX, or Udacity, which offer a variety of courses ranging from beginner to advanced levels.\n"
     ]
    }
   ],
   "source": [
    "# Assemble context using progressive disclosure\n",
    "turn1_context = context_assembler.assemble_hierarchical_context(\n",
    "    summaries=turn1_summaries,\n",
    "    details=turn1_details,\n",
    "    query=turn1_query\n",
    ")\n",
    "\n",
    "print(f\"   üìù Context assembled ({len(turn1_context)} characters)\")\n",
    "\n",
    "# Build messages for LLM\n",
    "turn1_messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful course advisor. Answer questions about courses based on the provided information. Be concise but informative.\"\n",
    "    ),\n",
    "    HumanMessage(content=f\"{turn1_context}\\n\\nUser question: {turn1_query}\"),\n",
    "]\n",
    "\n",
    "# Generate response using LLM\n",
    "print(\"\\nüí≠ Generating response using LLM...\")\n",
    "turn1_response = llm.invoke(turn1_messages).content\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {turn1_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ed0939f36c7a7",
   "metadata": {},
   "source": [
    "#### Step 5: Save to working memory\n",
    "\n",
    "Add both the user query and assistant response to working memory for future turns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2fb37911816d8c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:05.613587Z",
     "iopub.status.busy": "2025-12-05T00:17:05.613431Z",
     "iopub.status.idle": "2025-12-05T00:17:05.633899Z",
     "shell.execute_reply": "2025-12-05T00:17:05.633205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:05 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved to working memory\n",
      "   Messages now in memory: 8\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    # Add messages to working memory\n",
    "    turn1_working_memory.messages.extend(\n",
    "        [\n",
    "            MemoryMessage(role=\"user\", content=turn1_query),\n",
    "            MemoryMessage(role=\"assistant\", content=turn1_response),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Save to Memory Server\n",
    "    await memory_client.put_working_memory(\n",
    "        session_id=session_id,\n",
    "        memory=turn1_working_memory,\n",
    "        user_id=student_id,\n",
    "        model_name=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    print(f\"\"\"\n",
    "‚úÖ Saved to working memory\n",
    "   Messages now in memory: {len(turn1_working_memory.messages)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42840aa50ad80557",
   "metadata": {},
   "source": [
    "### What Just Happened in Turn 1?\n",
    "\n",
    "**Initial State:**\n",
    "- Working memory was empty (first turn)\n",
    "- No conversation history available\n",
    "\n",
    "**Actions (RAG Pattern with Progressive Disclosure):**\n",
    "1. **Retrieve:** Used hierarchical search (summaries ‚Üí details)\n",
    "2. **Augment:** Assembled context with progressive disclosure\n",
    "3. **Generate:** LLM created a natural language response\n",
    "4. **Save:** Stored conversation in working memory\n",
    "\n",
    "**Result:**\n",
    "- Working memory now contains 2 messages (1 user, 1 assistant)\n",
    "- This history will be available for the next turn\n",
    "\n",
    "**Key Insight:** We used the same hierarchical retrieval pattern from Module 2, now combined with memory!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a231ce0fc6a748a",
   "metadata": {},
   "source": [
    "### Turn 2: Follow-up with Pronoun Reference\n",
    "\n",
    "Now let's ask a follow-up question using \"it\" - a pronoun that requires context from Turn 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5629b2461afd0da",
   "metadata": {},
   "source": [
    "#### Step 1: Set up the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c58976432bee9a56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:05.635800Z",
     "iopub.status.busy": "2025-12-05T00:17:05.635667Z",
     "iopub.status.idle": "2025-12-05T00:17:05.638693Z",
     "shell.execute_reply": "2025-12-05T00:17:05.638089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç TURN 2: User uses pronoun reference ('it')\n",
      "================================================================================\n",
      "\n",
      "üë§ User: What are the prerequisites for it?\n",
      "   Note: 'it' refers to a course from Turn 1\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìç TURN 2: User uses pronoun reference ('it')\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    turn2_query = \"What are the prerequisites for it?\"\n",
    "    print(f\"\\nüë§ User: {turn2_query}\")\n",
    "    print(\"   Note: 'it' refers to a course from Turn 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3007669c59ae5",
   "metadata": {},
   "source": [
    "#### Step 2: Load working memory\n",
    "\n",
    "This time, working memory will contain the conversation from Turn 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36310565dd00c237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:05.640240Z",
     "iopub.status.busy": "2025-12-05T00:17:05.640108Z",
     "iopub.status.idle": "2025-12-05T00:17:05.647957Z",
     "shell.execute_reply": "2025-12-05T00:17:05.647224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:05 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Working Memory Status:\n",
      "   Messages in memory: 8\n",
      "   Contains: Turn 1 conversation\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    # Load working memory (now has 1 exchange from Turn 1)\n",
    "    _, turn2_working_memory = await memory_client.get_or_create_working_memory(\n",
    "        session_id=session_id, user_id=student_id, model_name=\"gpt-4o\"\n",
    "    )\n",
    "\n",
    "    print(f\"\"\"\n",
    "üìä Working Memory Status:\n",
    "   Messages in memory: {len(turn2_working_memory.messages)}\n",
    "   Contains: Turn 1 conversation\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835a58cba2bf1b6",
   "metadata": {},
   "source": [
    "#### Step 3: Build context with conversation history\n",
    "\n",
    "To resolve the pronoun \"it\", we need to include the conversation history in the LLM context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cab346e89ea60dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:05.649507Z",
     "iopub.status.busy": "2025-12-05T00:17:05.649370Z",
     "iopub.status.idle": "2025-12-05T00:17:05.652694Z",
     "shell.execute_reply": "2025-12-05T00:17:05.652158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Building context with conversation history...\n",
      "   Total messages in context: 10\n",
      "   Includes: System prompt + Turn 1 history + current query\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    print(\"\\nüîß Building context with conversation history...\")\n",
    "\n",
    "    # Start with system message\n",
    "    turn2_messages = [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful course advisor. Use conversation history to resolve references like 'it', 'that course', etc. Be concise but informative.\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Add conversation history from working memory\n",
    "    for msg in turn2_working_memory.messages:\n",
    "        if msg.role == \"user\":\n",
    "            turn2_messages.append(HumanMessage(content=msg.content))\n",
    "        elif msg.role == \"assistant\":\n",
    "            turn2_messages.append(AIMessage(content=msg.content))\n",
    "\n",
    "    # Add current query\n",
    "    turn2_messages.append(HumanMessage(content=turn2_query))\n",
    "\n",
    "    print(f\"\"\"   Total messages in context: {len(turn2_messages)}\n",
    "   Includes: System prompt + Turn 1 history + current query\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af331a75d883304",
   "metadata": {},
   "source": [
    "#### Step 4: Generate response using LLM\n",
    "\n",
    "The LLM can now resolve \"it\" by looking at the conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98780f327a7c485f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:05.654019Z",
     "iopub.status.busy": "2025-12-05T00:17:05.653922Z",
     "iopub.status.idle": "2025-12-05T00:17:08.836973Z",
     "shell.execute_reply": "2025-12-05T00:17:08.836119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí≠ LLM resolving 'it' using conversation history...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:08 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: The prerequisites for machine learning courses typically include:\n",
      "\n",
      "1. **Mathematics**: A good understanding of linear algebra, calculus, probability, and statistics is essential, as these are foundational to many machine learning algorithms.\n",
      "\n",
      "2. **Programming Skills**: Proficiency in a programming language, especially Python, is often required since it's widely used in machine learning for implementing algorithms and handling data.\n",
      "\n",
      "3. **Basic Data Handling**: Familiarity with data manipulation and analysis, often using libraries like Pandas and NumPy, is important for working with datasets.\n",
      "\n",
      "4. **Understanding of Algorithms**: A basic understanding of algorithms and data structures can be beneficial, as it helps in grasping how machine learning models work.\n",
      "\n",
      "5. **Familiarity with Tools**: Some courses might expect you to have a basic understanding of machine learning frameworks and libraries, such as TensorFlow, Keras, or Scikit-learn.\n",
      "\n",
      "These prerequisites ensure that you have the necessary background to understand and apply machine learning concepts effectively. If you're new to any of these areas, you might consider taking introductory courses in those subjects first.\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    print(\"\\nüí≠ LLM resolving 'it' using conversation history...\")\n",
    "    turn2_response = llm.invoke(turn2_messages).content\n",
    "\n",
    "    print(f\"\\nü§ñ Agent: {turn2_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e66854646ae9f",
   "metadata": {},
   "source": [
    "#### Step 5: Save to working memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73603a25026b0a03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:08.839605Z",
     "iopub.status.busy": "2025-12-05T00:17:08.839295Z",
     "iopub.status.idle": "2025-12-05T00:17:08.856062Z",
     "shell.execute_reply": "2025-12-05T00:17:08.854783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:08 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved to working memory\n",
      "   Messages now in memory: 10\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    # Add messages to working memory\n",
    "    turn2_working_memory.messages.extend(\n",
    "        [\n",
    "            MemoryMessage(role=\"user\", content=turn2_query),\n",
    "            MemoryMessage(role=\"assistant\", content=turn2_response),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Save to Memory Server\n",
    "    await memory_client.put_working_memory(\n",
    "        session_id=session_id,\n",
    "        memory=turn2_working_memory,\n",
    "        user_id=student_id,\n",
    "        model_name=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    print(f\"\"\"\n",
    "‚úÖ Saved to working memory\n",
    "   Messages now in memory: {len(turn2_working_memory.messages)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419c399e940b2ac",
   "metadata": {},
   "source": [
    "### What Just Happened in Turn 2?\n",
    "\n",
    "**Initial State:**\n",
    "- Working memory contained Turn 1 conversation (2 messages)\n",
    "- User asked about \"its prerequisites\" - pronoun reference\n",
    "\n",
    "**Actions:**\n",
    "1. Loaded working memory with Turn 1 history\n",
    "2. Built context including conversation history\n",
    "3. LLM resolved \"it\" ‚Üí the course from Turn 1\n",
    "4. Generated response about prerequisites\n",
    "5. Saved updated conversation to working memory\n",
    "\n",
    "**Result:**\n",
    "- Working memory now contains 4 messages (2 exchanges)\n",
    "- LLM successfully resolved pronoun reference using conversation history\n",
    "- Natural conversation flow maintained\n",
    "\n",
    "**Key Insight:** Without working memory, the LLM wouldn't know what \"it\" refers to!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7073faa996dab1",
   "metadata": {},
   "source": [
    "### Turn 3: Another Follow-up\n",
    "\n",
    "Let's ask one more follow-up question to demonstrate continued conversation continuity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c127596e4cffd40d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:08.857993Z",
     "iopub.status.busy": "2025-12-05T00:17:08.857848Z",
     "iopub.status.idle": "2025-12-05T00:17:12.185716Z",
     "shell.execute_reply": "2025-12-05T00:17:12.184606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç TURN 3: User asks another follow-up\n",
      "================================================================================\n",
      "\n",
      "üë§ User: Is it available online?\n",
      "   Note: 'it' still refers to the course from Turn 1\n",
      "19:17:08 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Working Memory Status:\n",
      "   Messages in memory: 10\n",
      "   Contains: Turns 1 and 2\n",
      "   Total messages in context: 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:12 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: Yes, machine learning courses are widely available online. Many platforms offer a variety of courses that cater to different skill levels, from beginner to advanced. Here are some popular online platforms where you can find machine learning courses:\n",
      "\n",
      "1. **Coursera**: Offers courses from universities like Stanford and institutions like Google, often including video lectures, assignments, and projects.\n",
      "\n",
      "2. **edX**: Provides courses from universities such as MIT and Harvard, covering both introductory and advanced topics in machine learning.\n",
      "\n",
      "3. **Udacity**: Known for its \"Nanodegree\" programs, which are more intensive and often include real-world projects and mentorship.\n",
      "\n",
      "4. **Udemy**: Offers a wide variety of courses on machine learning, often at a lower cost, with options for beginners and more experienced learners.\n",
      "\n",
      "5. **Khan Academy**: While not as comprehensive in machine learning specifically, it offers foundational courses in mathematics and programming.\n",
      "\n",
      "6. **DataCamp**: Focuses on data science and machine learning, offering interactive coding exercises and projects.\n",
      "\n",
      "7. **Fast.ai**: Provides a practical, hands-on approach to deep learning and machine learning, suitable for those with some programming experience.\n",
      "\n",
      "These platforms typically offer flexible learning schedules, allowing you to learn at your own pace. Many courses also provide certificates upon completion, which can be beneficial for showcasing your skills to potential employers.\n",
      "19:17:12 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved to working memory\n",
      "   Messages now in memory: 12\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìç TURN 3: User asks another follow-up\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    turn3_query = \"Is it available online?\"\n",
    "    print(f\"\\nüë§ User: {turn3_query}\")\n",
    "    print(\"   Note: 'it' still refers to the course from Turn 1\")\n",
    "\n",
    "    # Load working memory (now has 2 exchanges)\n",
    "    _, turn3_working_memory = await memory_client.get_or_create_working_memory(\n",
    "        session_id=session_id, user_id=student_id, model_name=\"gpt-4o\"\n",
    "    )\n",
    "\n",
    "    print(f\"\"\"\n",
    "üìä Working Memory Status:\n",
    "   Messages in memory: {len(turn3_working_memory.messages)}\n",
    "   Contains: Turns 1 and 2\"\"\")\n",
    "\n",
    "    # Build context with full conversation history\n",
    "    turn3_messages = [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful course advisor. Use conversation history to resolve references.\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for msg in turn3_working_memory.messages:\n",
    "        if msg.role == \"user\":\n",
    "            turn3_messages.append(HumanMessage(content=msg.content))\n",
    "        elif msg.role == \"assistant\":\n",
    "            turn3_messages.append(AIMessage(content=msg.content))\n",
    "\n",
    "    turn3_messages.append(HumanMessage(content=turn3_query))\n",
    "\n",
    "    print(f\"   Total messages in context: {len(turn3_messages)}\")\n",
    "\n",
    "    # Generate response\n",
    "    turn3_response = llm.invoke(turn3_messages).content\n",
    "\n",
    "    print(f\"\\nü§ñ Agent: {turn3_response}\")\n",
    "\n",
    "    # Save to working memory\n",
    "    turn3_working_memory.messages.extend(\n",
    "        [\n",
    "            MemoryMessage(role=\"user\", content=turn3_query),\n",
    "            MemoryMessage(role=\"assistant\", content=turn3_response),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    await memory_client.put_working_memory(\n",
    "        session_id=session_id,\n",
    "        memory=turn3_working_memory,\n",
    "        user_id=student_id,\n",
    "        model_name=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    print(f\"\"\"\n",
    "‚úÖ Saved to working memory\n",
    "   Messages now in memory: {len(turn3_working_memory.messages)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37c05d024d6df5",
   "metadata": {},
   "source": [
    "### üéØ Working Memory Demo Summary\n",
    "\n",
    "**üìä What Happened:**\n",
    "\n",
    "| Turn | Query | Working Memory | Result |\n",
    "|------|-------|----------------|--------|\n",
    "| 1 | \"Tell me about machine learning courses\" | Empty (first turn) | Stored query + response |\n",
    "| 2 | \"What are the prerequisites for it?\" | 1 exchange | LLM resolved 'it' using history |\n",
    "| 3 | \"Is it available online?\" | 2 exchanges | Continued conversation flow |\n",
    "\n",
    "**‚úÖ Key Benefits:**\n",
    "- Natural conversation flow\n",
    "- Pronoun reference resolution\n",
    "- No need to repeat context\n",
    "- Seamless user experience\n",
    "\n",
    "**‚ùå Without Working Memory:**\n",
    "- \"What are the prerequisites for it?\" ‚Üí \"What is 'it'? Please specify.\"\n",
    "- Each query is isolated\n",
    "- User must repeat context every time\n",
    "\n",
    "### Key Insight: Conversation Context Type\n",
    "\n",
    "Working memory provides the **Conversation Context** - the third context type from Module 1:\n",
    "\n",
    "1. **System Context** - Role and instructions (static)\n",
    "2. **User Context** - Profile and preferences (dynamic, user-specific)\n",
    "3. **Conversation Context** - Working memory (dynamic, session-specific) ‚Üê **We just demonstrated this!**\n",
    "4. **Retrieved Context** - RAG results (dynamic, query-specific)\n",
    "\n",
    "Without working memory, we only had 3 context types. Now we have all 4!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110cc39a22fde44a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Part 2: Long-term Memory for Context Engineering\n",
    "\n",
    "### What is Long-term Memory?\n",
    "\n",
    "Long-term memory enables AI agents to store **persistent knowledge** across sessions‚Äîincluding user preferences, domain facts, business rules, and system configuration. This is crucial for context engineering because it allows agents to:\n",
    "\n",
    "- **Personalize** interactions by remembering user-specific preferences and history\n",
    "- **Apply domain knowledge** consistently (prerequisites, policies, regulations)\n",
    "- **Maintain organizational context** (business rules, schedules, procedures)\n",
    "- **Search efficiently** using semantic vector search across all knowledge types\n",
    "\n",
    "Long-term memory is a flexible storage mechanism: user-scoped memories enable personalization (\"Student prefers online courses\"), while application-scoped memories provide consistent behavior for everyone (\"CS401 requires CS201\", \"Registration opens 2 weeks before semester\").\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Session 1: User shares preferences ‚Üí Store in long-term memory\n",
    "Session 2: User asks for recommendations ‚Üí Search memory ‚Üí Personalized response\n",
    "Session 3: User updates preferences ‚Üí Update memory accordingly\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Three Types of Long-term Memory\n",
    "\n",
    "The Agent Memory Server supports three distinct memory types, each optimized for different kinds of information:\n",
    "\n",
    "### 1. Semantic Memory - Facts and Knowledge\n",
    "\n",
    "**Purpose:** Store timeless facts, preferences, and knowledge independent of when they were learned. Can be user-scoped (personalization) or application-scoped (domain knowledge).\n",
    "\n",
    "**User-Scoped Examples:**\n",
    "- \"Student's major is Computer Science\"\n",
    "- \"Student prefers online courses\"\n",
    "- \"Student wants to graduate in Spring 2026\"\n",
    "\n",
    "**Application-Scoped Examples:**\n",
    "- \"CS401 requires CS201 and MATH301 as prerequisites\"\n",
    "- \"Online courses have asynchronous discussion forums\"\n",
    "- \"Maximum file upload size for assignments is 50MB\"\n",
    "\n",
    "**When to use:** Information that remains true regardless of time context.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Episodic Memory - Events and Experiences\n",
    "\n",
    "**Purpose:** Store time-bound events and experiences where sequence matters.\n",
    "\n",
    "**Examples:**\n",
    "- \"Student enrolled in CS101 on 2024-09-15\"\n",
    "- \"Student completed CS101 with grade A on 2024-12-10\"\n",
    "- \"Student asked about machine learning courses on 2024-09-20\"\n",
    "\n",
    "**When to use:** Timeline-based information where timing or sequence is important.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Message Memory - Context-Rich Conversations\n",
    "\n",
    "**Purpose:** Store full conversation snippets where complete context is crucial.\n",
    "\n",
    "**Examples:**\n",
    "- Detailed career planning discussion with nuanced advice\n",
    "- Professor's specific guidance about research opportunities\n",
    "- Student's explanation of personal learning challenges\n",
    "\n",
    "**When to use:** When summary would lose important nuance, tone, or exact wording.\n",
    "\n",
    "**‚ö†Ô∏è Use sparingly** - Message memories are token-expensive!\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Choosing the Right Memory Type\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "**Ask yourself these questions:**\n",
    "\n",
    "1. **Can you extract a simple fact?** ‚Üí Use **Semantic**\n",
    "2. **Does timing matter?** ‚Üí Use **Episodic**\n",
    "3. **Is full context crucial?** ‚Üí Use **Message** (rarely)\n",
    "\n",
    "**Default strategy: Prefer Semantic** - they're compact, searchable, and efficient.\n",
    "\n",
    "### Quick Reference Table\n",
    "\n",
    "| Information Type | Memory Type | Example |\n",
    "|-----------------|-------------|----------|\n",
    "| Preference | Semantic | \"Prefers morning classes\" |\n",
    "| Fact | Semantic | \"Major is Computer Science\" |\n",
    "| Goal | Semantic | \"Wants to graduate in 2026\" |\n",
    "| Event | Episodic | \"Enrolled in CS401 on 2024-09-15\" |\n",
    "| Timeline | Episodic | \"Completed CS101, then CS201\" |\n",
    "| Complex discussion | Message | [Full career planning conversation] |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Hands-On: Long-term Memory in Action\n",
    "\n",
    "Let's put these concepts into practice with code examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5117be1ffcf32e9",
   "metadata": {},
   "source": [
    "### Setup: Student ID for Long-term Memory\n",
    "\n",
    "Long-term memories are user-scoped, so we need a student ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf6440b84281516b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:12.188293Z",
     "iopub.status.busy": "2025-12-05T00:17:12.188083Z",
     "iopub.status.idle": "2025-12-05T00:17:12.191418Z",
     "shell.execute_reply": "2025-12-05T00:17:12.190708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Long-term Memory Demo Setup\n",
      "   Student ID: sarah_chen\n",
      "   Ready to store and search persistent memories\n"
     ]
    }
   ],
   "source": [
    "# Setup for long-term memory demo\n",
    "lt_student_id = \"sarah_chen\"\n",
    "\n",
    "print(f\"\"\"üéØ Long-term Memory Demo Setup\n",
    "   Student ID: {lt_student_id}\n",
    "   Ready to store and search persistent memories\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3571daa56c7415a",
   "metadata": {},
   "source": [
    "### Step 1: Store Semantic Memories (Facts)\n",
    "\n",
    "Semantic memories are timeless facts about the student. Let's store several facts about Sarah's preferences and academic status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10adf1e40476f738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:12.193213Z",
     "iopub.status.busy": "2025-12-05T00:17:12.193070Z",
     "iopub.status.idle": "2025-12-05T00:17:12.246914Z",
     "shell.execute_reply": "2025-12-05T00:17:12.246340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìç STEP 1: Storing Semantic Memories (Facts)\n",
      "================================================================================\n",
      "\n",
      "üìù Storing 6 semantic memories...\n",
      "19:17:12 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student prefers online courses over in-person classes\n",
      "19:17:12 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student's major is Computer Science with focus on AI/ML\n",
      "19:17:12 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student wants to graduate in Spring 2026\n",
      "19:17:12 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student prefers morning classes, no classes on Fridays\n",
      "19:17:12 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student has completed Introduction to Programming and Data Structures\n",
      "19:17:12 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student is currently taking Linear Algebra\n",
      "\n",
      "‚úÖ Stored 6 semantic memories\n",
      "   Memory type: semantic (timeless facts)\n",
      "   Topics: preferences, academic_info\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üìç STEP 1: Storing Semantic Memories (Facts)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Define semantic memories (timeless facts)\n",
    "    semantic_memories = [\n",
    "        \"Student prefers online courses over in-person classes\",\n",
    "        \"Student's major is Computer Science with focus on AI/ML\",\n",
    "        \"Student wants to graduate in Spring 2026\",\n",
    "        \"Student prefers morning classes, no classes on Fridays\",\n",
    "        \"Student has completed Introduction to Programming and Data Structures\",\n",
    "        \"Student is currently taking Linear Algebra\",\n",
    "    ]\n",
    "    print(f\"\\nüìù Storing {len(semantic_memories)} semantic memories...\")\n",
    "\n",
    "    # Store each semantic memory\n",
    "    for memory_text in semantic_memories:\n",
    "        memory_record = ClientMemoryRecord(\n",
    "            text=memory_text,\n",
    "            user_id=lt_student_id,\n",
    "            memory_type=\"semantic\",\n",
    "            topics=[\"preferences\", \"academic_info\"],\n",
    "        )\n",
    "        await memory_client.create_long_term_memory([memory_record])\n",
    "        print(f\"   ‚úÖ {memory_text}\")\n",
    "\n",
    "    print(f\"\"\"\n",
    "‚úÖ Stored {len(semantic_memories)} semantic memories\n",
    "   Memory type: semantic (timeless facts)\n",
    "   Topics: preferences, academic_info\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e7139023e0182",
   "metadata": {},
   "source": [
    "### What We Just Did: Semantic Memories\n",
    "\n",
    "**Stored 6 semantic memories:**\n",
    "- Student preferences (online courses, morning classes)\n",
    "- Academic information (major, graduation date)\n",
    "- Course history (completed, current)\n",
    "\n",
    "**Why semantic?**\n",
    "- These are timeless facts\n",
    "- No specific date/time context needed\n",
    "- Compact and efficient\n",
    "\n",
    "**How they're stored:**\n",
    "- Vector-indexed for semantic search\n",
    "- Tagged with topics for organization\n",
    "- Automatically deduplicated\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d11fddf0f3e95",
   "metadata": {},
   "source": [
    "### Step 2: Store Episodic Memories (Events)\n",
    "\n",
    "Episodic memories are time-bound events. Let's store some events from Sarah's academic timeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9eaf03f0ea2c003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:12.248169Z",
     "iopub.status.busy": "2025-12-05T00:17:12.248067Z",
     "iopub.status.idle": "2025-12-05T00:17:12.256144Z",
     "shell.execute_reply": "2025-12-05T00:17:12.255489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç STEP 2: Storing Episodic Memories (Events)\n",
      "================================================================================\n",
      "\n",
      "üìù Storing 3 episodic memories...\n",
      "19:17:12 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student enrolled in Introduction to Programming on 2024-09-01\n",
      "19:17:12 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student completed Introduction to Programming with grade A on 2024-12-15\n",
      "19:17:12 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student asked about machine learning courses on 2024-09-20\n",
      "\n",
      "‚úÖ Stored 3 episodic memories\n",
      "   Memory type: episodic (time-bound events)\n",
      "   Topics: enrollment, courses\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìç STEP 2: Storing Episodic Memories (Events)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Define episodic memories (time-bound events)\n",
    "    episodic_memories = [\n",
    "        \"Student enrolled in Introduction to Programming on 2024-09-01\",\n",
    "        \"Student completed Introduction to Programming with grade A on 2024-12-15\",\n",
    "        \"Student asked about machine learning courses on 2024-09-20\",\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüìù Storing {len(episodic_memories)} episodic memories...\")\n",
    "\n",
    "    # Store each episodic memory\n",
    "    for memory_text in episodic_memories:\n",
    "        memory_record = ClientMemoryRecord(\n",
    "            text=memory_text,\n",
    "            user_id=lt_student_id,\n",
    "            memory_type=\"episodic\",\n",
    "            topics=[\"enrollment\", \"courses\"],\n",
    "        )\n",
    "        await memory_client.create_long_term_memory([memory_record])\n",
    "        print(f\"   ‚úÖ {memory_text}\")\n",
    "\n",
    "    print(f\"\"\"\n",
    "‚úÖ Stored {len(episodic_memories)} episodic memories\n",
    "   Memory type: episodic (time-bound events)\n",
    "   Topics: enrollment, courses\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76039eb5e847a017",
   "metadata": {},
   "source": [
    "### What We Just Did: Episodic Memories\n",
    "\n",
    "**Stored 3 episodic memories:**\n",
    "- Enrollment event (Introduction to Programming on 2024-09-01)\n",
    "- Completion event (Introduction to Programming with grade A on 2024-12-15)\n",
    "- Interaction event (asked about ML courses on 2024-09-20)\n",
    "\n",
    "**Why episodic?**\n",
    "- These are time-bound events\n",
    "- Timing and sequence matter\n",
    "- Captures academic timeline\n",
    "\n",
    "**Difference from semantic:**\n",
    "- Semantic: \"Student has completed Introduction to Programming\" (timeless fact)\n",
    "- Episodic: \"Student completed Introduction to Programming with grade A on 2024-12-15\" (specific event)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185cb8ea27b023a",
   "metadata": {},
   "source": [
    "### Step 3: Search Long-term Memory\n",
    "\n",
    "Now let's search our long-term memories using natural language queries. The system will use semantic search to find relevant memories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56fc1a610c400036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:12.257396Z",
     "iopub.status.busy": "2025-12-05T00:17:12.257307Z",
     "iopub.status.idle": "2025-12-05T00:17:13.490562Z",
     "shell.execute_reply": "2025-12-05T00:17:13.489552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç STEP 3: Searching Long-term Memory\n",
      "================================================================================\n",
      "\n",
      "üîç Query: 'What does the student prefer?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:12 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ö†Ô∏è  No memories found\n",
      "\n",
      "üîç Query: 'What courses has the student completed?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:13 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìö Found 1 relevant memories:\n",
      "      1. Student has completed Introduction to Programming and Data Structures\n",
      "\n",
      "üîç Query: 'What is the student's major?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:13 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìö Found 3 relevant memories:\n",
      "      1. Student's major is Computer Science with focus on AI/ML\n",
      "      2. Student enrolled in Introduction to Programming on 2024-09-01\n",
      "      3. Student has completed Introduction to Programming and Data Structures\n",
      "\n",
      "================================================================================\n",
      "‚úÖ DEMO COMPLETE: Long-term memory enables persistent knowledge!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    from agent_memory_client.filters import UserId\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìç STEP 3: Searching Long-term Memory\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Query 1: What does the student prefer?\n",
    "    search_query_1 = \"What does the student prefer?\"\n",
    "    print(f\"\\nüîç Query: '{search_query_1}'\")\n",
    "\n",
    "    search_results_1 = await memory_client.search_long_term_memory(\n",
    "        text=search_query_1, user_id=UserId(eq=lt_student_id), limit=3\n",
    "    )\n",
    "\n",
    "    if search_results_1.memories:\n",
    "        print(f\"   üìö Found {len(search_results_1.memories)} relevant memories:\")\n",
    "        for i, memory in enumerate(search_results_1.memories[:3], 1):\n",
    "            print(f\"      {i}. {memory.text}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No memories found\")\n",
    "\n",
    "    # Query 2: What courses has the student completed?\n",
    "    search_query_2 = \"What courses has the student completed?\"\n",
    "    print(f\"\\nüîç Query: '{search_query_2}'\")\n",
    "\n",
    "    search_results_2 = await memory_client.search_long_term_memory(\n",
    "        text=search_query_2, user_id=UserId(eq=lt_student_id), limit=5\n",
    "    )\n",
    "\n",
    "    if search_results_2.memories:\n",
    "        print(f\"   üìö Found {len(search_results_2.memories)} relevant memories:\")\n",
    "        for i, memory in enumerate(search_results_2.memories[:5], 1):\n",
    "            print(f\"      {i}. {memory.text}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No memories found\")\n",
    "\n",
    "    # Query 3: What is the student's major?\n",
    "    search_query_3 = \"What is the student's major?\"\n",
    "    print(f\"\\nüîç Query: '{search_query_3}'\")\n",
    "\n",
    "    search_results_3 = await memory_client.search_long_term_memory(\n",
    "        text=search_query_3, user_id=UserId(eq=lt_student_id), limit=3\n",
    "    )\n",
    "\n",
    "    if search_results_3.memories:\n",
    "        print(f\"   üìö Found {len(search_results_3.memories)} relevant memories:\")\n",
    "        for i, memory in enumerate(search_results_3.memories[:3], 1):\n",
    "            print(f\"      {i}. {memory.text}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No memories found\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ DEMO COMPLETE: Long-term memory enables persistent knowledge!\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Memory Server not available. Skipping demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf894047127804ac",
   "metadata": {},
   "source": [
    "### üéØ Long-term Memory Demo Summary\n",
    "\n",
    "**üìä What We Did:**\n",
    "- **Step 1:** Stored 6 semantic memories (facts) - preferences, major, graduation date\n",
    "- **Step 2:** Stored 3 episodic memories (events) - enrollment, completion, interaction\n",
    "- **Step 3:** Searched long-term memory with natural language queries\n",
    "\n",
    "**‚úÖ Key Benefits:**\n",
    "- Persistent knowledge across sessions\n",
    "- Semantic search (not keyword matching)\n",
    "- Automatic deduplication\n",
    "- Topic-based organization\n",
    "\n",
    "**üí° Key Insight:**\n",
    "Long-term memory enables personalization and knowledge accumulation across sessions. It's the foundation for building agents that remember and learn from users.\n",
    "\n",
    "### Key Insight: User Context Type\n",
    "\n",
    "Long-term memory provides part of the **User Context** - the second context type from Module 1:\n",
    "\n",
    "1. **System Context** - Role and instructions (static)\n",
    "2. **User Context** - Profile + long-term memories (dynamic, user-specific) ‚Üê **Long-term memories contribute here!**\n",
    "3. **Conversation Context** - Working memory (dynamic, session-specific)\n",
    "4. **Retrieved Context** - RAG results (dynamic, query-specific)\n",
    "\n",
    "Long-term memories enhance User Context by adding persistent knowledge about the user's preferences, history, and goals.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Understanding Memory Search: Why Semantic Only?\n",
    "\n",
    "You might have noticed that the Agent Memory Server uses **semantic (vector) search only** - no keyword search or hybrid search. Let's understand why this is the right choice.\n",
    "\n",
    "### Memory vs. Course Catalog: Different Search Needs\n",
    "\n",
    "| Aspect | Memory Search | Course Catalog Search |\n",
    "|--------|---------------|----------------------|\n",
    "| **Data Type** | Conversational facts | Structured catalog |\n",
    "| **Content** | \"Student prefers online courses\"<br>\"Completed CS101 last semester\" | Course codes, titles, syllabi<br>Departments, prerequisites |\n",
    "| **Queries** | \"What does the student prefer?\"<br>\"What courses has the student taken?\" | \"CS101\"<br>\"beginner programming courses\" |\n",
    "| **Exact Matches?** | ‚ùå No codes/IDs to match | ‚úÖ Course codes, departments |\n",
    "| **Best Search** | **Semantic only** | **Hybrid (semantic + keyword)** |\n",
    "\n",
    "### Why Semantic Search for Memories?\n",
    "\n",
    "**1. Conversational Content**\n",
    "```python\n",
    "# Memory content is natural language\n",
    "memories = [\n",
    "    \"Student prefers online courses over in-person\",\n",
    "    \"Interested in machine learning and AI\",\n",
    "    \"Completed CS101 with grade A last semester\"\n",
    "]\n",
    "\n",
    "# Queries are also natural language\n",
    "query = \"What does the student prefer?\"\n",
    "# ‚úÖ Semantic search finds: \"Student prefers online courses...\"\n",
    "# ‚ùå Keyword search would miss it (no exact word \"prefer\" in memory)\n",
    "```\n",
    "\n",
    "**2. No Exact Codes/IDs**\n",
    "```python\n",
    "# Memories don't have exact codes to match\n",
    "memory = \"Student prefers online courses\"  # No \"ONLINE-001\" code\n",
    "\n",
    "# vs. Course catalog\n",
    "course = {\n",
    "    \"course_code\": \"CS101\",  # ‚Üê Exact code for keyword search\n",
    "    \"department\": \"Computer Science\",  # ‚Üê Exact category\n",
    "    \"title\": \"Introduction to Programming\"\n",
    "}\n",
    "```\n",
    "\n",
    "**3. Small Dataset Per User**\n",
    "```python\n",
    "# Typical user has <100 memories\n",
    "# Vector search is fast enough\n",
    "# No need for keyword optimization\n",
    "\n",
    "# vs. Course catalog with 1000s of courses\n",
    "# Hybrid search improves performance and precision\n",
    "```\n",
    "\n",
    "### Real-World Example\n",
    "\n",
    "**Memory Search (Semantic):**\n",
    "```python\n",
    "# Query: \"What are the student's interests?\"\n",
    "results = await memory_client.search_long_term_memory(\n",
    "    text=\"What are the student's interests?\",\n",
    "    user_id=UserId(eq=student_id)\n",
    ")\n",
    "# Finds: \"Interested in machine learning and AI\"\n",
    "#        \"Enjoys data science projects\"\n",
    "# ‚úÖ Semantic understanding matches conceptually\n",
    "```\n",
    "\n",
    "**Course Search (Hybrid):**\n",
    "```python\n",
    "# Query: \"beginner CS programming courses\"\n",
    "# Semantic: Finds conceptually similar courses\n",
    "# Keyword: Filters by department=\"Computer Science\", difficulty=\"Beginner\"\n",
    "# Hybrid: Best of both worlds!\n",
    "```\n",
    "\n",
    "### When Would Memory Need Hybrid Search?\n",
    "\n",
    "You'd add keyword/hybrid search to memories if:\n",
    "- ‚ùå Memories contained exact codes/IDs to match\n",
    "- ‚ùå Users searched for specific technical terms\n",
    "- ‚ùå Dataset was huge (millions of memories per user)\n",
    "\n",
    "**But:** None of these apply to conversational memory!\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**Different data types need different search strategies:**\n",
    "\n",
    "```\n",
    "Conversational Data (Memories)\n",
    "    ‚Üì\n",
    "Natural language content\n",
    "    ‚Üì\n",
    "Semantic search only ‚úÖ\n",
    "\n",
    "Structured Catalog (Courses)\n",
    "    ‚Üì\n",
    "Codes + descriptions + metadata\n",
    "    ‚Üì\n",
    "Hybrid search (semantic + keyword) ‚úÖ\n",
    "\n",
    "Reference Data (Course Details)\n",
    "    ‚Üì\n",
    "Fetched by ID only\n",
    "    ‚Üì\n",
    "No search needed (plain keys) ‚úÖ\n",
    "```\n",
    "\n",
    "This is why Module 2 teaches all search types, but Module 4 uses semantic-only for memories!\n",
    "\n",
    "---\n",
    "\n",
    "## üè∑Ô∏è Advanced: Topics and Filtering\n",
    "\n",
    "Topics help organize and filter memories. Let's explore how to use them effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de1264c1d1bfea80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:13.494017Z",
     "iopub.status.busy": "2025-12-05T00:17:13.493710Z",
     "iopub.status.idle": "2025-12-05T00:17:13.511849Z",
     "shell.execute_reply": "2025-12-05T00:17:13.511111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üè∑Ô∏è  TOPICS AND FILTERING DEMO\n",
      "================================================================================\n",
      "\n",
      "üìç Storing Memories with Topics\n",
      "--------------------------------------------------------------------------------\n",
      "19:17:13 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student prefers online courses\n",
      "      Topics: preferences, course_format\n",
      "19:17:13 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student's major is Computer Science\n",
      "      Topics: academic_info, major\n",
      "19:17:13 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student wants to graduate in Spring 2026\n",
      "      Topics: goals, graduation\n",
      "19:17:13 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student prefers morning classes\n",
      "      Topics: preferences, schedule\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    topics_student_id = \"sarah_chen\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üè∑Ô∏è  TOPICS AND FILTERING DEMO\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nüìç Storing Memories with Topics\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Define memories with their topics\n",
    "    memories_with_topics = [\n",
    "        (\"Student prefers online courses\", [\"preferences\", \"course_format\"]),\n",
    "        (\"Student's major is Computer Science\", [\"academic_info\", \"major\"]),\n",
    "        (\"Student wants to graduate in Spring 2026\", [\"goals\", \"graduation\"]),\n",
    "        (\"Student prefers morning classes\", [\"preferences\", \"schedule\"]),\n",
    "    ]\n",
    "\n",
    "    # Store each memory\n",
    "    for memory_text, topics in memories_with_topics:\n",
    "        memory_record = ClientMemoryRecord(\n",
    "            text=memory_text,\n",
    "            user_id=topics_student_id,\n",
    "            memory_type=\"semantic\",\n",
    "            topics=topics,\n",
    "        )\n",
    "        await memory_client.create_long_term_memory([memory_record])\n",
    "        print(f\"   ‚úÖ {memory_text}\")\n",
    "        print(f\"      Topics: {', '.join(topics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726735269a05be58",
   "metadata": {},
   "source": [
    "### Filter memories by type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a752aee40f46df64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:13.513910Z",
     "iopub.status.busy": "2025-12-05T00:17:13.513724Z",
     "iopub.status.idle": "2025-12-05T00:17:13.704611Z",
     "shell.execute_reply": "2025-12-05T00:17:13.703630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç Filtering by Memory Type: Semantic\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:13 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 5 semantic memories:\n",
      "   1. Student is currently taking Linear Algebra\n",
      "      Topics: preferences, academic_info\n",
      "   2. Student prefers online courses over in-person classes\n",
      "      Topics: preferences, academic_info\n",
      "   3. Student prefers morning classes, no classes on Fridays\n",
      "      Topics: preferences, academic_info\n",
      "   4. Student's major is Computer Science with focus on AI/ML\n",
      "      Topics: preferences, academic_info\n",
      "   5. Student has completed Introduction to Programming and Data Structures\n",
      "      Topics: preferences, academic_info\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Topics enable organized, filterable memory management!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    print(\"\\nüìç Filtering by Memory Type: Semantic\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    from agent_memory_client.filters import MemoryType, UserId\n",
    "\n",
    "    # Search for all semantic memories\n",
    "    results = await memory_client.search_long_term_memory(\n",
    "        text=\"\",  # Empty query returns all\n",
    "        user_id=UserId(eq=topics_student_id),\n",
    "        memory_type=MemoryType(eq=\"semantic\"),\n",
    "        limit=10,\n",
    "    )\n",
    "\n",
    "    print(f\"   Found {len(results.memories)} semantic memories:\")\n",
    "    for i, memory in enumerate(results.memories[:5], 1):\n",
    "        topics_str = \", \".join(memory.topics) if memory.topics else \"none\"\n",
    "        print(f\"   {i}. {memory.text}\")\n",
    "        print(f\"      Topics: {topics_str}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ Topics enable organized, filterable memory management!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46723832e273fad5",
   "metadata": {},
   "source": [
    "### üéØ Why Topics Matter\n",
    "\n",
    "**Organization:**\n",
    "- Group related memories together\n",
    "- Easy to find memories by category\n",
    "\n",
    "**Filtering:**\n",
    "- Search within specific topics\n",
    "- Filter by memory type (semantic, episodic, message)\n",
    "\n",
    "**Best Practices:**\n",
    "- Use consistent topic names\n",
    "- Keep topics broad enough to be useful\n",
    "- Common topics: `preferences`, `academic_info`, `goals`, `schedule`, `courses`\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Cross-Session Memory Persistence\n",
    "\n",
    "Let's verify that memories persist across sessions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3a39f026cd2f545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:13.707259Z",
     "iopub.status.busy": "2025-12-05T00:17:13.707015Z",
     "iopub.status.idle": "2025-12-05T00:17:13.978396Z",
     "shell.execute_reply": "2025-12-05T00:17:13.977610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîÑ CROSS-SESSION MEMORY PERSISTENCE DEMO\n",
      "================================================================================\n",
      "\n",
      "üìç SESSION 1: Storing Memories\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:13 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Stored: Student is interested in machine learning and AI\n",
      "\n",
      "üìç SESSION 2: New Session, Same Student\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîÑ New session started for the same student\n",
      "\n",
      "   üîç Searching: 'What are the student's interests?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:13 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ‚úÖ Memories accessible from new session:\n",
      "      1. Student's major is Computer Science with focus on AI/ML\n",
      "      2. Student prefers morning classes, no classes on Fridays\n",
      "      3. Student prefers online courses over in-person classes\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Long-term memories persist across sessions!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    cross_session_student_id = \"sarah_chen\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üîÑ CROSS-SESSION MEMORY PERSISTENCE DEMO\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nüìç SESSION 1: Storing Memories\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    memory_record = ClientMemoryRecord(\n",
    "        text=\"Student is interested in machine learning and AI\",\n",
    "        user_id=cross_session_student_id,\n",
    "        memory_type=\"semantic\",\n",
    "        topics=[\"interests\", \"AI\"],\n",
    "    )\n",
    "    await memory_client.create_long_term_memory([memory_record])\n",
    "    print(\"   ‚úÖ Stored: Student is interested in machine learning and AI\")\n",
    "\n",
    "    print(\"\\nüìç SESSION 2: New Session, Same Student\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Create a new memory client (simulating a new session)\n",
    "    new_session_config = MemoryClientConfig(\n",
    "        base_url=AGENT_MEMORY_URL,\n",
    "        default_namespace=\"redis_university\",\n",
    "    )\n",
    "    new_session_client = MemoryAPIClient(config=new_session_config)\n",
    "\n",
    "    print(\"   üîÑ New session started for the same student\")\n",
    "\n",
    "    print(\"\\n   üîç Searching: 'What are the student's interests?'\")\n",
    "    cross_session_results = await new_session_client.search_long_term_memory(\n",
    "        text=\"What are the student's interests?\",\n",
    "        user_id=UserId(eq=cross_session_student_id),\n",
    "        limit=3,\n",
    "    )\n",
    "\n",
    "    if cross_session_results.memories:\n",
    "        print(f\"\\n   ‚úÖ Memories accessible from new session:\")\n",
    "        for i, memory in enumerate(cross_session_results.memories[:3], 1):\n",
    "            print(f\"      {i}. {memory.text}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No memories found\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ Long-term memories persist across sessions!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1a0b5c218d7d2",
   "metadata": {},
   "source": [
    "### üéØ Cross-Session Persistence\n",
    "\n",
    "**What We Demonstrated:**\n",
    "- **Session 1:** Stored memories about student interests\n",
    "- **Session 2:** Created new client (simulating new session)\n",
    "- **Result:** Memories from Session 1 are accessible in Session 2\n",
    "\n",
    "**Why This Matters:**\n",
    "- Users don't have to repeat themselves\n",
    "- Personalization works across days, weeks, months\n",
    "- Knowledge accumulates over time\n",
    "\n",
    "**Contrast with Working Memory:**\n",
    "- Working memory: Session-scoped (persists within the session, like ChatGPT conversations)\n",
    "- Long-term memory: User-scoped (persists across all sessions indefinitely)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4dabaef45a0acf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Part 3: Memory-Enhanced RAG\n",
    "\n",
    "Now let's combine everything we've learned: working memory, long-term memory, and the hierarchical RAG system from Module 2.\n",
    "\n",
    "### The Complete Pattern\n",
    "\n",
    "```\n",
    "1. Load working memory (conversation history)\n",
    "2. Search long-term memory (user facts)\n",
    "3. Hierarchical RAG search (summaries ‚Üí details)\n",
    "4. Assemble all four context types\n",
    "5. Generate response\n",
    "6. Save working memory (updated conversation)\n",
    "```\n",
    "\n",
    "This gives us **stateful, personalized, context-aware conversations**.\n",
    "\n",
    "---\n",
    "\n",
    "## üö´ Before: Stateless RAG (Module 2 Approach)\n",
    "\n",
    "Let's first recall how Module 2's stateless RAG worked, and see its limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5576bc298f2db2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:13.980500Z",
     "iopub.status.busy": "2025-12-05T00:17:13.980329Z",
     "iopub.status.idle": "2025-12-05T00:17:16.373043Z",
     "shell.execute_reply": "2025-12-05T00:17:16.372296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üö´ STATELESS RAG DEMO\n",
      "================================================================================\n",
      "\n",
      "üë§ User: I'm interested in machine learning courses\n",
      "\n",
      "19:17:13 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'I'm interested in machine learning courses' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:14 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:14 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: I'm interested in machine learning courses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:14 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:14 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:16 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Agent: It seems there are no direct matches for machine learning courses in the current list. However, I can recommend some courses that might still be beneficial for your interests in data science and algorithms. Here are some relevant courses you might consider:\n",
      "\n",
      "1. **RU101: Introduction to Redis Data Structures** - This course will provide a solid foundation in understanding how data is structured and managed, which is crucial for data science.\n",
      "\n",
      "2. **RU102: Redis for Developers** - While not specifically about machine learning, this course will enhance your skills in using Redis, which can be a valuable tool in data-driven applications.\n",
      "\n",
      "3. **RU202: Redis Streams** - This course focuses on real-time data processing, which is an important aspect of data science and can be useful in machine learning applications.\n",
      "\n",
      "These courses can help build a strong foundation in data management and processing, which are key components in the field of data science and machine learning.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üö´ STATELESS RAG DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stateless_query_1 = \"I'm interested in machine learning courses\"\n",
    "print(f\"\\nüë§ User: {stateless_query_1}\\n\")\n",
    "\n",
    "# Search courses using hierarchical retrieval\n",
    "stateless_summaries, stateless_details = await hierarchical_manager.hierarchical_search(\n",
    "    query=stateless_query_1,\n",
    "    summary_limit=3,\n",
    "    detail_limit=2\n",
    ")\n",
    "\n",
    "# Assemble context (System + User + Retrieved only - NO conversation history)\n",
    "stateless_system_prompt = \"\"\"You are a Redis University course advisor.\n",
    "\n",
    "CRITICAL RULES:\n",
    "- ONLY discuss and recommend courses from the \"Relevant Courses\" list provided below\n",
    "- Do NOT mention, suggest, or make up any courses that are not in the provided list\n",
    "- If the available courses don't perfectly match the request, recommend the best options from what IS available\"\"\"\n",
    "\n",
    "stateless_user_context = f\"\"\"Student: {sarah.name}\n",
    "Major: {sarah.major}\n",
    "Interests: {', '.join(sarah.interests)}\n",
    "Completed: {', '.join(sarah.completed_courses)}\n",
    "\"\"\"\n",
    "\n",
    "# Use context assembler for progressive disclosure\n",
    "stateless_retrieved_context = context_assembler.assemble_hierarchical_context(\n",
    "    summaries=stateless_summaries,\n",
    "    details=stateless_details,\n",
    "    query=stateless_query_1\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "stateless_messages_1 = [\n",
    "    SystemMessage(content=stateless_system_prompt),\n",
    "    HumanMessage(\n",
    "        content=f\"{stateless_user_context}\\n\\n{stateless_retrieved_context}\\n\\nQuery: {stateless_query_1}\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "stateless_response_1 = llm.invoke(stateless_messages_1).content\n",
    "print(f\"ü§ñ Agent: {stateless_response_1}\")\n",
    "\n",
    "# ‚ùå No conversation history stored\n",
    "# ‚ùå Next query won't remember this interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6347c1cafa2a2",
   "metadata": {},
   "source": [
    "### Query 2: Follow-up with pronoun reference (fails)\n",
    "\n",
    "Now let's try a follow-up that requires conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4dc4ac3144b6fcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:16.375442Z",
     "iopub.status.busy": "2025-12-05T00:17:16.375264Z",
     "iopub.status.idle": "2025-12-05T00:17:21.317509Z",
     "shell.execute_reply": "2025-12-05T00:17:21.316835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ User: What are the prerequisites for the first one?\n",
      "   Note: 'the first one' refers to the first course from Query 1\n",
      "\n",
      "19:17:16 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'What are the prerequisites for the first one?' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:16 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:16 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: What are the prerequisites for the first one?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:16 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:16 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:21 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: It seems there are no relevant courses found based on your query. However, I can recommend some courses that might align with Sarah's interests in machine learning, data science, and algorithms. Here are some options from the \"Relevant Courses\" list:\n",
      "\n",
      "1. **RU101: Introduction to Redis Data Structures** - This course provides a foundational understanding of Redis, which can be beneficial for data science applications.\n",
      "\n",
      "2. **RU102: Redis for Developers** - This course is suitable for developers looking to integrate Redis into their applications, which can be useful for building efficient algorithms.\n",
      "\n",
      "3. **RU202: Redis Streams** - This course focuses on Redis Streams, which can be useful for handling real-time data, a key component in data science and machine learning.\n",
      "\n",
      "These courses do not have specific prerequisites listed, but a background in computer science should be beneficial. If you have any other questions or need further assistance, feel free to ask!\n",
      "\n",
      "‚ùå Agent can't resolve 'the first one' - no conversation history!\n"
     ]
    }
   ],
   "source": [
    "stateless_query_2 = \"What are the prerequisites for the first one?\"\n",
    "print(f\"üë§ User: {stateless_query_2}\")\n",
    "print(\"   Note: 'the first one' refers to the first course from Query 1\\n\")\n",
    "\n",
    "# Search courses (will search for \"prerequisites first one\" - not helpful)\n",
    "stateless_summaries_2, stateless_details_2 = await hierarchical_manager.hierarchical_search(\n",
    "    query=stateless_query_2,\n",
    "    summary_limit=3,\n",
    "    detail_limit=2\n",
    ")\n",
    "\n",
    "# Assemble context (NO conversation history from Query 1)\n",
    "stateless_retrieved_context_2 = context_assembler.assemble_hierarchical_context(\n",
    "    summaries=stateless_summaries_2,\n",
    "    details=stateless_details_2,\n",
    "    query=stateless_query_2\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "stateless_messages_2 = [\n",
    "    SystemMessage(content=stateless_system_prompt),\n",
    "    HumanMessage(\n",
    "        content=f\"{stateless_user_context}\\n\\n{stateless_retrieved_context_2}\\n\\nQuery: {stateless_query_2}\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "stateless_response_2 = llm.invoke(stateless_messages_2).content\n",
    "print(f\"\\nü§ñ Agent: {stateless_response_2}\")\n",
    "print(\"\\n‚ùå Agent can't resolve 'the first one' - no conversation history!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b0e0ae54a4b70",
   "metadata": {},
   "source": [
    "### üéØ What Just Happened?\n",
    "\n",
    "**Query 1:** \"I'm interested in machine learning courses\"\n",
    "- ‚úÖ Works fine - searches and returns ML courses\n",
    "\n",
    "**Query 2:** \"What are the prerequisites for **the first one**?\"\n",
    "- ‚ùå **Fails** - Agent doesn't know what \"the first one\" refers to\n",
    "- ‚ùå No conversation history stored\n",
    "- ‚ùå Each query is completely independent\n",
    "\n",
    "**The Problem:** Natural conversation requires context from previous turns.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ After: Memory-Enhanced RAG\n",
    "\n",
    "Now let's add memory to enable natural conversations.\n",
    "\n",
    "### Helper Function: Memory-Enhanced RAG with Hierarchical Retrieval\n",
    "\n",
    "This function combines all four context types with hierarchical retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ada2c98ee43badd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:21.319981Z",
     "iopub.status.busy": "2025-12-05T00:17:21.319760Z",
     "iopub.status.idle": "2025-12-05T00:17:21.325415Z",
     "shell.execute_reply": "2025-12-05T00:17:21.324991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory-enhanced RAG function created\n",
      "   Uses: Working memory + Long-term memory + Hierarchical RAG\n"
     ]
    }
   ],
   "source": [
    "async def memory_enhanced_rag_query(\n",
    "    user_query: str,\n",
    "    student_profile: StudentProfile,\n",
    "    session_id: str,\n",
    "    summary_limit: int = 3,\n",
    "    detail_limit: int = 2\n",
    ") -> str:\n",
    "    \"\"\"Generate response using memory-enhanced RAG with hierarchical retrieval\"\"\"\n",
    "\n",
    "    if not MEMORY_SERVER_AVAILABLE:\n",
    "        return \"‚ö†Ô∏è Memory Server not available\"\n",
    "\n",
    "    from agent_memory_client.filters import UserId\n",
    "\n",
    "    student_id = student_profile.email.split(\"@\")[0]\n",
    "\n",
    "    # 1. Load working memory (conversation history)\n",
    "    _, working_memory = await memory_client.get_or_create_working_memory(\n",
    "        session_id=session_id, user_id=student_id, model_name=\"gpt-4o\"\n",
    "    )\n",
    "\n",
    "    # Build conversation messages\n",
    "    conversation_messages = []\n",
    "    for msg in working_memory.messages:\n",
    "        if msg.role == \"user\":\n",
    "            conversation_messages.append(HumanMessage(content=msg.content))\n",
    "        elif msg.role == \"assistant\":\n",
    "            conversation_messages.append(AIMessage(content=msg.content))\n",
    "\n",
    "    # 2. Search long-term memory (user facts)\n",
    "    longterm_results = await memory_client.search_long_term_memory(\n",
    "        text=user_query, user_id=UserId(eq=student_id), limit=5\n",
    "    )\n",
    "    longterm_memories = (\n",
    "        [m.text for m in longterm_results.memories] if longterm_results.memories else []\n",
    "    )\n",
    "\n",
    "    # 3. Hierarchical RAG search (summaries ‚Üí details)\n",
    "    summaries, details = await hierarchical_manager.hierarchical_search(\n",
    "        query=user_query,\n",
    "        summary_limit=summary_limit,\n",
    "        detail_limit=detail_limit\n",
    "    )\n",
    "\n",
    "    # 4. Assemble all four context types\n",
    "    # System Context\n",
    "    system_prompt = \"\"\"You are a Redis University course advisor.\n",
    "\n",
    "Your role:\n",
    "- Help students find and enroll in courses from our catalog\n",
    "- Provide personalized recommendations based on available courses\n",
    "- Answer questions about courses, prerequisites, schedules\n",
    "\n",
    "CRITICAL RULES:\n",
    "- You can ONLY recommend courses that appear in the \"Relevant Courses\" list below\n",
    "- Do NOT suggest courses that are not in the \"Relevant Courses\" list\n",
    "- Use conversation history to resolve references (\"it\", \"that course\", \"the first one\")\n",
    "- Use long-term memories to personalize your recommendations\n",
    "- Be helpful, supportive, and encouraging\"\"\"\n",
    "\n",
    "    # User Context (profile + long-term memories)\n",
    "    user_context = f\"\"\"Student Profile:\n",
    "- Name: {student_profile.name}\n",
    "- Major: {student_profile.major}\n",
    "- Year: {student_profile.year}\n",
    "- Interests: {', '.join(student_profile.interests)}\n",
    "- Completed: {', '.join(student_profile.completed_courses)}\n",
    "- Current: {', '.join(student_profile.current_courses)}\n",
    "- Preferred Format: {student_profile.preferred_format.value}\n",
    "- Preferred Difficulty: {student_profile.preferred_difficulty.value}\"\"\"\n",
    "\n",
    "    if longterm_memories:\n",
    "        user_context += f\"\\n\\nLong-term Memories:\\n\" + \"\\n\".join(\n",
    "            [f\"- {m}\" for m in longterm_memories]\n",
    "        )\n",
    "\n",
    "    # Retrieved Context (hierarchical)\n",
    "    retrieved_context = context_assembler.assemble_hierarchical_context(\n",
    "        summaries=summaries,\n",
    "        details=details,\n",
    "        query=user_query\n",
    "    )\n",
    "\n",
    "    # 5. Build messages and generate response\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.extend(conversation_messages)  # Conversation Context\n",
    "    messages.append(\n",
    "        HumanMessage(\n",
    "            content=f\"{user_context}\\n\\n{retrieved_context}\\n\\nQuery: {user_query}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(messages).content\n",
    "\n",
    "    # 6. Save working memory (updated conversation)\n",
    "    working_memory.messages.extend(\n",
    "        [\n",
    "            MemoryMessage(role=\"user\", content=user_query),\n",
    "            MemoryMessage(role=\"assistant\", content=response),\n",
    "        ]\n",
    "    )\n",
    "    await memory_client.put_working_memory(\n",
    "        session_id=session_id,\n",
    "        memory=working_memory,\n",
    "        user_id=student_id,\n",
    "        model_name=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"‚úÖ Memory-enhanced RAG function created\")\n",
    "print(\"   Uses: Working memory + Long-term memory + Hierarchical RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e5c1bc501f416",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Complete Demo: Memory-Enhanced RAG\n",
    "\n",
    "Now let's test the complete system with a multi-turn conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "643a19d378897a68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:21.326815Z",
     "iopub.status.busy": "2025-12-05T00:17:21.326709Z",
     "iopub.status.idle": "2025-12-05T00:17:21.328810Z",
     "shell.execute_reply": "2025-12-05T00:17:21.328489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üß™ MEMORY-ENHANCED RAG DEMO\n",
      "================================================================================\n",
      "\n",
      "üë§ Student: Sarah Chen\n",
      "üìß Session: complete_demo_6ac53750\n"
     ]
    }
   ],
   "source": [
    "# Set up demo session\n",
    "demo_session_id = f\"complete_demo_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ MEMORY-ENHANCED RAG DEMO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüë§ Student: {sarah.name}\")\n",
    "print(f\"üìß Session: {demo_session_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833d0da7f03778b",
   "metadata": {},
   "source": [
    "### Turn 1: Initial Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ba1e4fc900497b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:21.330168Z",
     "iopub.status.busy": "2025-12-05T00:17:21.330054Z",
     "iopub.status.idle": "2025-12-05T00:17:24.797992Z",
     "shell.execute_reply": "2025-12-05T00:17:24.797304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç TURN 1: Initial Query\n",
      "================================================================================\n",
      "\n",
      "üë§ User: I'm interested in machine learning courses\n",
      "19:17:21 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/complete_demo_6ac53750?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:21 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/complete_demo_6ac53750?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:21 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:21 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'I'm interested in machine learning courses' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:22 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:22 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: I'm interested in machine learning courses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:22 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:22 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:24 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:24 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/complete_demo_6ac53750?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: Hi Sarah! It looks like we don't have any machine learning courses available at the moment. However, I can recommend some other courses that might align with your interests in data science and algorithms. Let's see what we have:\n",
      "\n",
      "### Relevant Courses:\n",
      "1. **Data Structures and Algorithms (CS301)**\n",
      "   - Format: Online\n",
      "   - Difficulty: Intermediate\n",
      "   - Description: Dive deeper into data structures and algorithms, building on what you've learned in CS201.\n",
      "\n",
      "2. **Introduction to Data Science (DS101)**\n",
      "   - Format: Online\n",
      "   - Difficulty: Intermediate\n",
      "   - Description: Explore the basics of data science, including data manipulation and visualization techniques.\n",
      "\n",
      "Given your background and interests, \"Data Structures and Algorithms (CS301)\" could be a great fit to further enhance your algorithm skills. Additionally, \"Introduction to Data Science (DS101)\" would be a good starting point to delve into data science concepts.\n",
      "\n",
      "Let me know if you would like more information on any of these courses or if you have any other questions!\n",
      "\n",
      "‚úÖ Conversation saved to working memory\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìç TURN 1: Initial Query\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "demo_query_1 = \"I'm interested in machine learning courses\"\n",
    "print(f\"\\nüë§ User: {demo_query_1}\")\n",
    "\n",
    "demo_response_1 = await memory_enhanced_rag_query(demo_query_1, sarah, demo_session_id)\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {demo_response_1}\")\n",
    "print(\"\\n‚úÖ Conversation saved to working memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373996c88eb205b9",
   "metadata": {},
   "source": [
    "### Turn 2: Follow-up with Pronoun Reference\n",
    "\n",
    "Now let's ask about \"the first one\" - a reference that requires conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9be49bea743df8bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:24.800343Z",
     "iopub.status.busy": "2025-12-05T00:17:24.800193Z",
     "iopub.status.idle": "2025-12-05T00:17:27.954206Z",
     "shell.execute_reply": "2025-12-05T00:17:27.953026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç TURN 2: Follow-up with Pronoun Reference\n",
      "================================================================================\n",
      "\n",
      "üë§ User: What are the prerequisites for the first one?\n",
      "   Note: 'the first one' refers to the first course mentioned in Turn 1\n",
      "19:17:24 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/complete_demo_6ac53750?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:25 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:25 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'What are the prerequisites for the first one?' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:25 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:25 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: What are the prerequisites for the first one?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:25 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:25 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:27 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:27 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/complete_demo_6ac53750?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: Hi Sarah! It seems like there was a bit of confusion with the search results. Based on your interests and completed courses, I previously recommended \"Data Structures and Algorithms (CS301)\" and \"Introduction to Data Science (DS101)\" as potential courses for you.\n",
      "\n",
      "### Prerequisites:\n",
      "\n",
      "1. **Data Structures and Algorithms (CS301)**\n",
      "   - Prerequisite: Completion of CS201 (which you've already completed)\n",
      "\n",
      "2. **Introduction to Data Science (DS101)**\n",
      "   - Prerequisite: None specified, but a basic understanding of programming and data manipulation is helpful.\n",
      "\n",
      "Both courses are offered online and are at an intermediate level, which matches your preferences. If you have any more questions or need further assistance, feel free to ask!\n",
      "\n",
      "‚úÖ Agent resolved 'the first one' using conversation history!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìç TURN 2: Follow-up with Pronoun Reference\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "demo_query_2 = \"What are the prerequisites for the first one?\"\n",
    "print(f\"\\nüë§ User: {demo_query_2}\")\n",
    "print(\"   Note: 'the first one' refers to the first course mentioned in Turn 1\")\n",
    "\n",
    "demo_response_2 = await memory_enhanced_rag_query(demo_query_2, sarah, demo_session_id)\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {demo_response_2}\")\n",
    "print(\"\\n‚úÖ Agent resolved 'the first one' using conversation history!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa82d08b39bb6c",
   "metadata": {},
   "source": [
    "### Turn 3: Another Follow-up\n",
    "\n",
    "Let's ask if the student meets the prerequisites mentioned in Turn 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "199a67faea2fcc7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:17:27.956401Z",
     "iopub.status.busy": "2025-12-05T00:17:27.956204Z",
     "iopub.status.idle": "2025-12-05T00:17:30.037039Z",
     "shell.execute_reply": "2025-12-05T00:17:30.036430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç TURN 3: Another Follow-up\n",
      "================================================================================\n",
      "\n",
      "üë§ User: Do I meet those prerequisites?\n",
      "   Note: 'those prerequisites' refers to prerequisites from Turn 2\n",
      "19:17:27 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/complete_demo_6ac53750?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:28 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:28 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'Do I meet those prerequisites?' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:28 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:28 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: Do I meet those prerequisites?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:28 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:28 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:30 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:17:30 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/complete_demo_6ac53750?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: Hi Sarah! Based on your completed courses and current enrollment, you meet the prerequisites for the courses I previously mentioned:\n",
      "\n",
      "1. **Data Structures and Algorithms (CS301)**\n",
      "   - Prerequisite: Completion of CS201 (which you've completed)\n",
      "\n",
      "2. **Introduction to Data Science (DS101)**\n",
      "   - Prerequisite: None specified, so you're all set to enroll.\n",
      "\n",
      "Both courses align well with your interests in algorithms and data science, and they are offered online at an intermediate level, which matches your preferences. If you have any more questions or need further assistance, feel free to ask!\n",
      "\n",
      "‚úÖ Agent resolved 'those prerequisites' and checked student's transcript!\n",
      "\n",
      "================================================================================\n",
      "‚úÖ DEMO COMPLETE: Memory-enhanced RAG enables natural conversations!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìç TURN 3: Another Follow-up\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "demo_query_3 = \"Do I meet those prerequisites?\"\n",
    "print(f\"\\nüë§ User: {demo_query_3}\")\n",
    "print(\"   Note: 'those prerequisites' refers to prerequisites from Turn 2\")\n",
    "\n",
    "demo_response_3 = await memory_enhanced_rag_query(demo_query_3, sarah, demo_session_id)\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {demo_response_3}\")\n",
    "print(\"\\n‚úÖ Agent resolved 'those prerequisites' and checked student's transcript!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ DEMO COMPLETE: Memory-enhanced RAG enables natural conversations!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0bda325b89750",
   "metadata": {},
   "source": [
    "### üéØ What Just Happened?\n",
    "\n",
    "**Turn 1:** \"I'm interested in machine learning courses\"\n",
    "- System uses hierarchical search (summaries ‚Üí details)\n",
    "- Finds ML-related courses\n",
    "- Responds with recommendations\n",
    "- **Saves conversation to working memory**\n",
    "\n",
    "**Turn 2:** \"What are the prerequisites for **the first one**?\"\n",
    "- System loads working memory (Turn 1)\n",
    "- Resolves \"the first one\" ‚Üí first course mentioned in Turn 1\n",
    "- Responds with prerequisites\n",
    "- **Saves updated conversation**\n",
    "\n",
    "**Turn 3:** \"Do I meet **those prerequisites**?\"\n",
    "- System loads working memory (Turns 1-2)\n",
    "- Resolves \"those prerequisites\" ‚Üí prerequisites from Turn 2\n",
    "- Checks student's completed courses from profile\n",
    "- Responds with personalized assessment\n",
    "\n",
    "**Key Insight:** Memory transforms stateless RAG into stateful, personalized, context-aware conversations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d53baf5137ef3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Before vs. After Comparison\n",
    "\n",
    "Let's visualize the difference between stateless and memory-enhanced RAG.\n",
    "\n",
    "### **Stateless RAG (Module 2):**\n",
    "\n",
    "```\n",
    "Query 1: \"I'm interested in ML courses\"\n",
    "  ‚Üí ‚úÖ Works (searches and returns courses)\n",
    "\n",
    "Query 2: \"What are the prerequisites for the first one?\"\n",
    "  ‚Üí ‚ùå Fails (no conversation history)\n",
    "  ‚Üí Agent: \"Which course are you referring to?\"\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- ‚ùå No conversation continuity\n",
    "- ‚ùå Can't resolve references\n",
    "- ‚ùå Each query is independent\n",
    "- ‚ùå Poor user experience\n",
    "\n",
    "### **Memory-Enhanced RAG (This Module):**\n",
    "\n",
    "```\n",
    "Query 1: \"I'm interested in ML courses\"\n",
    "  ‚Üí ‚úÖ Works (searches and returns courses)\n",
    "  ‚Üí Saves to working memory\n",
    "\n",
    "Query 2: \"What are the prerequisites for the first one?\"\n",
    "  ‚Üí ‚úÖ Works (loads conversation history)\n",
    "  ‚Üí Resolves \"the first one\" ‚Üí first course from Query 1\n",
    "  ‚Üí Responds with prerequisites\n",
    "  ‚Üí Saves updated conversation\n",
    "\n",
    "Query 3: \"Do I meet those prerequisites?\"\n",
    "  ‚Üí ‚úÖ Works (loads conversation history)\n",
    "  ‚Üí Resolves \"those prerequisites\" ‚Üí prerequisites from Query 2\n",
    "  ‚Üí Checks student transcript\n",
    "  ‚Üí Responds with personalized answer\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Conversation continuity\n",
    "- ‚úÖ Reference resolution\n",
    "- ‚úÖ Personalization\n",
    "- ‚úÖ Natural user experience\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### **1. Memory Transforms RAG**\n",
    "\n",
    "**Without Memory (Module 2):**\n",
    "- Stateless queries\n",
    "- No conversation continuity\n",
    "- Limited to 3 context types (System, User, Retrieved)\n",
    "\n",
    "**With Memory (This Module):**\n",
    "- Stateful conversations\n",
    "- Reference resolution\n",
    "- All 4 context types (System, User, Conversation, Retrieved)\n",
    "\n",
    "### **2. Two Types of Memory Work Together**\n",
    "\n",
    "**Working Memory:**\n",
    "- Session-scoped conversation history\n",
    "- Enables reference resolution\n",
    "- Persists within the session (like ChatGPT conversations)\n",
    "\n",
    "**Long-term Memory:**\n",
    "- User-scoped persistent facts\n",
    "- Enables personalization\n",
    "- Persists indefinitely\n",
    "\n",
    "### **3. Hierarchical Retrieval + Memory**\n",
    "\n",
    "**What We Built:**\n",
    "- Combined hierarchical RAG (summaries ‚Üí details) with memory\n",
    "- Progressive disclosure pattern from Module 2\n",
    "- Memory-enhanced context assembly\n",
    "- All four context types working together\n",
    "\n",
    "**Why This Matters:**\n",
    "- Efficient token usage (progressive disclosure)\n",
    "- Natural conversations (memory)\n",
    "- Personalization (long-term memory)\n",
    "- Foundation for agentic workflows (Module 5)\n",
    "\n",
    "### **4. All Four Context Types**\n",
    "\n",
    "| Context Type | Source | Purpose |\n",
    "|--------------|--------|---------|\n",
    "| **System Context** | Static prompt | Role, instructions, guidelines |\n",
    "| **User Context** | Profile + long-term memories | Personalization |\n",
    "| **Conversation Context** | Working memory | Reference resolution |\n",
    "| **Retrieved Context** | Hierarchical RAG | Relevant information |\n",
    "\n",
    "**Together:** Natural, stateful, personalized conversations\n",
    "\n",
    "**üí° Research Insight (From Module 1):** Context Rot research demonstrates that context structure and organization affect LLM attention. Memory systems that selectively retrieve and organize context outperform systems that dump all available information. This validates our approach: quality over quantity, semantic similarity, and selective retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## üèãÔ∏è Practice Exercises\n",
    "\n",
    "### **Exercise 1: Cross-Session Personalization**\n",
    "\n",
    "Modify the `memory_enhanced_rag_query` function to:\n",
    "1. Store user preferences in long-term memory when mentioned\n",
    "2. Use those preferences in future sessions\n",
    "3. Test with two different sessions for the same student\n",
    "\n",
    "**Hint:** Look for phrases like \"I prefer...\", \"I like...\", \"I want...\" and store them as semantic memories.\n",
    "\n",
    "### **Exercise 2: Memory-Aware Filtering**\n",
    "\n",
    "Enhance the hierarchical search to use long-term memories as filters:\n",
    "1. Search long-term memory for preferences (format, difficulty, schedule)\n",
    "2. Apply those preferences as filters to `hierarchical_manager.hierarchical_search()`\n",
    "3. Compare results with and without memory-aware filtering\n",
    "\n",
    "**Hint:** Use the `filters` parameter in the search methods.\n",
    "\n",
    "### **Exercise 3: Conversation Summarization**\n",
    "\n",
    "Implement a function that summarizes long conversations:\n",
    "1. When working memory exceeds 10 messages, summarize the conversation\n",
    "2. Store the summary in long-term memory\n",
    "3. Clear old messages from working memory (keep only recent 4)\n",
    "4. Test that reference resolution still works with summarized history\n",
    "\n",
    "**Hint:** Use the LLM to generate summaries, then store as semantic memories.\n",
    "\n",
    "### **Exercise 4: Multi-User Memory Management**\n",
    "\n",
    "Create a simple CLI that:\n",
    "1. Supports multiple students (different user IDs)\n",
    "2. Maintains separate working memory per session\n",
    "3. Maintains separate long-term memory per user\n",
    "4. Demonstrates cross-session continuity for each user\n",
    "\n",
    "**Hint:** Use different `session_id` and `user_id` for each student.\n",
    "\n",
    "### **Exercise 5: Memory Search Quality**\n",
    "\n",
    "Experiment with long-term memory search:\n",
    "1. Store 20+ diverse memories for a student\n",
    "2. Try different search queries\n",
    "3. Analyze which memories are retrieved\n",
    "4. Adjust memory text to improve search relevance\n",
    "\n",
    "**Hint:** More specific memory text leads to better semantic search results.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "### **What You Learned:**\n",
    "\n",
    "1. **The Grounding Problem** - Why agents need memory to resolve references\n",
    "2. **Working Memory** - Session-scoped conversation history for continuity\n",
    "3. **Long-term Memory** - Cross-session persistent knowledge for personalization\n",
    "4. **Memory Integration** - Combining memory with Module 2's hierarchical RAG system\n",
    "5. **Complete Context Engineering** - All four context types working together\n",
    "6. **Production Architecture** - Using Agent Memory Server for scalable memory\n",
    "\n",
    "### **What You Built:**\n",
    "\n",
    "- ‚úÖ Working memory demo (multi-turn conversations)\n",
    "- ‚úÖ Long-term memory demo (persistent knowledge)\n",
    "- ‚úÖ Complete memory-enhanced RAG system with hierarchical retrieval\n",
    "- ‚úÖ Integration of all four context types\n",
    "\n",
    "### **Key Functions:**\n",
    "\n",
    "- `memory_enhanced_rag_query()` - Complete memory + hierarchical RAG pipeline\n",
    "- Working memory operations - Load, save, update conversation history\n",
    "- Long-term memory operations - Store, search, filter persistent facts\n",
    "\n",
    "### **Architecture Pattern:**\n",
    "\n",
    "```\n",
    "User Query\n",
    "    ‚Üì\n",
    "Load Working Memory (conversation history)\n",
    "    ‚Üì\n",
    "Search Long-term Memory (user facts)\n",
    "    ‚Üì\n",
    "Hierarchical RAG Search (summaries ‚Üí details)\n",
    "    ‚Üì\n",
    "Assemble Context (System + User + Conversation + Retrieved)\n",
    "    ‚Üì\n",
    "Generate Response\n",
    "    ‚Üì\n",
    "Save Working Memory (updated conversation)\n",
    "```\n",
    "\n",
    "### **From Module 2 to Module 4:**\n",
    "\n",
    "**Module 2 (Stateless RAG):**\n",
    "- ‚ùå No conversation history\n",
    "- ‚ùå Each query independent\n",
    "- ‚ùå Can't resolve references\n",
    "- ‚úÖ Retrieves relevant documents\n",
    "- ‚úÖ Progressive disclosure (hierarchical)\n",
    "\n",
    "**Module 4 (Memory-Enhanced RAG):**\n",
    "- ‚úÖ Conversation history (working memory)\n",
    "- ‚úÖ Multi-turn conversations\n",
    "- ‚úÖ Reference resolution\n",
    "- ‚úÖ Persistent user knowledge (long-term memory)\n",
    "- ‚úÖ Personalization across sessions\n",
    "- ‚úÖ Progressive disclosure (hierarchical)\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "**Module 5** will add **tools** and **agentic workflows** using **LangGraph**, completing your journey from context engineering fundamentals to production-ready AI agents.\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully built a **memory-enhanced RAG system** that:\n",
    "- Remembers conversations (working memory)\n",
    "- Accumulates knowledge (long-term memory)\n",
    "- Resolves references naturally\n",
    "- Personalizes responses\n",
    "- Uses progressive disclosure (hierarchical retrieval)\n",
    "- Integrates all four context types\n",
    "\n",
    "**You're now ready for Module 5: Building Agents!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- [Agent Memory Server Documentation](https://github.com/redis/agent-memory-server) - Production-ready memory management\n",
    "- [Agent Memory Client](https://pypi.org/project/agent-memory-client/) - Python client for Agent Memory Server\n",
    "- [RedisVL Documentation](https://redisvl.com/) - Redis Vector Library\n",
    "- [LangChain Guide](https://python.langchain.com/docs/modules/memory/) - LangChain memory patterns\n",
    "- [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/) - Building agents with LangGraph\n",
    "\n",
    "---\n",
    "\n",
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "**Redis University - Context Engineering Workshop**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef5b878cca12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
