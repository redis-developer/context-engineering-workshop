{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fab7a26bf5c3a5",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Module 3: Chunking and Data Modeling for RAG\n",
    "\n",
    "## From Basic RAG to Production-Ready Knowledge Bases\n",
    "\n",
    "In Module 2, you built a working RAG system with hierarchical search. Now you'll learn the critical engineering decisions that separate toy demos from production systems: **when and how to chunk your data**.\n",
    "\n",
    "**The Critical Question:** Does my data need chunking?\n",
    "\n",
    "This module teaches you that **chunking is a design choice, not a default step**. Just like database schema design, how you structure your knowledge base dramatically affects retrieval quality, token efficiency, and system performance.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "**1. The \"Don't Chunk\" Strategy:**\n",
    "- When whole-document embedding is the right choice\n",
    "- Why structured records (courses, products, FAQs) often don't need chunking\n",
    "- How to recognize natural retrieval boundaries in your data\n",
    "\n",
    "**2. When Chunking Helps:**\n",
    "- Document types that benefit from chunking (research papers, long-form content)\n",
    "- Research-backed insights: \"Lost in the Middle\", \"Context Rot\"\n",
    "- How chunking improves retrieval precision\n",
    "\n",
    "**3. Chunking Strategies:**\n",
    "- Document-based (structure-aware): Split by sections/headers\n",
    "- Fixed-size (token-based): Using LangChain's RecursiveCharacterTextSplitter\n",
    "- Semantic (meaning-based): Using embeddings to detect topic shifts\n",
    "- Trade-offs and decision framework\n",
    "\n",
    "**4. Data Modeling for RAG:**\n",
    "- The hierarchical pattern: summaries + details\n",
    "- Engineering workflow: Extract â†’ Clean â†’ Transform â†’ Optimize â†’ Store\n",
    "- Real-world examples with Redis University course catalog\n",
    "\n",
    "**â±ï¸ Estimated Time:** 60-75 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Module 2: RAG Fundamentals and Implementation\n",
    "- Redis 8 running locally with course data loaded\n",
    "- OpenAI API key set\n",
    "- Understanding of vector embeddings and semantic search\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c04ea493c9b4a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a9f0372e941e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Handle both running from workshop/ directory and from project root\n",
    "if Path.cwd().name == \"workshop\":\n",
    "    project_root = Path.cwd().parent\n",
    "else:\n",
    "    project_root = Path.cwd()\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables from project root\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Verify required environment variables\n",
    "required_vars = [\"OPENAI_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\"\"âš ï¸  Missing required environment variables: {', '.join(missing_vars)}\n",
    "\n",
    "Please create a .env file with:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "REDIS_URL=redis://localhost:6379\n",
    "\"\"\")\n",
    "    sys.exit(1)\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(\"âœ… Environment variables loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d320da647edaf123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import redis\n",
    "import tiktoken\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Import hierarchical components (from Module 2)\n",
    "from redis_context_course.hierarchical_manager import HierarchicalCourseManager\n",
    "from redis_context_course.hierarchical_context import HierarchicalContextAssembler\n",
    "\n",
    "# Initialize\n",
    "hierarchical_manager = HierarchicalCourseManager(redis_client=redis.from_url(REDIS_URL, decode_responses=True))\n",
    "context_assembler = HierarchicalContextAssembler()\n",
    "redis_client = redis.from_url(REDIS_URL, decode_responses=True)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Token counter\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "print(\"âœ… Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2efd2fd5842a5e9",
   "metadata": {},
   "source": [
    "## Part 1: Data Modeling - The Foundation of RAG Quality\n",
    "\n",
    "### The Critical First Question: What is My Natural Retrieval Unit?\n",
    "\n",
    "Before thinking about chunking, ask: **\"What is the natural unit of information I want to retrieve?\"**\n",
    "\n",
    "This is similar to database design - you wouldn't store all customer data in one row, and you shouldn't embed all document content in one vector without thinking about retrieval patterns.\n",
    "\n",
    "**Examples of Natural Retrieval Units:**\n",
    "\n",
    "| Domain | Natural Unit | Why |\n",
    "|--------|-------------|-----|\n",
    "| **Course Catalog** | Individual course | Each course is self-contained, complete |\n",
    "| **Product Catalog** | Individual product | All product info should be retrieved together |\n",
    "| **FAQ Database** | Question + Answer pair | Q&A is an atomic unit |\n",
    "| **Research Papers** | Section or paragraph | Different sections answer different queries |\n",
    "| **Legal Contracts** | Clause or section | Need clause-level precision |\n",
    "| **Support Tickets** | Individual ticket | Single issue with context |\n",
    "\n",
    "Let's see this in practice with our course catalog:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5125abde3b3ad",
   "metadata": {},
   "source": [
    "### Example: Course Catalog - A Natural Retrieval Unit\n",
    "\n",
    "Let's examine a single course to understand why it's already an optimal retrieval unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b763338dc9b9e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:11:27 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "11:11:27 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "11:11:27 redis_context_course.hierarchical_manager INFO   Created summary index: course_summaries\n",
      "11:11:27 redis_context_course.hierarchical_manager INFO   Found 3 course summaries for query: programming courses\n",
      "ðŸ“š Sample Course: CS003\n",
      "================================================================================\n",
      "Title: Programming Fundamentals with C++\n",
      "Department: Computer Science\n",
      "Level: beginner\n",
      "Credits: 3\n",
      "Instructor: Angie Henderson\n",
      "\n",
      "Description:\n",
      "Core programming concepts using C++ for beginners.\n",
      "\n",
      "Prerequisites: None\n",
      "Tags: programming, c++, beginner, fundamentals, systems\n",
      "================================================================================\n",
      "\n",
      "Token count: 39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a sample course to analyze using search\n",
    "sample_courses = await hierarchical_manager.search_summaries(\n",
    "    query=\"programming courses\", limit=3\n",
    ")\n",
    "sample_course = sample_courses[0]  # Get first course\n",
    "\n",
    "# Generate embedding text if not present\n",
    "if not sample_course.embedding_text:\n",
    "    sample_course.generate_embedding_text()\n",
    "\n",
    "# Display the course summary\n",
    "print(f\"\"\"ðŸ“š Sample Course: {sample_course.course_code}\n",
    "{'=' * 80}\n",
    "Title: {sample_course.title}\n",
    "Department: {sample_course.department}\n",
    "Level: {sample_course.difficulty_level.value}\n",
    "Credits: {sample_course.credits}\n",
    "Instructor: {sample_course.instructor}\n",
    "\n",
    "Description:\n",
    "{sample_course.short_description}\n",
    "\n",
    "Prerequisites: {', '.join(sample_course.prerequisite_codes) if sample_course.prerequisite_codes else 'None'}\n",
    "Tags: {', '.join(sample_course.tags) if sample_course.tags else 'None'}\n",
    "{'=' * 80}\n",
    "\n",
    "Token count: {count_tokens(sample_course.embedding_text)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0c2bc03a500fb",
   "metadata": {},
   "source": [
    "### Analysis: Why Courses Don't Need Chunking\n",
    "\n",
    "**Semantic Completeness:** âœ… Each course is self-contained\n",
    "- All information about the course is in one record\n",
    "- No cross-references to other sections\n",
    "- Natural boundary exists (one course = one retrieval unit)\n",
    "\n",
    "**Query Patterns:** âœ… Users ask about specific courses or course types\n",
    "- \"What machine learning courses are available?\"\n",
    "- \"Tell me about CS016\"\n",
    "- \"What are the prerequisites for RU102JS?\"\n",
    "\n",
    "**Retrieval Precision:** âœ… Whole-course embedding maximizes relevance\n",
    "- When a user asks about a course, they need ALL the information\n",
    "- Splitting would fragment related information (e.g., separating prerequisites from description)\n",
    "- Each course is already the optimal retrieval unit\n",
    "\n",
    "**Token Efficiency:** âœ… Courses are reasonably sized (~150-200 tokens each)\n",
    "- Not too large (no wasted context)\n",
    "- Not too small (no fragmentation)\n",
    "\n",
    "**Decision:** âŒ **Don't chunk course data** - it's already optimally structured!\n",
    "\n",
    "This is the **\"don't chunk\" strategy** - a valid and often optimal choice for structured records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9ef0558e87e03",
   "metadata": {},
   "source": [
    "### The Hierarchical Pattern: A Better Data Model\n",
    "\n",
    "Instead of chunking, we use a **hierarchical pattern** with two tiers:\n",
    "\n",
    "**Tier 1: Summaries (Lightweight)**\n",
    "- Searchable, compact course overviews\n",
    "- Stored in vector index for fast retrieval\n",
    "- ~150-200 tokens each\n",
    "\n",
    "**Tier 2: Details (On-Demand)**\n",
    "- Full course information with all fields\n",
    "- Retrieved only when needed\n",
    "- Stored as plain Redis keys (not in vector index)\n",
    "\n",
    "This is **data modeling**, not chunking - we're structuring data for optimal retrieval patterns.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d10899a005e07b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:11:27 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'beginner programming courses' (summaries=5, details=3)\n",
      "11:11:29 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "11:11:29 redis_context_course.hierarchical_manager INFO   Found 5 course summaries for query: beginner programming courses\n",
      "11:11:29 redis_context_course.hierarchical_manager INFO   Fetched 3 course details\n",
      "11:11:29 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 5 summaries, 3 details\n",
      "ðŸ” Query: \"beginner programming courses\"\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Tier 1: Summary Results (5 courses)\n",
      "\n",
      "1. CS001: Introduction to Programming with Python (DifficultyLevel.BEGINNER)\n",
      "2. CS002: Web Development Fundamentals (DifficultyLevel.BEGINNER)\n",
      "3. CS003: Programming Fundamentals with C++ (DifficultyLevel.BEGINNER)\n",
      "4. CS012: Machine Learning Fundamentals (DifficultyLevel.ADVANCED)\n",
      "5. CS006: Web Development (DifficultyLevel.INTERMEDIATE)\n",
      "\n",
      "================================================================================\n",
      "ðŸ“„ Tier 2: Detailed Information (top 3 courses)\n",
      "\n",
      "\n",
      "CS001: Introduction to Programming with Python\n",
      "Department: Computer Science | Credits: 3\n",
      "Prerequisites: None\n",
      "\n",
      "Description: Learn programming fundamentals using Python for beginners. This course introduces students to computational thinking, problem-solving, and programming basics. No prior experience required. Students wi...\n",
      "\n",
      "\n",
      "CS002: Web Development Fundamentals\n",
      "Department: Computer Science | Credits: 3\n",
      "Prerequisites: None\n",
      "\n",
      "Description: Introduction to web programming with HTML, CSS, and JavaScript. This beginner-friendly course teaches students how to build interactive websites from scratch. No prior programming experience required....\n",
      "\n",
      "\n",
      "CS003: Programming Fundamentals with C++\n",
      "Department: Computer Science | Credits: 3\n",
      "Prerequisites: None\n",
      "\n",
      "Description: Core programming concepts using C++ for beginners. This course introduces students to programming fundamentals using C++, focusing on computational thinking, problem-solving, and software development....\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š Context Statistics:\n",
      "- Summaries: 5 courses\n",
      "- Details: 3 courses\n",
      "- Total tokens: 3,486\n",
      "- Retrieval pattern: Hierarchical (summaries + details)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical retrieval example\n",
    "query = \"beginner programming courses\"\n",
    "\n",
    "# Tier 1: Search summaries (fast, lightweight)\n",
    "summaries, details = await hierarchical_manager.hierarchical_search(\n",
    "    query=query,\n",
    "    summary_limit=5,  # Get 5 summary matches\n",
    "    detail_limit=3,   # Fetch full details for top 3\n",
    ")\n",
    "\n",
    "print(f\"\"\"ðŸ” Query: \"{query}\"\n",
    "{'=' * 80}\n",
    "\n",
    "ðŸ“Š Tier 1: Summary Results (5 courses)\n",
    "\"\"\")\n",
    "\n",
    "for i, summary in enumerate(summaries, 1):\n",
    "    print(f\"{i}. {summary.course_code}: {summary.title} ({summary.difficulty_level})\")\n",
    "\n",
    "print(f\"\"\"\n",
    "{'=' * 80}\n",
    "ðŸ“„ Tier 2: Detailed Information (top 3 courses)\n",
    "\"\"\")\n",
    "\n",
    "for detail in details:\n",
    "    prereq_codes = [p.course_code for p in detail.prerequisites] if detail.prerequisites else []\n",
    "    print(f\"\"\"\n",
    "{detail.course_code}: {detail.title}\n",
    "Department: {detail.department} | Credits: {detail.credits}\n",
    "Prerequisites: {', '.join(prereq_codes) if prereq_codes else 'None'}\n",
    "\n",
    "Description: {detail.full_description[:200]}...\n",
    "\"\"\")\n",
    "\n",
    "# Assemble context\n",
    "context = context_assembler.assemble_hierarchical_context(summaries, details, query)\n",
    "context_tokens = count_tokens(context)\n",
    "\n",
    "print(f\"\"\"\n",
    "{'=' * 80}\n",
    "ðŸ“Š Context Statistics:\n",
    "- Summaries: 5 courses\n",
    "- Details: 3 courses\n",
    "- Total tokens: {context_tokens:,}\n",
    "- Retrieval pattern: Hierarchical (summaries + details)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0dbcd5795aedec",
   "metadata": {},
   "source": [
    "**Key Takeaway:** For structured records like courses, the hierarchical pattern (summaries + details) is superior to chunking because it respects natural data boundaries and retrieval patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: When Documents DO Need Chunking\n",
    "\n",
    "Now let's look at a completely different type of data: **long-form documents** with multiple distinct topics.\n",
    "\n",
    "### Example: Research Paper\n",
    "\n",
    "Let's load a real research paper about semantic caching for LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb0dc692ca494be9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pypdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the actual research paper PDF\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpypdf\u001b[39;00m\n\u001b[32m      4\u001b[39m pdf_path = project_root / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33marxiv_2504_02268.pdf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m reader = pypdf.PdfReader(pdf_path)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pypdf'"
     ]
    }
   ],
   "source": [
    "# Load the actual research paper PDF\n",
    "import pypdf\n",
    "\n",
    "pdf_path = project_root / \"data\" / \"arxiv_2504_02268.pdf\"\n",
    "reader = pypdf.PdfReader(pdf_path)\n",
    "\n",
    "# Extract text from all pages\n",
    "research_paper = \"\"\n",
    "for page in reader.pages:\n",
    "    research_paper += page.extract_text() + \"\\n\"\n",
    "\n",
    "paper_tokens = count_tokens(research_paper)\n",
    "print(f\"\"\"ðŸ“„ Real Research Paper\n",
    "{'=' * 80}\n",
    "Title: \"Advancing Semantic Caching for LLMs with Domain-Specific Embeddings\"\n",
    "Authors: Waris Gill et al. (Redis & Virginia Tech, 2025)\n",
    "Source: arXiv:2504.02268\n",
    "\n",
    "Structure:\n",
    "- Abstract\n",
    "- Introduction\n",
    "- Background and Related Work\n",
    "- Methodology (Synthetic Data Generation)\n",
    "- Evaluation and Results\n",
    "- Conclusion\n",
    "\n",
    "Pages: {len(reader.pages)}\n",
    "Token count: {paper_tokens:,}\n",
    "Characters: {len(research_paper):,}\n",
    "{'=' * 80}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7e06337016528",
   "metadata": {},
   "source": [
    "### Analysis: Why This Research Paper NEEDS Chunking\n",
    "\n",
    "Let's compare the course catalog (doesn't need chunking) with the research paper (does need chunking):\n",
    "\n",
    "| Factor | Course Catalog | Research Paper |\n",
    "|--------|---------------|----------------|\n",
    "| **Document Structure** | Single topic per record | Multiple distinct sections |\n",
    "| **Semantic Completeness** | Each course is self-contained | Sections cover different topics and types (text, formulas, charts, etc.) |\n",
    "| **Query Patterns** | \"Show me CS courses\" | \"How is synthetic data generated?\" |\n",
    "| **Optimal Retrieval Unit** | Whole course | Specific section |\n",
    "| **Chunking Decision** | âŒ Don't chunk | âœ… Chunk by section |\n",
    "\n",
    "**Why the research paper needs chunking:**\n",
    "\n",
    "**1. Multiple Distinct Topics:**\n",
    "- Abstract, Introduction, Methodology, Evaluation, Conclusion each cover different aspects\n",
    "- A query about \"synthetic data generation\" only needs the Methodology section, not the entire paper\n",
    "\n",
    "**2. Retrieval Precision:**\n",
    "- Without chunking: Retrieve entire ~6,000-token paper for every query\n",
    "- With chunking: Retrieve only the 300-500 token section that's relevant\n",
    "- Result: 85-90% reduction in irrelevant context\n",
    "\n",
    "**3. Query-Specific Needs:**\n",
    "\n",
    "| Query | Needs | Without Chunking | With Chunking |\n",
    "|-------|-------|------------------|---------------|\n",
    "| \"How is synthetic data generated?\" | Methodology section | Entire paper (~6,000 tokens) | Methodology (~500 tokens) |\n",
    "| \"What were the hit rate results?\" | Evaluation + Tables | Entire paper (~6,000 tokens) | Evaluation (~400 tokens) |\n",
    "| \"What embedding models were tested?\" | Results section | Entire paper (~6,000 tokens) | Results (~300 tokens) |\n",
    "| \"What is semantic caching?\" | Introduction + Background | Entire paper (~6,000 tokens) | Intro+Background (~600 tokens) |\n",
    "\n",
    "**Impact:** 8-12x reduction in irrelevant context, leading to faster responses and better quality.\n",
    "\n",
    "**ðŸ’¡ Key Insight:** Chunking isn't about fitting in context windows - it's about **data modeling for retrieval**. Just like you wouldn't store all customer data in one database row, you shouldn't embed all document content in one vector when sections serve different purposes.\n",
    "\n",
    "### Research Background: Why Chunking Matters\n",
    "\n",
    "Even with large context windows (128K+ tokens), research shows that **how you structure context matters more than fitting everything in**.\n",
    "\n",
    "**Key Research Findings:**\n",
    "\n",
    "**1. \"Lost in the Middle\" (Stanford/UC Berkeley, 2023)** - [arXiv:2307.03172](https://arxiv.org/abs/2307.03172)\n",
    "- LLMs exhibit **U-shaped attention**: high recall at beginning/end, degraded in middle\n",
    "- **Implication:** Chunking ensures relevant sections are retrieved and placed prominently\n",
    "\n",
    "**2. \"Context Rot\" (Chroma Research, 2025)** - [research.trychroma.com/context-rot](https://research.trychroma.com/context-rot)\n",
    "- Performance degrades as input length increases, even when relevant info is present\n",
    "- **Distractor effect**: Irrelevant content actively hurts model performance\n",
    "- **Implication:** Smaller, focused chunks reduce \"distractor tokens\"\n",
    "\n",
    "**3. Needle in the Haystack (NIAH)** - [github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)\n",
    "- Models often fail to retrieve information buried in long context\n",
    "- **Implication:** For structured data, NIAH is irrelevantâ€”each record IS the needle\n",
    "\n",
    "**The Takeaway:** These findings inform design decisions but don't prescribe universal rules. Structured records (courses, products) don't need chunking. Long-form documents (papers, books) benefit from chunking. Experiment with YOUR data.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Core Chunking Strategies\n",
    "\n",
    "Now that we understand **when** to chunk (long-form documents with multiple topics) and **when not to** (structured records), let's explore **how** to chunk effectively.\n",
    "\n",
    "There's no single \"best\" strategy - the optimal approach depends on YOUR data characteristics and query patterns.\n",
    "\n",
    "We'll explore three core approaches with hands-on examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d079e72743b37",
   "metadata": {},
   "source": [
    "### Strategy 1: Document-Based Chunking (Structure-Aware)\n",
    "\n",
    "**Concept:** Split documents based on their inherent structure (sections, paragraphs, headings).\n",
    "\n",
    "**Best for:** Structured documents with clear logical divisions (research papers, technical docs, books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14864605b206341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Document-Based Chunking\n",
    "# Split research paper by sections (using markdown headers)\n",
    "\n",
    "\n",
    "def chunk_by_structure(text: str, separator: str = \"\\n## \") -> List[str]:\n",
    "    \"\"\"Split text by structural markers (e.g., markdown headers).\"\"\"\n",
    "\n",
    "    # Split by headers\n",
    "    sections = text.split(separator)\n",
    "\n",
    "    # Clean and format chunks\n",
    "    chunks = []\n",
    "    for i, section in enumerate(sections):\n",
    "        if section.strip():\n",
    "            # Add header back (except for first chunk which is title)\n",
    "            if i > 0:\n",
    "                chunk = \"## \" + section\n",
    "            else:\n",
    "                chunk = section\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Apply to research paper\n",
    "structure_chunks = chunk_by_structure(research_paper)\n",
    "\n",
    "print(f\"\"\"ðŸ“Š Strategy 1: Document-Based (Structure-Aware) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Number of chunks: {len(structure_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\")\n",
    "\n",
    "for i, chunk in enumerate(structure_chunks):\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    # Show first 100 chars of each chunk\n",
    "    preview = chunk[:300].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f523c504991979f",
   "metadata": {},
   "source": [
    "**Strategy 1 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- Respects document structure (sections stay together)\n",
    "- Semantically coherent (each chunk is a complete section)\n",
    "- Easy to implement for structured documents\n",
    "- **Keeps tables, formulas, and code WITH their context**\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Variable chunk sizes (some sections longer than others)\n",
    "- Requires documents to have clear structure\n",
    "- May create chunks that are still too large\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Research papers with clear sections\n",
    "- Technical documentation with headers\n",
    "- Books with chapters/sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108caf4be1a606b4",
   "metadata": {},
   "source": [
    "### Strategy 2: Fixed-Size Chunking (Token-Based)\n",
    "\n",
    "**Concept:** Split text into chunks of a predetermined size (e.g., 512 tokens) with overlap.\n",
    "\n",
    "**Best for:** Unstructured text, quick prototyping, when you need consistent chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca013174da787352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Fixed-Size Chunking (Using LangChain)\n",
    "# Industry-standard approach with smart boundary detection\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter with smart boundary detection\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # Target chunk size in characters\n",
    "    chunk_overlap=100,  # Overlap to preserve context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Try these in order\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "print(\"ðŸ”„ Running fixed-size chunking with LangChain...\")\n",
    "print(\"   Trying to split on: paragraphs â†’ sentences â†’ words â†’ characters\\n\")\n",
    "\n",
    "# Apply to research paper\n",
    "fixed_chunks_docs = text_splitter.create_documents([research_paper])\n",
    "fixed_chunks = [doc.page_content for doc in fixed_chunks_docs]\n",
    "\n",
    "print(f\"\"\"ðŸ“Š Strategy 2: Fixed-Size (LangChain) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Target chunk size: 800 characters (~200 words)\n",
    "Overlap: 100 characters\n",
    "Number of chunks: {len(fixed_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\")\n",
    "\n",
    "for i, chunk in enumerate(fixed_chunks[:5]):  # Show first 5\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:100].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "print(f\"... ({len(fixed_chunks) - 5} more chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70130fcdc66f34",
   "metadata": {},
   "source": [
    "**Strategy 2 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- **Respects natural boundaries**: Tries paragraphs â†’ sentences â†’ words â†’ characters\n",
    "- Consistent chunk sizes (predictable token usage)\n",
    "- Works on any text (structured or unstructured)\n",
    "- **Doesn't split mid-sentence** (unless absolutely necessary)\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Ignores document structure (doesn't understand sections)\n",
    "- Can break semantic coherence (may split related content)\n",
    "- Overlap creates redundancy (increases storage/cost)\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Unstructured text (no clear sections)\n",
    "- Quick prototyping and baselines\n",
    "- When consistent chunk sizes are required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afda8cda87c128",
   "metadata": {},
   "source": [
    "### Strategy 3: Semantic Chunking (Meaning-Based)\n",
    "\n",
    "**Concept:** Split text based on semantic similarity using embeddings - create new chunks when topic changes significantly.\n",
    "\n",
    "**How it works:**\n",
    "1. Split text into sentences or paragraphs\n",
    "2. Generate embeddings for each segment\n",
    "3. Calculate similarity between consecutive segments\n",
    "4. Create chunk boundaries where similarity drops (topic shift detected)\n",
    "\n",
    "**Best for:** Dense academic text, legal documents, narratives where semantic boundaries don't align with structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d3ece2f9f06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Semantic Chunking (Using LangChain)\n",
    "# Industry-standard approach with local embeddings (no API costs!)\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Suppress tokenizer warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Initialize local embeddings (no API costs!)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "# Create semantic chunker with percentile-based breakpoint detection\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",  # Split at bottom 25% of similarities\n",
    "    breakpoint_threshold_amount=25,  # 25th percentile\n",
    "    buffer_size=1,  # Compare consecutive sentences\n",
    ")\n",
    "\n",
    "print(\"ðŸ”„ Running semantic chunking with LangChain...\")\n",
    "print(\"   Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\")\n",
    "print(\"   Breakpoint detection: 25th percentile of similarity scores\\n\")\n",
    "\n",
    "# Apply to research paper\n",
    "semantic_chunks_docs = semantic_chunker.create_documents([research_paper])\n",
    "\n",
    "# Extract text from Document objects\n",
    "semantic_chunks = [doc.page_content for doc in semantic_chunks_docs]\n",
    "\n",
    "print(f\"\"\"ðŸ“Š Strategy 3: Semantic (LangChain) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Number of chunks: {len(semantic_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\")\n",
    "\n",
    "for i, chunk in enumerate(semantic_chunks[:5]):  # Show first 5\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:100].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "if len(semantic_chunks) > 5:\n",
    "    print(f\"... ({len(semantic_chunks) - 5} more chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01f496f5384cbe",
   "metadata": {},
   "source": [
    "**Strategy 3 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- **Meaning-aware**: Chunks based on topic shifts, not arbitrary boundaries\n",
    "- **Adaptive**: Chunk sizes vary based on content coherence\n",
    "- **Better retrieval**: Each chunk is semantically focused\n",
    "- **Free**: Uses local embeddings (no API costs)\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Slower processing (requires embedding generation)\n",
    "- Variable chunk sizes (harder to predict token usage)\n",
    "- May not respect document structure (sections, headers)\n",
    "- Requires tuning (threshold, buffer size)\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Dense academic text\n",
    "- Legal documents\n",
    "- Narratives and stories\n",
    "- Content where semantic boundaries don't align with structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39c7fe9faa9a03",
   "metadata": {},
   "source": [
    "### Comparing Chunking Strategies: Decision Framework\n",
    "\n",
    "Now let's compare all strategies side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d0828e8512bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{'=' * 80}\n",
    "CHUNKING STRATEGY COMPARISON\n",
    "{'=' * 80}\n",
    "\n",
    "Document: Research Paper ({paper_tokens:,} tokens)\n",
    "\n",
    "Strategy              | Chunks | Avg Size | Complexity | Best For\n",
    "--------------------- | ------ | -------- | ---------- | --------\n",
    "Document-Based        | {len(structure_chunks):>6} | {sum(count_tokens(c) for c in structure_chunks) // len(structure_chunks):>8} | Low        | Structured docs\n",
    "Fixed-Size            | {len(fixed_chunks):>6} | {sum(count_tokens(c) for c in fixed_chunks) // len(fixed_chunks):>8} | Low        | Unstructured text\n",
    "Semantic              | {len(semantic_chunks):>6} | {sum(count_tokens(c) for c in semantic_chunks) // len(semantic_chunks):>8} | High       | Dense academic text\n",
    "\n",
    "{'=' * 80}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399cd8dfad49c0af",
   "metadata": {},
   "source": [
    "### YOUR Chunking Decision Framework\n",
    "\n",
    "Chunking strategy is a **design choice** that depends on your specific context. There's no universal \"correct\" chunk size.\n",
    "\n",
    "**Step 1: Start with Document Type**\n",
    "\n",
    "| Document Type | Default Approach | Reasoning |\n",
    "|---------------|------------------|----------|\n",
    "| **Structured records** (courses, products, FAQs) | Don't chunk | Natural boundaries already exist |\n",
    "| **Long-form text** (papers, books, docs) | Consider chunking | May need retrieval precision |\n",
    "| **PDFs with visual layout** | Page-level | Preserves tables, figures |\n",
    "| **Code** | Function/class boundaries | Semantic structure matters |\n",
    "\n",
    "**Step 2: Evaluate These Factors**\n",
    "\n",
    "1. **Semantic completeness:** Is each item self-contained?\n",
    "   - âœ… Yes â†’ Don't chunk (preserve natural boundaries)\n",
    "   - âŒ No â†’ Consider chunking strategy\n",
    "\n",
    "2. **Query patterns:** What will users ask?\n",
    "   - Specific facts â†’ Smaller, focused chunks help\n",
    "   - Summaries/overviews â†’ Larger chunks or hierarchical\n",
    "   - Mixed â†’ Consider hierarchical approach\n",
    "\n",
    "3. **Topic density:** How many distinct topics per document?\n",
    "   - Single topic â†’ Whole-document embedding often works\n",
    "   - Multiple distinct topics â†’ Chunking may improve precision\n",
    "\n",
    "**Example Decisions:**\n",
    "\n",
    "| Domain | Data Characteristics | Decision | Why |\n",
    "|--------|---------------------|----------|-----|\n",
    "| **Course Catalog** | Small, self-contained records | **Don't chunk** | Each course is a complete retrieval unit |\n",
    "| **Research Papers** | Multi-section, dense topics | Document-Based | Sections are natural semantic units |\n",
    "| **Support Tickets** | Single issue per ticket | **Don't chunk** | Already at optimal granularity |\n",
    "| **Legal Contracts** | Nested structure, many clauses | Hierarchical | Need both overview and clause-level detail |\n",
    "\n",
    "> ðŸ’¡ **Key Takeaway:** Ask \"What is my natural retrieval unit?\" before deciding on a chunking strategy. For many structured data use cases, the answer is \"don't chunk.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c058c97c25c9b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Advanced Example - Research Paper with Multimodal Content\n",
    "\n",
    "You've learned the three core chunking strategies. Now let's apply them to a **real-world research paper** and tackle a common challenge: **multimodal content** (tables, formulas, figures).\n",
    "\n",
    "**The Challenge:** Research papers aren't just text - they contain:\n",
    "- **Tables** with structured data\n",
    "- **Formulas** with variable definitions\n",
    "- **Figures** with visual patterns\n",
    "- **Code** with implementation details\n",
    "\n",
    "Standard text chunking can break these elements. Let's see how to handle them properly using the actual arXiv paper on semantic caching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fde7b8f9ea7523",
   "metadata": {},
   "source": [
    "### Real-World Example: Chunking a Research Paper with Multimodal Content\n",
    "\n",
    "Research papers contain heterogeneous content that requires specialized handling:\n",
    "- **Text**: Paragraphs, sections, abstracts\n",
    "- **Tables**: Structured data with captions\n",
    "- **Figures**: Visual information with descriptions\n",
    "- **Formulas**: Equations with variable definitions\n",
    "- **Code**: Implementation examples\n",
    "\n",
    "Let's apply our chunking strategies to the actual arXiv paper and see how to handle each content type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abdc374a1a3453",
   "metadata": {},
   "source": [
    "**Paper:** [\"Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and Synthetic Data\"](https://arxiv.org/abs/2504.02268)\n",
    "**Authors:** Waris Gill et al. (Redis & Virginia Tech, 2025)\n",
    "**Length:** 12 pages, ~42,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e90434abf7470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the actual research paper PDF\n",
    "import pypdf\n",
    "\n",
    "pdf_path = project_root / \"data\" / \"arxiv_2504_02268.pdf\"\n",
    "reader = pypdf.PdfReader(pdf_path)\n",
    "\n",
    "# Extract text from all pages\n",
    "full_text = \"\"\n",
    "for page in reader.pages:\n",
    "    full_text += page.extract_text() + \"\\n\"\n",
    "\n",
    "print(f\"\"\"âœ… PDF loaded successfully:\n",
    "  Pages: {len(reader.pages)}\n",
    "  Characters: {len(full_text):,}\n",
    "  File: {pdf_path.name}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ae3f2b2e41f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Page-based chunking (simplest approach for PDFs)\n",
    "page_chunks = []\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    page_chunks.append({\n",
    "        \"page\": i + 1,\n",
    "        \"content\": text,\n",
    "        \"char_count\": len(text)\n",
    "    })\n",
    "\n",
    "print(\"PAGE-BASED CHUNKING:\")\n",
    "print(f\"Number of chunks: {len(page_chunks)}\")\n",
    "print(f\"Average chunk size: {sum(c['char_count'] for c in page_chunks) // len(page_chunks):,} chars\")\n",
    "print(f\"\\nFirst 3 pages:\")\n",
    "for chunk in page_chunks[:3]:\n",
    "    preview = chunk['content'][:100].replace('\\n', ' ')\n",
    "    print(f\"  Page {chunk['page']}: {preview}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13480a3c7450005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Fixed-size chunking with overlap (using full text)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "fixed_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    ")\n",
    "\n",
    "fixed_chunks = fixed_splitter.split_text(full_text)\n",
    "print(\"FIXED-SIZE CHUNKING (1000 chars, 150 overlap):\")\n",
    "print(f\"Number of chunks: {len(fixed_chunks)}\")\n",
    "print(f\"Average chunk size: {sum(len(c) for c in fixed_chunks) // len(fixed_chunks):,} chars\")\n",
    "print(f\"\\nFirst chunk preview:\\n{fixed_chunks[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba68f2b5bb9c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the approaches\n",
    "print(\"CHUNKING STRATEGY COMPARISON:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Strategy':<25} {'Chunks':<10} {'Avg Size':<15} {'Best For'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Page-based':<25} {len(page_chunks):<10} {f'{sum(c[\"char_count\"] for c in page_chunks) // len(page_chunks):,} chars':<15} {'Preserves layout/tables'}\")\n",
    "print(f\"{'Fixed-size (1000)':<25} {len(fixed_chunks):<10} {f'{sum(len(c) for c in fixed_chunks) // len(fixed_chunks):,} chars':<15} {'Uniform retrieval'}\")\n",
    "print()\n",
    "print(\"RECOMMENDATION: For this PDF:\")\n",
    "print(\"  - Page-based: Best for preserving tables, figures, and layout\")\n",
    "print(\"  - Fixed-size: Better for semantic search across the full text\")\n",
    "print(\"  - In production: Combine both (page metadata + semantic chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bacb84aa7e55ff",
   "metadata": {},
   "source": [
    "### Handling Multimodal Content: Tables, Formulas, Figures\n",
    "\n",
    "**The Challenge:** Standard text chunking can break tables, formulas, and figures. Let's extract and chunk these properly from our PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa799680fdaaef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and chunk a table from the PDF\n",
    "# The paper contains Table 1 on page 6 comparing embedding models\n",
    "\n",
    "import re\n",
    "\n",
    "# Find table content in the text\n",
    "table_pattern = r'(Table \\d+:.*?)(?=\\n\\n[A-Z]|\\nFigure|\\n\\d+\\.|\\Z)'\n",
    "tables_found = re.findall(table_pattern, full_text, re.DOTALL)\n",
    "\n",
    "if tables_found:\n",
    "    table_chunk = {\n",
    "        \"content_type\": \"table\",\n",
    "        \"text\": tables_found[0][:500],  # First 500 chars\n",
    "        \"metadata\": {\n",
    "            \"page\": \"6\",\n",
    "            \"section\": \"Evaluation\",\n",
    "            \"table_id\": \"Table 1\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"âœ… TABLE CHUNKING EXAMPLE:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Content Type: {table_chunk['content_type']}\")\n",
    "    print(f\"Metadata: {table_chunk['metadata']}\")\n",
    "    print(f\"\\nChunk Text:\\n{table_chunk['text'][:300]}...\")\n",
    "    print(\"\\nâœ… Best Practice: Keep table WITH caption and surrounding context\")\n",
    "else:\n",
    "    print(\"Table extraction pattern needs adjustment for this PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a40d54-51a4-4a64-bfd5-fea936fac66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and chunk formulas/equations\n",
    "# The paper discusses contrastive loss functions\n",
    "\n",
    "formula_pattern = r'(loss.*?=.*?(?:\\n|$))'\n",
    "formulas = re.findall(formula_pattern, full_text, re.IGNORECASE)\n",
    "\n",
    "if formulas:\n",
    "    # Find context around the formula\n",
    "    formula_text = formulas[0]\n",
    "    formula_idx = full_text.find(formula_text)\n",
    "    context_start = max(0, formula_idx - 200)\n",
    "    context_end = min(len(full_text), formula_idx + len(formula_text) + 200)\n",
    "\n",
    "    formula_chunk = {\n",
    "        \"content_type\": \"formula\",\n",
    "        \"text\": full_text[context_start:context_end],\n",
    "        \"metadata\": {\n",
    "            \"section\": \"Methodology\",\n",
    "            \"formula_type\": \"contrastive_loss\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"\\nâœ… FORMULA CHUNKING EXAMPLE:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Content Type: {formula_chunk['content_type']}\")\n",
    "    print(f\"Metadata: {formula_chunk['metadata']}\")\n",
    "    print(f\"\\nChunk Text:\\n{formula_chunk['text'][:300]}...\")\n",
    "    print(\"\\nâœ… Best Practice: Keep formula WITH variable definitions and explanation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe26ea7c7819e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and chunk figure descriptions\n",
    "# The paper has multiple figures comparing model performance\n",
    "\n",
    "figure_pattern = r'(Figure \\d+:.*?)(?=\\n\\n[A-Z]|\\nTable|\\n\\d+\\.|\\Z)'\n",
    "figures = re.findall(figure_pattern, full_text, re.DOTALL)\n",
    "\n",
    "if figures:\n",
    "    figure_chunk = {\n",
    "        \"content_type\": \"figure\",\n",
    "        \"text\": figures[0][:400],\n",
    "        \"metadata\": {\n",
    "            \"section\": \"Evaluation\",\n",
    "            \"figure_id\": \"Figure 1\",\n",
    "            \"visual_type\": \"bar_chart\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"\\nâœ… FIGURE CHUNKING EXAMPLE:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Content Type: {figure_chunk['content_type']}\")\n",
    "    print(f\"Metadata: {figure_chunk['metadata']}\")\n",
    "    print(f\"\\nChunk Text:\\n{figure_chunk['text'][:300]}...\")\n",
    "    print(\"\\nâœ… Best Practice: Describe visual patterns in text, keep WITH caption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d7ac9317435a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Multimodal chunking principles\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MULTIMODAL CHUNKING PRINCIPLES:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. **Tables**: Keep WITH caption and explanation\n",
    "   - Preserve structure (markdown/HTML)\n",
    "   - Add metadata: table_id, section, content_type\n",
    "\n",
    "2. **Formulas**: Keep WITH variable definitions\n",
    "   - Include surrounding context (Â±200 chars)\n",
    "   - Preserve LaTeX if available\n",
    "\n",
    "3. **Figures**: Describe visual patterns in text\n",
    "   - Keep caption WITH discussion\n",
    "   - Add metadata: figure_id, visual_type\n",
    "\n",
    "4. **Code**: Keep WITH usage examples and context\n",
    "   - Preserve syntax and comments\n",
    "   - Include function/class definitions\n",
    "\n",
    "5. **General Rule**: Context is king - never separate content from explanation\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ac0a39655cc",
   "metadata": {},
   "source": [
    "### Advanced Topic: When Chunking Isn't Enough - Legal Contracts\n",
    "\n",
    "**Note:** Some document types require approaches beyond chunking. Legal contracts are a prime example.\n",
    "\n",
    "**Why Legal Documents Are Different:**\n",
    "\n",
    "Legal contracts require sophisticated data engineering beyond simple chunking:\n",
    "\n",
    "**Key Challenges:**\n",
    "1. **Clause-level granularity** with hierarchical numbering (Section 3.2.1)\n",
    "2. **Cross-references** between clauses (\"as defined in Section 1.5...\")\n",
    "3. **Hierarchical dependencies** (amendments modify earlier provisions)\n",
    "4. **Legal precedence** (\"Notwithstanding Section 2.1...\" creates overrides)\n",
    "\n",
    "**What This Requires:**\n",
    "\n",
    "Simple chunking is insufficient. You need:\n",
    "- **Knowledge graphs** to capture clause relationships\n",
    "- **Recursive retrieval** to fetch referenced clauses\n",
    "- **Metadata enrichment** (clause type, parties, dates, jurisdiction)\n",
    "\n",
    "**Example Retrieval Flow:**\n",
    "```\n",
    "Query: \"What are the payment terms?\"\n",
    "\n",
    "1. Retrieve: Clause 3.2 (Payment Terms)\n",
    "2. Detect reference: \"as defined in Section 1.5\"\n",
    "3. Fetch: Clause 1.5 (Definitions: \"Net 30\")\n",
    "4. Detect modification: Clause 8.1 modifies 3.2\n",
    "5. Fetch: Clause 8.1 (Amendment: \"Net 45 for Q4\")\n",
    "6. Assemble: [3.2 + 1.5 + 8.1] with relationship metadata\n",
    "```\n",
    "\n",
    "**Recommendation:** This is a **research-level problem** requiring domain expertise. For production systems:\n",
    "- Start with clause-level chunking as baseline\n",
    "- Build knowledge graphs for relationships (Neo4j, etc.)\n",
    "- Implement recursive retrieval for dependencies\n",
    "- Consider specialized legal NLP tools (LexNLP, Blackstone)\n",
    "\n",
    "**Resources:** [Multi-Graph Multi-Agent Systems](https://medium.com/enterprise-rag/legal-document-rag-multi-graph-multi-agent-recursive-retrieval-through-legal-clauses-c90e073e0052), [GraphRAG for Contracts](https://neo4j.com/blog/developer/agentic-graphrag-for-commercial-contracts/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920d4094cea6315",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Troubleshooting Chunking\n",
    "\n",
    "**Common Failure Patterns and Solutions:**\n",
    "\n",
    "| Problem | Likely Cause | Solution |\n",
    "|---------|--------------|----------|\n",
    "| Tables split across chunks | Fixed-size chunking | Use structure-aware chunking |\n",
    "| Formulas without context | Naive chunking | Keep formulas with explanations |\n",
    "| Missing cross-references | Single-chunk retrieval | Implement recursive retrieval |\n",
    "| Generic answers | Chunks too large | Reduce chunk size or use semantic chunking |\n",
    "| Incomplete answers | Chunks too small | Increase chunk size or add overlap |\n",
    "\n",
    "**Iterative Process:** Start simple â†’ Measure baseline â†’ Identify failures â†’ Test improvements â†’ Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a617b97c06dcf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "> **Chunking isn't about fitting in context windows - it's about data modeling for retrieval.**\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "| Question | Answer | Strategy |\n",
    "|----------|--------|----------|\n",
    "| **What is my natural retrieval unit?** | Single record (course, product, FAQ) | Don't chunk - use hierarchical patterns |\n",
    "| | Long-form document (paper, book) | Chunk by sections or semantically |\n",
    "| | Legal contract with cross-references | Advanced: knowledge graphs + recursive retrieval |\n",
    "| **How many topics per document?** | Single topic | Whole-document embedding |\n",
    "| | Multiple distinct topics | Chunking improves precision |\n",
    "| **What content types?** | Text-only | Standard chunking strategies |\n",
    "| | Multimodal (tables, figures) | Keep content WITH context |\n",
    "\n",
    "### Core Strategies\n",
    "\n",
    "1. **Document-Based:** Split by sections/headers - best for structured documents\n",
    "2. **Fixed-Size:** Split into fixed chunks with overlap - best for unstructured text\n",
    "3. **Semantic:** Split based on topic shifts - best for dense academic text\n",
    "\n",
    "**Remember:** Experiment, measure, iterate. This is engineering, not magic.\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "### Module 4: Memory Systems for Context Engineering\n",
    "\n",
    "Now that you understand data modeling and chunking for knowledge bases, you'll learn to manage conversation context:\n",
    "- **Working Memory:** Track conversation history within a session\n",
    "- **Long-term Memory:** Remember user preferences across sessions\n",
    "- **Memory-Enhanced RAG:** Combine retrieved knowledge with conversation memory\n",
    "- **Redis Agent Memory Server:** Automatic memory extraction and retrieval\n",
    "\n",
    "```\n",
    "Module 1: Context Engineering Fundamentals\n",
    "    â†“\n",
    "Module 2: RAG Fundamentals â† Completed\n",
    "    â†“\n",
    "Module 3: Chunking and Data Modeling â† You are here\n",
    "    â†“\n",
    "Module 4: Memory Systems â† Next\n",
    "    â†“\n",
    "Module 5: Building Agents (Complete System)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Analyze Your Data\n",
    "Think about a dataset you work with. Answer these questions:\n",
    "1. What is the natural retrieval unit?\n",
    "2. Does it need chunking? Why or why not?\n",
    "3. If yes, which chunking strategy would you use?\n",
    "\n",
    "### Exercise 2: Design a Chunking Strategy\n",
    "For each document type, choose the best approach:\n",
    "1. Product catalog with 1,000 items\n",
    "2. 50-page technical manual with chapters\n",
    "3. Customer support tickets (avg 200 words each)\n",
    "4. Legal contracts (avg 20 pages, multiple clauses)\n",
    "\n",
    "### Exercise 3: Experiment with Chunking\n",
    "Take the research paper example and:\n",
    "1. Try all three chunking strategies\n",
    "2. Compare the number of chunks and average size\n",
    "3. Which strategy would work best for queries about \"semantic caching methodology\"?\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "**Chunking Strategies:**\n",
    "- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "- [LlamaIndex Node Parsers](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/)\n",
    "\n",
    "**Research Papers:**\n",
    "- [\"Lost in the Middle\" (arXiv:2307.03172)](https://arxiv.org/abs/2307.03172) - U-shaped attention patterns in LLMs\n",
    "- [\"Context Rot\" (Chroma Research, 2025)](https://research.trychroma.com/context-rot) - Performance degradation with input length\n",
    "- [Needle in the Haystack Benchmark](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) - Retrieval in long contexts\n",
    "- [\"Contextual Retrieval\" (Anthropic, 2024)](https://www.anthropic.com/news/contextual-retrieval) - 49-67% reduction in retrieval failures\n",
    "- [\"Advancing Semantic Caching for LLMs\" (arXiv:2504.02268)](https://arxiv.org/abs/2504.02268) - Redis/Virginia Tech research\n",
    "- [\"VoxRAG\" (arXiv:2505.17326, 2025)](https://arxiv.org/abs/2505.17326) - Transcription-free RAG with silence-aware chunking\n",
    "\n",
    "**Advanced Topics:**\n",
    "- [Multi-Graph Multi-Agent Systems for Legal Documents (Medium, 2024)](https://medium.com/enterprise-rag/legal-document-rag-multi-graph-multi-agent-recursive-retrieval-through-legal-clauses-c90e073e0052)\n",
    "- [GraphRAG for Commercial Contracts (Neo4j, 2024)](https://neo4j.com/blog/developer/agentic-graphrag-for-commercial-contracts/)\n",
    "\n",
    "**Data Modeling for RAG:**\n",
    "- [OpenAI Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "\n",
    "**Vector Databases:**\n",
    "- [Redis Vector Search Documentation](https://redis.io/docs/stack/search/reference/vectors/)\n",
    "- [RedisVL Python Library](https://github.com/RedisVentures/redisvl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f00db46d91bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
