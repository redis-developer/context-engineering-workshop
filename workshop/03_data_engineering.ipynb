{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fab7a26bf5c3a5",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Module 3: Data Engineering for Context\n",
    "\n",
    "## From RAG Basics to Practical Context Engineering\n",
    "\n",
    "In Module 2, you built a working RAG system and saw why context quality matters. Now you'll learn to engineer context with professional-level rigor.\n",
    "\n",
    "**What makes context \"good\"?**\n",
    "\n",
    "This module teaches you that **context engineering is real engineering** - it requires the same rigor, analysis, and deliberate decision-making as any other engineering discipline. Context isn't just \"data you feed to an LLM\" - it requires thoughtful preparation, quality assessment, and optimization.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "**The Engineering Mindset:**\n",
    "- Why context quality matters (concrete impact on accuracy, relevance, cost)\n",
    "- The transformation workflow: Raw Data â†’ Engineered Context â†’ Quality Responses\n",
    "- Contrasts between naive and engineered approaches\n",
    "\n",
    "**Data Engineering for Context:**\n",
    "- Systematic transformation: Extract â†’ Clean â†’ Transform â†’ Optimize â†’ Store\n",
    "- Engineering decisions based on YOUR domain requirements\n",
    "- When to use different approaches (RAG, Structured Views, Hybrid)\n",
    "\n",
    "**Introduction to Chunking:**\n",
    "- When does your data need chunking? (Critical first question)\n",
    "- Different chunking strategies and their trade-offs\n",
    "- How to choose based on YOUR data characteristics\n",
    "\n",
    "**Context Preparation Pipelines:**\n",
    "- Three pipeline architectures (Request-Time, Batch, Event-Driven)\n",
    "- How to choose based on YOUR constraints\n",
    "- Building reusable context preparation workflows\n",
    "\n",
    "**â±ï¸ Estimated Time:** 90-105 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Module 2: RAG Fundamentals and Implementation\n",
    "- Redis 8 running locally\n",
    "- OpenAI API key set\n",
    "- Understanding of RAG basics and vector embeddings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c04ea493c9b4a",
   "metadata": {},
   "source": [
    "## Part 1: Context is Data - and Data Requires Engineering\n",
    "\n",
    "### The Naive Approach (What NOT to Do)\n",
    "\n",
    "Let's start by seeing what happens when you treat context as \"just data\" without engineering discipline.\n",
    "\n",
    "**Scenario:** A student asks \"What machine learning courses are available?\"\n",
    "\n",
    "Let's see what happens with a naive approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f0372e941e8e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d320da647edaf123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:55.417839Z",
     "iopub.status.busy": "2025-12-05T00:13:55.417588Z",
     "iopub.status.idle": "2025-12-05T00:13:55.434920Z",
     "shell.execute_reply": "2025-12-05T00:13:55.434210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Handle both running from workshop/ directory and from project root\n",
    "if Path.cwd().name == \"workshop\":\n",
    "    project_root = Path.cwd().parent\n",
    "else:\n",
    "    project_root = Path.cwd()\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables from project root\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Verify required environment variables\n",
    "required_vars = [\"OPENAI_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\"\"âš ï¸  Missing required environment variables: {', '.join(missing_vars)}\n",
    "\n",
    "Please create a .env file with:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "REDIS_URL=redis://localhost:6379\n",
    "\"\"\")\n",
    "    sys.exit(1)\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(\"âœ… Environment variables loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2efd2fd5842a5e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:55.436523Z",
     "iopub.status.busy": "2025-12-05T00:13:55.436385Z",
     "iopub.status.idle": "2025-12-05T00:13:57.635752Z",
     "shell.execute_reply": "2025-12-05T00:13:57.635264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:57 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import redis\n",
    "import tiktoken\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Import hierarchical components (from Module 2)\n",
    "from redis_context_course.hierarchical_manager import HierarchicalCourseManager\n",
    "from redis_context_course.hierarchical_context import HierarchicalContextAssembler\n",
    "from redis_context_course import CourseManager, redis_config\n",
    "\n",
    "# Initialize\n",
    "course_manager = CourseManager()\n",
    "hierarchical_manager = HierarchicalCourseManager(redis_client=redis.from_url(REDIS_URL, decode_responses=True))\n",
    "context_assembler = HierarchicalContextAssembler()\n",
    "redis_client = redis.from_url(REDIS_URL, decode_responses=True)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Token counter\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "print(\"âœ… Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5125abde3b3ad",
   "metadata": {},
   "source": [
    "### Naive Approach: Dump Everything\n",
    "\n",
    "The simplest approach is to include all course data in every request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b763338dc9b9e287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:57.637042Z",
     "iopub.status.busy": "2025-12-05T00:13:57.636924Z",
     "iopub.status.idle": "2025-12-05T00:13:58.508765Z",
     "shell.execute_reply": "2025-12-05T00:13:58.508147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:13:58 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Naive Approach Results:\n",
      "   Courses included: 10\n",
      "   Token count: 2,053\n",
      "   Estimated cost per request: $0.0051\n",
      "\n",
      "   For 100 courses, this would be ~20,530 tokens!\n",
      "\n",
      "\n",
      "ðŸ“„ Sample of raw JSON context:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"course_catalog:01KB16WAF4TQ2W7EBDBRNPW201\",\n",
      "    \"course_code\": \"CS008\",\n",
      "    \"title\": \"Natural Language Processing\",\n",
      "    \"description\": \"Processing and understanding human language with machine learning. This course covers text processing, language models, sentiment analysis, and modern NLP techniques. Students will work with transformers, BERT, and GPT models. Projects include building chatbots, text classifiers, and language generation systems.\",\n",
      "    \"department\": \"Computer Sci...\n"
     ]
    }
   ],
   "source": [
    "# Naive Approach: Get all courses and dump as JSON\n",
    "all_courses = await course_manager.get_all_courses()\n",
    "\n",
    "# Convert to raw JSON (what many developers do first)\n",
    "raw_context = json.dumps(\n",
    "    [\n",
    "        {\n",
    "            \"id\": c.id,\n",
    "            \"course_code\": c.course_code,\n",
    "            \"title\": c.title,\n",
    "            \"description\": c.description,\n",
    "            \"department\": c.department,\n",
    "            \"credits\": c.credits,\n",
    "            \"difficulty_level\": c.difficulty_level.value,\n",
    "            \"format\": c.format.value,\n",
    "            \"instructor\": c.instructor,\n",
    "            \"prerequisites\": (\n",
    "                [p.course_code for p in c.prerequisites] if c.prerequisites else []\n",
    "            ),\n",
    "            \"created_at\": str(c.created_at) if hasattr(c, \"created_at\") else None,\n",
    "            \"updated_at\": str(c.updated_at) if hasattr(c, \"updated_at\") else None,\n",
    "        }\n",
    "        for c in all_courses[:10]  # Just first 10 for demo\n",
    "    ],\n",
    "    indent=2,\n",
    ")\n",
    "\n",
    "token_count = count_tokens(raw_context)\n",
    "\n",
    "print(f\"\"\"ðŸ“Š Naive Approach Results:\n",
    "   Courses included: {len(all_courses[:10])}\n",
    "   Token count: {token_count:,}\n",
    "   Estimated cost per request: ${(token_count / 1_000_000) * 2.50:.4f}\n",
    "\n",
    "   For 100 courses, this would be ~{token_count * 10:,} tokens!\n",
    "\"\"\")\n",
    "\n",
    "# Show a sample\n",
    "print(\"\\nðŸ“„ Sample of raw JSON context:\")\n",
    "print(raw_context[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb0c2bc03a500fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:58.510511Z",
     "iopub.status.busy": "2025-12-05T00:13:58.510358Z",
     "iopub.status.idle": "2025-12-05T00:13:58.515316Z",
     "shell.execute_reply": "2025-12-05T00:13:58.514712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing',\n",
       " 'Natural Language Processing',\n",
       " 'Natural Language Processing',\n",
       " 'Natural Language Processing',\n",
       " 'Natural Language Processing',\n",
       " 'Natural Language Processing',\n",
       " 'Natural Language Processing',\n",
       " 'Natural Language Processing',\n",
       " 'Natural Language Processing',\n",
       " 'Natural Language Processing',\n",
       " 'Linear Algebra for Machine Learning',\n",
       " 'Linear Algebra for Machine Learning',\n",
       " 'Linear Algebra for Machine Learning',\n",
       " 'Linear Algebra for Machine Learning',\n",
       " 'Linear Algebra for Machine Learning',\n",
       " 'Linear Algebra for Machine Learning',\n",
       " 'Linear Algebra for Machine Learning',\n",
       " 'Linear Algebra for Machine Learning',\n",
       " 'Linear Algebra for Machine Learning',\n",
       " 'Linear Algebra for Machine Learning',\n",
       " 'Linear Algebra for Machine Learning',\n",
       " 'Computer Vision',\n",
       " 'Computer Vision',\n",
       " 'Computer Vision',\n",
       " 'Computer Vision',\n",
       " 'Computer Vision',\n",
       " 'Computer Vision',\n",
       " 'Computer Vision',\n",
       " 'Computer Vision',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Deep Learning and Neural Networks',\n",
       " 'Machine Learning Fundamentals',\n",
       " 'Machine Learning Fundamentals',\n",
       " 'Machine Learning Fundamentals',\n",
       " 'Machine Learning Fundamentals',\n",
       " 'Machine Learning Fundamentals',\n",
       " 'Machine Learning Fundamentals',\n",
       " 'Machine Learning Fundamentals',\n",
       " 'Machine Learning Fundamentals']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[course.title for course in all_courses]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9ef0558e87e03",
   "metadata": {},
   "source": [
    "### Test the Naive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d10899a005e07b82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:13:58.517078Z",
     "iopub.status.busy": "2025-12-05T00:13:58.516922Z",
     "iopub.status.idle": "2025-12-05T00:14:02.536941Z",
     "shell.execute_reply": "2025-12-05T00:14:02.536097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:02 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Query: \"What machine learning courses are available?\"\n",
      "\n",
      "Response:\n",
      "The available course related to machine learning is \"Natural Language Processing.\" This course focuses on processing and understanding human language with machine learning techniques. It covers text processing, language models, sentiment analysis, and modern NLP techniques, including working with transformers, BERT, and GPT models. Students will engage in projects such as building chatbots, text classifiers, and language generation systems.\n",
      "\n",
      "Here are the different formats and instructors for this course:\n",
      "\n",
      "1. **Hybrid Format**\n",
      "   - Instructor: Cindy Sandoval (Course Code: CS008)\n",
      "   - Instructor: Joshua Smith (Course Code: CS037)\n",
      "   - Instructor: Frank Walker (Course Code: CS010)\n",
      "   - Instructor: Julie Bell (Course Code: CS042)\n",
      "\n",
      "2. **Online Format**\n",
      "   - Instructor: Nichole Moss (Course Code: CS018)\n",
      "   - Instructor: Phillip Perez (Course Code: CS048)\n",
      "   - Instructor: Shannon Reed (Course Code: CS050)\n",
      "\n",
      "3. **In-Person Format**\n",
      "   - Instructor: Carla Evans (Course Code: CS020)\n",
      "   - Instructor: Meghan Rhodes (Course Code: CS027)\n",
      "   - Instructor: Darren Horton (Course Code: CS034)\n",
      "\n",
      "Each course has specific prerequisites, so please ensure you meet them before enrolling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with a real query\n",
    "query = \"What machine learning courses are available?\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "Available Courses:\n",
    "{raw_context}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(f\"\"\"ðŸ¤– Query: \"{query}\"\n",
    "\n",
    "Response:\n",
    "{response.content}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0dbcd5795aedec",
   "metadata": {},
   "source": [
    "### Problems with the Naive Approach\n",
    "\n",
    "As discussed in Module 2, this approach has several problems:\n",
    "\n",
    "1. **Excessive Token Usage**\n",
    "   - 10 courses = ~1,703 tokens\n",
    "   - 100 courses would be ~17,030 tokens\n",
    "\n",
    "\n",
    "2. **Raw JSON is Inefficient**\n",
    "   - Includes internal fields (IDs, timestamps, created_at, updated_at)\n",
    "   - Verbose formatting (indentation, field names repeated)\n",
    "\n",
    "\n",
    "3. **No Filtering**\n",
    "   - Student asked about ML, but got all courses, even irrelevant ones\n",
    "   - **Dilutes relevant information with noise**\n",
    "\n",
    "\n",
    "4. **Poor Response Quality**\n",
    "   - Generic responses (\"We have many courses...\")\n",
    "   - May miss the most relevant courses\n",
    "   - Can't provide personalized recommendations\n",
    "\n",
    "\n",
    "5. **Not Scalable**\n",
    "   - What if you have 1,000 courses? 10,000?\n",
    "   - What if courses change daily?\n",
    "   - Requires code changes to update\n",
    "\n",
    "**Therefore, the goal is not only to give the LLM \"all the data\" - it's to *give it the useful data.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0dc692ca494be9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Engineering Mindset\n",
    "\n",
    "Context is data that flows through a pipeline. Like any data engineering problem, it requires:\n",
    "\n",
    "### 1. Requirements Analysis\n",
    "- What is the intended use case?\n",
    "- What queries will users ask?\n",
    "- What information do they need?\n",
    "- What constraints exist (token budget, latency, cost)?\n",
    "\n",
    "### 2. Data Transformation\n",
    "- Raw data â†’ Cleaned data â†’ Structured data â†’ LLM-optimized context\n",
    "\n",
    "### 3. Quality Metrics\n",
    "- How do we measure if context is \"good\"?\n",
    "- Relevance, completeness, efficiency, accuracy\n",
    "\n",
    "### 4. Testing and Iteration\n",
    "- Test with real queries\n",
    "- Measure quality metrics\n",
    "- Iterate based on results\n",
    "\n",
    "**The Engineering Question:** \"How do we transform raw course data into high-quality context that produces accurate, relevant, efficient responses?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7e06337016528",
   "metadata": {},
   "source": [
    "### Three Engineering Approaches\n",
    "\n",
    "Let's compare three approaches with concrete examples:\n",
    "\n",
    "| Approach | Description | Token Usage | Response Quality | Maintenance | Verdict |\n",
    "|----------|-------------|-------------|------------------|-------------|---------|\n",
    "| **Naive** | Include all raw data | 50K tokens | Poor (generic) | Easy | âŒ Not practical |\n",
    "| **RAG** | Semantic search for relevant courses | 3K tokens | Good (relevant) | Moderate | âœ… Good for most cases |\n",
    "| **Structured Views** | Pre-compute LLM-optimized summaries | 2K tokens | Excellent (overview + details) | Higher | âœ… Best for real-world use |\n",
    "| **Hybrid** | Structured view + RAG | 5K tokens | Excellent (best of both) | Higher | âœ… Best for real-world use |\n",
    "\n",
    "Let's implement each approach and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d079e72743b37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Data Engineering Workflow - From Raw to Optimized\n",
    "\n",
    "### The Data Engineering Pipeline\n",
    "\n",
    "Context preparation follows a systematic workflow:\n",
    "\n",
    "```\n",
    "Raw Data (Database/API)\n",
    "    â†“\n",
    "1. EXTRACT: Pull relevant data\n",
    "    â†“\n",
    "2. CLEAN: Remove noise, fix inconsistencies\n",
    "    â†“\n",
    "3. TRANSFORM: Structure for LLM consumption\n",
    "    â†“\n",
    "4. OPTIMIZE: Reduce tokens, improve relevance\n",
    "    â†“\n",
    "5. STORE: Index for efficient retrieval\n",
    "    â†“\n",
    "Optimized Context (Ready for LLM)\n",
    "```\n",
    "\n",
    "Let's implement each step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14864605b206341",
   "metadata": {},
   "source": [
    "### Step 1: Extract (Raw Data)\n",
    "\n",
    "First, let's look at what raw course data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f523c504991979f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:02.539082Z",
     "iopub.status.busy": "2025-12-05T00:14:02.538915Z",
     "iopub.status.idle": "2025-12-05T00:14:02.543583Z",
     "shell.execute_reply": "2025-12-05T00:14:02.542909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 1: Raw Database Record\n",
      "================================================================================\n",
      "{\n",
      "  \"id\": \"course_catalog:01KB16WAF4TQ2W7EBDBRNPW201\",\n",
      "  \"course_code\": \"CS008\",\n",
      "  \"title\": \"Natural Language Processing\",\n",
      "  \"description\": \"Processing and understanding human language with machine learning. This course covers text processing, language models, sentiment analysis, and modern NLP techniques. Students will work with transformers, BERT, and GPT models. Projects include building chatbots, text classifiers, and language generation systems.\",\n",
      "  \"department\": \"Computer Science\",\n",
      "  \"credits\": 3,\n",
      "  \"difficulty_level\": \"advanced\",\n",
      "  \"format\": \"hybrid\",\n",
      "  \"instructor\": \"Cindy Sandoval\",\n",
      "  \"prerequisites\": [\n",
      "    \"CS001\"\n",
      "  ],\n",
      "  \"created_at\": \"2025-12-04 19:13:58.502499\",\n",
      "  \"updated_at\": \"2025-12-04 19:13:58.502505\"\n",
      "}\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 204\n"
     ]
    }
   ],
   "source": [
    "# Get a sample course\n",
    "sample_course = all_courses[0]\n",
    "\n",
    "# Show raw database record\n",
    "raw_record = {\n",
    "    \"id\": sample_course.id,\n",
    "    \"course_code\": sample_course.course_code,\n",
    "    \"title\": sample_course.title,\n",
    "    \"description\": sample_course.description,\n",
    "    \"department\": sample_course.department,\n",
    "    \"credits\": sample_course.credits,\n",
    "    \"difficulty_level\": sample_course.difficulty_level.value,\n",
    "    \"format\": sample_course.format.value,\n",
    "    \"instructor\": sample_course.instructor,\n",
    "    \"prerequisites\": (\n",
    "        [p.course_code for p in sample_course.prerequisites]\n",
    "        if sample_course.prerequisites\n",
    "        else []\n",
    "    ),\n",
    "    \"created_at\": (\n",
    "        str(sample_course.created_at)\n",
    "        if hasattr(sample_course, \"created_at\")\n",
    "        else \"2024-01-15T08:30:00Z\"\n",
    "    ),\n",
    "    \"updated_at\": (\n",
    "        str(sample_course.updated_at)\n",
    "        if hasattr(sample_course, \"updated_at\")\n",
    "        else \"2024-09-01T14:22:00Z\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "raw_json = json.dumps(raw_record, indent=2)\n",
    "raw_tokens = count_tokens(raw_json)\n",
    "\n",
    "print(\"ðŸ“„ Step 1: Raw Database Record\")\n",
    "print(\"=\" * 80)\n",
    "print(raw_json)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š Token count: {raw_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108caf4be1a606b4",
   "metadata": {},
   "source": [
    "Issues with above:\n",
    " - Internal fields (IDs, timestamps) waste tokens\n",
    " - Verbose JSON formatting\n",
    " - Prerequisites are codes, not human-readable\n",
    " - No structure for LLM consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca013174da787352",
   "metadata": {},
   "source": [
    "### Step 2: Clean (Remove Noise)\n",
    "\n",
    "Remove fields that don't help the LLM answer user queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f70130fcdc66f34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:02.545082Z",
     "iopub.status.busy": "2025-12-05T00:14:02.544960Z",
     "iopub.status.idle": "2025-12-05T00:14:02.548577Z",
     "shell.execute_reply": "2025-12-05T00:14:02.547994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 2: Cleaned Record\n",
      "================================================================================\n",
      "{\n",
      "  \"course_code\": \"CS008\",\n",
      "  \"title\": \"Natural Language Processing\",\n",
      "  \"description\": \"Processing and understanding human language with machine learning. This course covers text processing, language models, sentiment analysis, and modern NLP techniques. Students will work with transformers, BERT, and GPT models. Projects include building chatbots, text classifiers, and language generation systems.\",\n",
      "  \"department\": \"Computer Science\",\n",
      "  \"credits\": 3,\n",
      "  \"difficulty_level\": \"advanced\",\n",
      "  \"format\": \"hybrid\",\n",
      "  \"instructor\": \"Cindy Sandoval\",\n",
      "  \"prerequisites\": [\n",
      "    \"CS001\"\n",
      "  ]\n",
      "}\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 134 (saved 70 tokens, 34.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Clean - Remove internal fields\n",
    "cleaned_record = {\n",
    "    \"course_code\": sample_course.course_code,\n",
    "    \"title\": sample_course.title,\n",
    "    \"description\": sample_course.description,\n",
    "    \"department\": sample_course.department,\n",
    "    \"credits\": sample_course.credits,\n",
    "    \"difficulty_level\": sample_course.difficulty_level.value,\n",
    "    \"format\": sample_course.format.value,\n",
    "    \"instructor\": sample_course.instructor,\n",
    "    \"prerequisites\": (\n",
    "        [p.course_code for p in sample_course.prerequisites]\n",
    "        if sample_course.prerequisites\n",
    "        else []\n",
    "    ),\n",
    "}\n",
    "\n",
    "cleaned_json = json.dumps(cleaned_record, indent=2)\n",
    "cleaned_tokens = count_tokens(cleaned_json)\n",
    "\n",
    "print(\"ðŸ“„ Step 2: Cleaned Record\")\n",
    "print(\"=\" * 80)\n",
    "print(cleaned_json)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š Token count: {cleaned_tokens} (saved {raw_tokens - cleaned_tokens} tokens, {((raw_tokens - cleaned_tokens) / raw_tokens * 100):.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afda8cda87c128",
   "metadata": {},
   "source": [
    "\n",
    "Improvements:\n",
    " - Removed id, created_at, updated_at\n",
    " - Still has all information needed to answer queries\n",
    "\n",
    "Still has minor problems:\n",
    " - JSON formatting is verbose (this is a *minor* issue as LLMs can handle it; however)\n",
    " - Prerequisites are still codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d3ece2f9f06b8",
   "metadata": {},
   "source": [
    "### Step 3: Transform (Structure for LLM)\n",
    "\n",
    "Convert to a format optimized for LLM consumption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b01f496f5384cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:02.549880Z",
     "iopub.status.busy": "2025-12-05T00:14:02.549770Z",
     "iopub.status.idle": "2025-12-05T00:14:02.553117Z",
     "shell.execute_reply": "2025-12-05T00:14:02.552605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 3: Transformed to LLM-Friendly Format\n",
      "================================================================================\n",
      "CS008: Natural Language Processing\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: advanced\n",
      "Format: hybrid\n",
      "Instructor: Cindy Sandoval\n",
      "Prerequisites: CS001\n",
      "Description: Processing and understanding human language with machine learning. This course covers text processing, language models, sentiment analysis, and modern NLP techniques. Students will work with transformers, BERT, and GPT models. Projects include building chatbots, text classifiers, and language generation systems.\n",
      "    \n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 93 (saved 41 tokens, 30.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Transform - Convert to LLM-friendly format\n",
    "\n",
    "\n",
    "def transform_course_to_text(course) -> str:\n",
    "    \"\"\"Transform course object to LLM-optimized text format.\"\"\"\n",
    "\n",
    "    # Build prerequisites text\n",
    "    prereq_text = \"\"\n",
    "    if course.prerequisites:\n",
    "        prereq_codes = [p.course_code for p in course.prerequisites]\n",
    "        prereq_text = f\"\\nPrerequisites: {', '.join(prereq_codes)}\"\n",
    "\n",
    "    # Build course text\n",
    "    course_text = f\"\"\"{course.course_code}: {course.title}\n",
    "Department: {course.department}\\nCredits: {course.credits}\\nLevel: {course.difficulty_level.value}\\nFormat: {course.format.value}\n",
    "Instructor: {course.instructor}{prereq_text}\n",
    "Description: {course.description}\n",
    "    \"\"\"\n",
    "\n",
    "    return course_text\n",
    "\n",
    "\n",
    "transformed_text = transform_course_to_text(sample_course)\n",
    "transformed_tokens = count_tokens(transformed_text)\n",
    "\n",
    "print(\"ðŸ“„ Step 3: Transformed to LLM-Friendly Format\")\n",
    "print(\"=\" * 80)\n",
    "print(transformed_text)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š Token count: {transformed_tokens} (saved {cleaned_tokens - transformed_tokens} tokens, {((cleaned_tokens - transformed_tokens) / cleaned_tokens * 100):.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39c7fe9faa9a03",
   "metadata": {},
   "source": [
    "\n",
    "âœ… Improvements:\n",
    " - Natural text format with the correct metadata\n",
    " - Clear structure with labels\n",
    " - No JSON overhead (brackets, quotes, commas)\n",
    "\n",
    "**Note:** In case the description is too long, we can apply compression techniques such as summarization to keep the description within a desired token limit. Module 4 will cover compression in more detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d0828e8512bd8",
   "metadata": {},
   "source": [
    "### Step 4: Optimize (Further Reduce Tokens)\n",
    "\n",
    "For even more efficiency, we can create a summarized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "399cd8dfad49c0af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:02.554549Z",
     "iopub.status.busy": "2025-12-05T00:14:02.554425Z",
     "iopub.status.idle": "2025-12-05T00:14:02.557740Z",
     "shell.execute_reply": "2025-12-05T00:14:02.557287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Step 4: Optimized (Ultra-Compact)\n",
      "================================================================================\n",
      "CS008: Natural Language Processing - Processing and understanding human language with machine learning. This course covers text processin... (Prereq: CS001)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Token count: 30 (saved 63 tokens, 67.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Optimize - Create ultra-compact version\n",
    "\n",
    "def optimize_course_text(course) -> str:\n",
    "    \"\"\"Create ultra-compact course description.\"\"\"\n",
    "    prereqs = (\n",
    "        f\" (Prereq: {', '.join([p.course_code for p in course.prerequisites])})\"\n",
    "        if course.prerequisites\n",
    "        else \"\"\n",
    "    )\n",
    "    return (\n",
    "        f\"{course.course_code}: {course.title} - {course.description[:100]}...{prereqs}\"\n",
    "    )\n",
    "\n",
    "\n",
    "optimized_text = optimize_course_text(sample_course)\n",
    "optimized_tokens = count_tokens(optimized_text)\n",
    "\n",
    "print(\"ðŸ“„ Step 4: Optimized (Ultra-Compact)\")\n",
    "print(\"=\" * 80)\n",
    "print(optimized_text)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š Token count: {optimized_tokens} (saved {transformed_tokens - optimized_tokens} tokens, {((transformed_tokens - optimized_tokens) / transformed_tokens * 100):.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c058c97c25c9b3",
   "metadata": {},
   "source": [
    "Improvements:\n",
    "   - Truncated description to 100 chars\n",
    "   - Removed metadata (instructor, format, credits)\n",
    "\n",
    "Trade-off:\n",
    "   - Lost some detail (may need for specific queries)\n",
    "   - Best for overview/catalog views\n",
    "\n",
    "**Note:** This is just an example of what you can do to be more efficient. This is where you have to be creative and engineer based on the use case and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fde7b8f9ea7523",
   "metadata": {},
   "source": [
    "### Step 5: Store (Choose Storage Strategy)\n",
    "\n",
    "Now we need to decide HOW to store this engineered context:\n",
    "\n",
    "**Option 1: Vector Database (RAG)**\n",
    "- Store transformed text with embeddings\n",
    "- Retrieve relevant courses at query time\n",
    "- Good for: Large datasets, specific queries\n",
    "\n",
    "**Option 2: Pre-Computed Views**\n",
    "- Create structured summaries ahead of time\n",
    "- Store in Redis as cached views\n",
    "- Good for: Common queries, overview information\n",
    "\n",
    "**Option 3: Hybrid**\n",
    "- Combine both approaches\n",
    "- Pre-compute catalog view + RAG for details\n",
    "- Good for: Real-world systems\n",
    "\n",
    "Let's implement all three and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a1e4732561411",
   "metadata": {},
   "source": [
    "### Summary: The Transformation Pipeline\n",
    "\n",
    "Let's see the complete transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed00ffe2784d5831",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:02.559173Z",
     "iopub.status.busy": "2025-12-05T00:14:02.559057Z",
     "iopub.status.idle": "2025-12-05T00:14:02.561817Z",
     "shell.execute_reply": "2025-12-05T00:14:02.561419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMPLE PIPELINE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Step 1: Raw Database Record\n",
      "   Token count: 204\n",
      "   Format: JSON with all fields\n",
      "\n",
      "Step 2: Cleaned Record\n",
      "   Token count: 134 (34.3% reduction)\n",
      "   Removed: Internal fields (IDs, timestamps)\n",
      "\n",
      "Step 3: Transformed to LLM Format\n",
      "   Token count: 93 (30.6% reduction from Step 2)\n",
      "   Format: Natural text, structured\n",
      "\n",
      "Step 4: Optimized (Ultra-Compact)\n",
      "   Token count: 30 (67.7% reduction from Step 3)\n",
      "   Format: Single line, truncated\n",
      "\n",
      "TOTAL REDUCTION: 204 â†’ 30 tokens (85.3% reduction)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸŽ¯ Key Insight:\n",
      "   Through systematic engineering, we reduced token usage by ~70%\n",
      "   while IMPROVING readability for the LLM!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "Step 1: Raw Database Record\n",
    "   Token count: {raw_tokens}\n",
    "   Format: JSON with all fields\n",
    "\n",
    "Step 2: Cleaned Record\n",
    "   Token count: {cleaned_tokens} ({((raw_tokens - cleaned_tokens) / raw_tokens * 100):.1f}% reduction)\n",
    "   Removed: Internal fields (IDs, timestamps)\n",
    "\n",
    "Step 3: Transformed to LLM Format\n",
    "   Token count: {transformed_tokens} ({((cleaned_tokens - transformed_tokens) / cleaned_tokens * 100):.1f}% reduction from Step 2)\n",
    "   Format: Natural text, structured\n",
    "\n",
    "Step 4: Optimized (Ultra-Compact)\n",
    "   Token count: {optimized_tokens} ({((transformed_tokens - optimized_tokens) / transformed_tokens * 100):.1f}% reduction from Step 3)\n",
    "   Format: Single line, truncated\n",
    "\n",
    "TOTAL REDUCTION: {raw_tokens} â†’ {optimized_tokens} tokens ({((raw_tokens - optimized_tokens) / raw_tokens * 100):.1f}% reduction)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nðŸŽ¯ Key Insight:\")\n",
    "print(\"   Through systematic engineering, we reduced token usage by ~70%\")\n",
    "print(\"   while IMPROVING readability for the LLM!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad594586803b5f6",
   "metadata": {},
   "source": [
    "The key insight states that we reduced token usage.\n",
    "\n",
    "However, it should be noted that reduction is not the goal. The goal is to optimize the content and provide the most relevant information to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c0f8a83da6588",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Engineering Decision - When to Use Each Approach\n",
    "\n",
    "Now let's implement the three approaches and compare them with real queries.\n",
    "\n",
    "### Approach 1: RAG (Semantic Search)\n",
    "\n",
    "Retrieve only relevant courses using vector search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2d24082f05979c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:02.563738Z",
     "iopub.status.busy": "2025-12-05T00:14:02.563639Z",
     "iopub.status.idle": "2025-12-05T00:14:02.817128Z",
     "shell.execute_reply": "2025-12-05T00:14:02.816105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:02 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'What machine learning courses are available?' (summaries=5, details=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:02 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:02 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:02 redis_context_course.hierarchical_manager INFO   Created summary index: course_summaries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:02 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: What machine learning courses are available?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:02 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:02 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š RAG Approach Results:\n",
      "   Query: \"What machine learning courses are available?\"\n",
      "   Token count: 25\n",
      "\n",
      "ðŸ“„ Context Preview:\n",
      "# Course Search Results for: What machine learning courses are available?\n",
      "\n",
      "## Overview of All Matches\n",
      "\n",
      "Found 0 relevant courses:\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach 1: RAG with Hierarchical Search\n",
    "# Use the hierarchical manager from Module 2\n",
    "\n",
    "async def rag_approach(query: str, summary_limit: int = 5, detail_limit: int = 3) -> str:\n",
    "    \"\"\"Retrieve relevant courses using hierarchical semantic search.\"\"\"\n",
    "\n",
    "    # Use hierarchical search (summaries + details)\n",
    "    summaries, details = await hierarchical_manager.hierarchical_search(\n",
    "        query=query,\n",
    "        summary_limit=summary_limit,\n",
    "        detail_limit=detail_limit\n",
    "    )\n",
    "\n",
    "    # Use context assembler for LLM-friendly format\n",
    "    context = context_assembler.assemble_hierarchical_context(\n",
    "        summaries=summaries,\n",
    "        details=details,\n",
    "        query=query\n",
    "    )\n",
    "    return context\n",
    "\n",
    "\n",
    "# Test RAG approach\n",
    "query = \"What machine learning courses are available?\"\n",
    "rag_context = await rag_approach(query)\n",
    "rag_tokens = count_tokens(rag_context)\n",
    "\n",
    "print(f\"\"\"ðŸ“Š RAG Approach Results:\n",
    "   Query: \"{query}\"\n",
    "   Token count: {rag_tokens:,}\n",
    "\n",
    "ðŸ“„ Context Preview:\n",
    "{rag_context[:500]}...\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af068f155fb8bdf",
   "metadata": {},
   "source": [
    "### Approach 2: Structured Views (Pre-Computed Summaries)\n",
    "\n",
    "Create a pre-computed catalog view that's optimized for LLM consumption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "762c72487dffacdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:02.818998Z",
     "iopub.status.busy": "2025-12-05T00:14:02.818858Z",
     "iopub.status.idle": "2025-12-05T00:14:02.827061Z",
     "shell.execute_reply": "2025-12-05T00:14:02.826580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Structured View Approach Results:\n",
      "   Total courses: 50\n",
      "   Token count: 259\n",
      "   Cached in Redis: âœ…\n",
      "\n",
      "ðŸ“„ Catalog Preview:\n",
      "# Redis University Course Catalog\n",
      "\n",
      "## Computer Science (39 courses)\n",
      "- CS008: Natural Language Processing (advanced)\n",
      "- CS018: Natural Language Processing (advanced)\n",
      "- CS020: Natural Language Processing (advanced)\n",
      "- CS027: Natural Language Processing (advanced)\n",
      "- CS034: Natural Language Processing (advanced)\n",
      "- CS037: Natural Language Processing (advanced)\n",
      "- CS048: Natural Language Processing (advanced)\n",
      "- CS050: Natural Language Processing (advanced)\n",
      "- CS010: Natural Language Processing (advanced)\n",
      "- CS042: Natural Language Processing (advanced)\n",
      "\n",
      "## Mathematics (11 courses)\n",
      "- MATH001: Linear Algeb...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach 2: Structured Views\n",
    "# Pre-compute a catalog summary organized by department\n",
    "\n",
    "\n",
    "async def create_catalog_view() -> str:\n",
    "    \"\"\"Create a pre-computed catalog view organized by department.\"\"\"\n",
    "\n",
    "    # Group courses by department\n",
    "    by_department = {}\n",
    "    for course in all_courses:\n",
    "        dept = course.department\n",
    "        if dept not in by_department:\n",
    "            by_department[dept] = []\n",
    "        by_department[dept].append(course)\n",
    "\n",
    "    # Build catalog view\n",
    "    catalog_sections = []\n",
    "\n",
    "    for dept_name in sorted(by_department.keys()):\n",
    "        courses = by_department[dept_name]\n",
    "\n",
    "        # Create department section\n",
    "        dept_section = f\"\\n## {dept_name} ({len(courses)} courses)\\n\"\n",
    "\n",
    "        # Add course summaries (optimized format)\n",
    "        course_summaries = []\n",
    "        for course in courses[:10]:  # Limit for demo\n",
    "            summary = f\"- {course.course_code}: {course.title} ({course.difficulty_level.value})\"\n",
    "            course_summaries.append(summary)\n",
    "\n",
    "        dept_section += \"\\n\".join(course_summaries)\n",
    "        catalog_sections.append(dept_section)\n",
    "\n",
    "    catalog_view = \"# Redis University Course Catalog\\n\" + \"\\n\".join(catalog_sections)\n",
    "    return catalog_view\n",
    "\n",
    "\n",
    "# Create and cache the view\n",
    "catalog_view = await create_catalog_view()\n",
    "catalog_tokens = count_tokens(catalog_view)\n",
    "\n",
    "# Store in Redis for reuse\n",
    "redis_client.set(\"course_catalog_view\", catalog_view)\n",
    "\n",
    "print(f\"\"\"ðŸ“Š Structured View Approach Results:\n",
    "   Total courses: {len(all_courses)}\n",
    "   Token count: {catalog_tokens:,}\n",
    "   Cached in Redis: âœ…\n",
    "\n",
    "ðŸ“„ Catalog Preview:\n",
    "{catalog_view[:600]}...\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26216d8ef8834c41",
   "metadata": {},
   "source": [
    "### Approach 3: Hybrid (Best of Both Worlds)\n",
    "\n",
    "Combine structured view (overview) + RAG (specific details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69ba876f54b6b950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:02.828723Z",
     "iopub.status.busy": "2025-12-05T00:14:02.828591Z",
     "iopub.status.idle": "2025-12-05T00:14:03.111645Z",
     "shell.execute_reply": "2025-12-05T00:14:03.110522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:02 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'What machine learning courses are available?' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:03 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:03 redis_context_course.hierarchical_manager INFO   Found 0 course summaries for query: What machine learning courses are available?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:03 redis_context_course.hierarchical_manager INFO   Fetched 0 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:03 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 0 summaries, 0 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Hybrid Approach Results:\n",
      "   Query: \"What machine learning courses are available?\"\n",
      "   Token count: 297\n",
      "\n",
      "   Components:\n",
      "   - Catalog overview: 259 tokens\n",
      "   - Specific details (RAG): 25 tokens\n",
      "\n",
      "ðŸ“„ Context Structure:\n",
      "   1. Full catalog overview (all departments)\n",
      "   2. Detailed info for most relevant courses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach 3: Hybrid\n",
    "\n",
    "\n",
    "async def hybrid_approach(query: str) -> str:\n",
    "    \"\"\"Combine catalog overview with RAG for specific details.\"\"\"\n",
    "\n",
    "    # Part 1: Get catalog overview (from cache)\n",
    "    catalog_overview = redis_client.get(\"course_catalog_view\")\n",
    "\n",
    "    # Part 2: Get specific course details via hierarchical RAG\n",
    "    specific_courses = await rag_approach(query, summary_limit=3, detail_limit=2)\n",
    "\n",
    "    # Combine\n",
    "    hybrid_context = f\"\"\"# Course Catalog Overview\n",
    "{catalog_overview}\n",
    "\n",
    "---\n",
    "\n",
    "# Detailed Information for Your Query\n",
    "{specific_courses}\n",
    "\"\"\"\n",
    "\n",
    "    return hybrid_context\n",
    "\n",
    "\n",
    "# Test hybrid approach\n",
    "hybrid_context = await hybrid_approach(query)\n",
    "hybrid_tokens = count_tokens(hybrid_context)\n",
    "\n",
    "print(f\"\"\"ðŸ“Š Hybrid Approach Results:\n",
    "   Query: \"{query}\"\n",
    "   Token count: {hybrid_tokens:,}\n",
    "\n",
    "   Components:\n",
    "   - Catalog overview: {catalog_tokens:,} tokens\n",
    "   - Specific details (RAG): {rag_tokens:,} tokens\n",
    "\n",
    "ðŸ“„ Context Structure:\n",
    "   1. Full catalog overview (all departments)\n",
    "   2. Detailed info for most relevant courses\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b461f50f419e7b",
   "metadata": {},
   "source": [
    "### Compare All Three Approaches\n",
    "\n",
    "Let's test all three with the same query and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d6d13e5df18d7d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:03.113762Z",
     "iopub.status.busy": "2025-12-05T00:14:03.113586Z",
     "iopub.status.idle": "2025-12-05T00:14:10.516986Z",
     "shell.execute_reply": "2025-12-05T00:14:10.515649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARING THREE APPROACHES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:05 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:07 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:10 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: \"What machine learning courses are available?\"\n",
      "\n",
      "================================================================================\n",
      "APPROACH 1: RAG (Hierarchical Search)\n",
      "================================================================================\n",
      "Token count: 25\n",
      "Response:\n",
      "Currently, there are no machine learning courses available at Redis University. If you are interested in other topics or need assistance with something else, feel free to ask!\n",
      "\n",
      "================================================================================\n",
      "APPROACH 2: Structured View (Pre-Computed)\n",
      "================================================================================\n",
      "Token count: 259\n",
      "Response:\n",
      "In the Mathematics category, we offer several courses on Linear Algebra for Machine Learning, which are at an intermediate level. Here are the available courses:\n",
      "\n",
      "- MATH001: Linear Algebra for Machine Learning\n",
      "- MATH003: Linear Algebra for Machine Learning\n",
      "- MATH012: Linear Algebra for Machine Learning\n",
      "- MATH017: Linear Algebra for Machine Learning\n",
      "- MATH026: Linear Algebra for Machine Learning\n",
      "- MATH028: Linear Algebra for Machine Learning\n",
      "- MATH036: Linear Algebra for Machine Learning\n",
      "- MATH038: Linear Algebra for Machine Learning\n",
      "- MATH040: Linear Algebra for Machine Learning\n",
      "- MATH045: Linear Algebra for Machine Learning\n",
      "\n",
      "These courses focus on the mathematical foundations necessary for understanding and applying machine learning techniques.\n",
      "\n",
      "================================================================================\n",
      "APPROACH 3: Hybrid (View + RAG)\n",
      "================================================================================\n",
      "Token count: 297\n",
      "Response:\n",
      "Currently, there are no specific courses listed under the title \"Machine Learning\" in the Redis University Course Catalog. However, you might be interested in the \"Linear Algebra for Machine Learning\" courses offered under the Mathematics category, as they provide foundational knowledge that is often essential for understanding machine learning concepts. If you have any other specific interests or need further assistance, feel free to ask!\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test all three approaches\n",
    "query = \"What machine learning courses are available?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARING THREE APPROACHES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Approach 1: RAG\n",
    "messages_rag = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "Available Courses:\n",
    "{rag_context}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "response_rag = llm.invoke(messages_rag)\n",
    "\n",
    "# Approach 2: Structured View\n",
    "messages_view = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "{catalog_view}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "response_view = llm.invoke(messages_view)\n",
    "\n",
    "# Approach 3: Hybrid\n",
    "messages_hybrid = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a Redis University course advisor.\n",
    "\n",
    "{hybrid_context}\n",
    "\n",
    "Help students find relevant courses.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=query),\n",
    "]\n",
    "response_hybrid = llm.invoke(messages_hybrid)\n",
    "\n",
    "# Display comparison\n",
    "print(f\"\"\"\n",
    "Query: \"{query}\"\n",
    "\n",
    "{'=' * 80}\n",
    "APPROACH 1: RAG (Hierarchical Search)\n",
    "{'=' * 80}\n",
    "Token count: {rag_tokens:,}\n",
    "Response:\n",
    "{response_rag.content}\n",
    "\n",
    "{'=' * 80}\n",
    "APPROACH 2: Structured View (Pre-Computed)\n",
    "{'=' * 80}\n",
    "Token count: {catalog_tokens:,}\n",
    "Response:\n",
    "{response_view.content}\n",
    "\n",
    "{'=' * 80}\n",
    "APPROACH 3: Hybrid (View + RAG)\n",
    "{'=' * 80}\n",
    "Token count: {hybrid_tokens:,}\n",
    "Response:\n",
    "{response_hybrid.content}\n",
    "\n",
    "{'=' * 80}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215ecaab5676ae3",
   "metadata": {},
   "source": [
    "### Decision Framework: Which Approach to Use?\n",
    "\n",
    "Here's how to choose based on YOUR requirements:\n",
    "\n",
    "| Factor | RAG | Structured Views | Hybrid |\n",
    "|--------|-----|------------------|--------|\n",
    "| **Token Efficiency** | âœ… Good (3K) | âœ…âœ… Excellent (2K) | âš ï¸ Moderate (5K) |\n",
    "| **Response Quality** | âœ… Good (relevant) | âœ… Good (overview) | âœ…âœ… Excellent (both) |\n",
    "| **Latency** | âš ï¸ Moderate (search) | âœ…âœ… Fast (cached) | âš ï¸ Moderate (search) |\n",
    "| **Maintenance** | âœ… Low (auto-updates) | âš ï¸ Higher (rebuild views) | âš ï¸ Higher (both) |\n",
    "| **Best For** | Specific queries | Overview queries | Real-world systems |\n",
    "\n",
    "**Decision Process:**\n",
    "\n",
    "1. **Analyze YOUR data characteristics:**\n",
    "   - How many items? (10s, 100s, 1000s, millions?)\n",
    "   - How often does it change? (Real-time, daily, weekly?)\n",
    "   - What's the average item size? (100 words, 1000 words, 10K words?)\n",
    "\n",
    "2. **Analyze YOUR query patterns:**\n",
    "   - Specific queries (\"Show me RU101\") â†’ RAG\n",
    "   - Overview queries (\"What courses exist?\") â†’ Structured Views\n",
    "   - Mixed queries â†’ Hybrid\n",
    "\n",
    "3. **Analyze YOUR constraints:**\n",
    "   - Tight token budget â†’ Structured Views\n",
    "   - Real-time updates required â†’ RAG\n",
    "   - Best quality needed â†’ Hybrid\n",
    "\n",
    "**Example Decision:**\n",
    "\n",
    "For Redis University:\n",
    "- âœ… **Data:** 100-500 courses, updated weekly, 200-500 words each\n",
    "- âœ… **Queries:** Mix of overview (\"What's available?\") and specific (\"ML courses?\")\n",
    "- âœ… **Constraints:** Moderate token budget, weekly updates acceptable\n",
    "- âœ… **Decision:** **Hybrid approach** (pre-compute catalog + RAG for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f7e9b076dc401",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Introduction to Chunking - When and Why\n",
    "\n",
    "So far, we've worked with course data where each course is a complete, self-contained unit. But what happens when you have **long documents** that contain multiple distinct topics?\n",
    "\n",
    "This is where **chunking** *may* become necessaryâ€”but it's not always the right choice.\n",
    "\n",
    "### The Critical First Question: Does My Data Need Chunking?\n",
    "\n",
    "**Chunking is NOT a default step** - it's an engineering decision that depends on multiple factors:\n",
    "\n",
    "1. **Document type:** Structured records vs. long-form text vs. PDFs\n",
    "2. **Data characteristics:** Small discrete items vs. large continuous documents\n",
    "3. **Application requirements:** Query patterns, retrieval precision needs\n",
    "4. **Embedding model limitations:** Context window size\n",
    "\n",
    "âš ï¸ **Important:** With modern long-context models (128K+ tokens), \"fitting in context\" is less of a concern. The real question is about **retrieval precision** and **data modeling**.\n",
    "\n",
    "Let's understand when chunking helpsâ€”and when it hurts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5796dfa9ac9e4a6",
   "metadata": {},
   "source": [
    "### The \"Don't Chunk\" Strategy: A Valid Option\n",
    "\n",
    "For structured data and naturally small documents, **not chunking is often the best strategy**:\n",
    "\n",
    "**Examples where whole-record embedding works better:**\n",
    "- âœ… **Course catalogs** - Each course is a complete, self-contained unit\n",
    "- âœ… **Product listings** - All product info should be retrieved together\n",
    "- âœ… **FAQ entries** - Question + answer form an atomic unit\n",
    "- âœ… **Database records** - Structured data with natural boundaries\n",
    "- âœ… **Support tickets** - Single issue with context\n",
    "\n",
    "**Why \"don't chunk\" works here:**\n",
    "- Each unit is **semantically complete** - all relevant info is together\n",
    "- Natural boundaries exist - chunking would split related information\n",
    "- Retrieval precision is maximized - you get exactly what's relevant\n",
    "- Simpler implementation - no chunking logic, no overlap decisions\n",
    "\n",
    "> ðŸ’¡ **Key Insight:** For our course catalog, each course is already an optimal retrieval unit. Chunking it would only hurt quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e43dcd77cd7ac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:10.519490Z",
     "iopub.status.busy": "2025-12-05T00:14:10.519298Z",
     "iopub.status.idle": "2025-12-05T00:14:10.523204Z",
     "shell.execute_reply": "2025-12-05T00:14:10.522310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Example: Course Description\n",
      "================================================================================\n",
      "CS008: Natural Language Processing\n",
      "Department: Computer Science\n",
      "Credits: 3\n",
      "Level: advanced\n",
      "Format: hybrid\n",
      "Instructor: Cindy Sandoval\n",
      "Prerequisites: CS001\n",
      "Description: Processing and understanding human language with machine learning. This course covers text processing, language models, sentiment analysis, and modern NLP techniques. Students will work with transformers, BERT, and GPT models. Projects include building chatbots, text classifiers, and language generation systems.\n",
      "    \n",
      "================================================================================\n",
      "\n",
      "Token count: 93\n",
      "Semantic completeness: âœ… Complete (has all info about this course)\n",
      "Chunking needed? âŒ NO\n",
      "\n",
      "Why whole-record embedding works here:\n",
      "- Self-contained (doesn't reference other sections)\n",
      "- Semantically complete (has all course details)\n",
      "- Natural boundary (one course = one retrieval unit)\n",
      "- Breaking it up would hurt retrieval quality\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Course data (NO chunking needed)\n",
    "sample_course_text = transform_course_to_text(all_courses[0])\n",
    "sample_tokens = count_tokens(sample_course_text)\n",
    "\n",
    "print(f\"\"\"ðŸ“Š Example: Course Description\n",
    "{'=' * 80}\n",
    "{sample_course_text}\n",
    "{'=' * 80}\n",
    "\n",
    "Token count: {sample_tokens}\n",
    "Semantic completeness: âœ… Complete (has all info about this course)\n",
    "Chunking needed? âŒ NO\n",
    "\n",
    "Why whole-record embedding works here:\n",
    "- Self-contained (doesn't reference other sections)\n",
    "- Semantically complete (has all course details)\n",
    "- Natural boundary (one course = one retrieval unit)\n",
    "- Breaking it up would hurt retrieval quality\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6376e7b5b6f0b",
   "metadata": {},
   "source": [
    "### When Chunking May Help\n",
    "\n",
    "Chunking *can* improve retrieval when documents have certain characteristics:\n",
    "\n",
    "**Document types that may benefit from chunking:**\n",
    "- Research papers with multiple distinct sections\n",
    "- Technical documentation spanning many topics\n",
    "- Books and long-form content\n",
    "- Legal contracts with multiple clauses\n",
    "- Medical records with multiple visits/conditions\n",
    "\n",
    "**Why chunking helps in these cases:**\n",
    "- **Multiple distinct topics** - Different sections should be retrieved separately for precision\n",
    "- **Improves retrieval precision** - Find specific sections, not whole document\n",
    "- **Better data modeling** - Just like database design, structure affects quality\n",
    "- **Addresses research-documented context quality problems** (see below)\n",
    "\n",
    "âš ï¸ **Note:** Modern embedding models (jina-embeddings-v2, etc.) support 8K+ tokens. Model limits are less of a concern than **retrieval precision**.\n",
    "\n",
    "### Research Background: Why Long Context Can Hurt\n",
    "\n",
    "Even with large context windows (128K+ tokens), research shows that **how you structure context matters more than fitting everything in**:\n",
    "\n",
    "**1. \"Lost in the Middle\" (Stanford/UC Berkeley, 2023)**\n",
    "\n",
    "*Source: [arXiv:2307.03172](https://arxiv.org/abs/2307.03172), published in TACL 2024*\n",
    "\n",
    "LLMs exhibit a **\"U-shaped\" attention pattern**:\n",
    "- High recall for information at the **beginning** and **end** of context\n",
    "- **Significantly degraded performance** for information in the middle\n",
    "- This happens even in models explicitly designed for long contexts\n",
    "\n",
    "**Implication for chunking:** For long documents, chunking ensures the relevant section is retrieved and placed prominently in context, rather than buried in the middle of a 50-page document.\n",
    "\n",
    "**2. \"Context Rot\" (Chroma Research, 2025)**\n",
    "\n",
    "*Source: [research.trychroma.com/context-rot](https://research.trychroma.com/context-rot)*\n",
    "\n",
    "Tested 18 LLMs (GPT-4.1, Claude 4, Gemini 2.5, Qwen3) and found:\n",
    "- **Performance degrades as input length increases**, even when relevant info is present\n",
    "- **Distractor effect**: Irrelevant content actively hurts model performance\n",
    "- Even 4 distractor documents can significantly degrade output quality\n",
    "- Position of irrelevant content matters (middle is worst)\n",
    "\n",
    "**Implication for chunking:** Retrieving smaller, focused chunks reduces \"distractor tokens\" that poison the context.\n",
    "\n",
    "**3. Needle in the Haystack (NIAH) Benchmark**\n",
    "\n",
    "*Source: [Greg Kamradt, github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)*\n",
    "\n",
    "Tests whether LLMs can find a specific fact (\"needle\") buried in long context (\"haystack\"):\n",
    "- Models often **fail to retrieve information** even when it's present in context\n",
    "- Performance varies by position (middle is typically worst)\n",
    "- **Limitation**: NIAH tests lexical retrieval only, not semantic understanding\n",
    "\n",
    "**Implication for chunking:** For structured data like course catalogs, NIAH is **irrelevant**â€”each record IS the needle. Chunking would only fragment complete units.\n",
    "\n",
    "---\n",
    "\n",
    "**The Key Insight:**\n",
    "\n",
    "These research findings don't prescribe a universal chunking ruleâ€”they inform your design decisions:\n",
    "\n",
    "- **Structured records** (courses, products, FAQs): The \"lost in the middle\" problem typically doesn't apply because each record is already a focused, atomic unit.\n",
    "\n",
    "- **Long-form documents**: Context rot and positional bias become more relevant as document length increases. Chunking can help surface relevant sections, but over-chunking fragments context.\n",
    "\n",
    "- **Mixed content types**: Real-world data rarely fits neat categories. Experiment with your actual data and queries.\n",
    "\n",
    "### Emerging Strategies (2024)\n",
    "\n",
    "Recent research has introduced new approaches that challenge traditional chunking:\n",
    "\n",
    "1. **Late Chunking** (Jina AI): Embed the entire document first, then chunk the embeddings. Preserves cross-chunk context without ColBERT-level storage costs.\n",
    "\n",
    "2. **Contextual Retrieval** (Anthropic): Add LLM-generated context to each chunk before embedding. Can reduce retrieval failures by 49-67%.\n",
    "\n",
    "3. **Hybrid Search**: Combine vector embeddings with BM25 keyword search. Often outperforms either approach alone.\n",
    "\n",
    "The key insight: **chunking strategy is a design choice, not a one-size-fits-all prescription**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0e5a30afefc3dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:10.525155Z",
     "iopub.status.busy": "2025-12-05T00:14:10.525019Z",
     "iopub.status.idle": "2025-12-05T00:14:10.529911Z",
     "shell.execute_reply": "2025-12-05T00:14:10.529251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 890 | Words: ~532\n"
     ]
    }
   ],
   "source": [
    "# Example: Research paper (NEEDS chunking)\n",
    "# Let's simulate a long research paper about Redis\n",
    "\n",
    "research_paper = \"\"\"\n",
    "# Optimizing Vector Search Performance in Redis\n",
    "\n",
    "## Abstract\n",
    "This paper presents a comprehensive analysis of vector search optimization techniques in Redis,\n",
    "examining the trade-offs between search quality, latency, and memory usage. We evaluate multiple\n",
    "indexing strategies including HNSW and FLAT indexes across datasets ranging from 10K to 10M vectors.\n",
    "Our results demonstrate that careful index configuration can improve search latency by up to 10x\n",
    "while maintaining 95%+ recall. We also introduce novel compression techniques that reduce memory\n",
    "usage by 75% with minimal impact on search quality.\n",
    "\n",
    "## 1. Introduction\n",
    "Vector databases have become essential infrastructure for modern AI applications, enabling semantic\n",
    "search, recommendation systems, and retrieval-augmented generation (RAG). Redis, traditionally known\n",
    "as an in-memory data structure store, has evolved to support high-performance vector search through\n",
    "the RediSearch module. However, optimizing vector search performance requires understanding complex\n",
    "trade-offs between multiple dimensions...\n",
    "\n",
    "[... 5,000 more words covering methodology, experiments, results, discussion ...]\n",
    "\n",
    "## 2. Background and Related Work\n",
    "Previous work on vector search optimization has focused primarily on algorithmic improvements to\n",
    "approximate nearest neighbor (ANN) search. Malkov and Yashunin (2018) introduced HNSW, which has\n",
    "become the de facto standard for high-dimensional vector search. Johnson et al. (2019) developed\n",
    "FAISS, demonstrating that product quantization can significantly reduce memory usage...\n",
    "\n",
    "[... 2,000 more words ...]\n",
    "\n",
    "## 3. Performance Analysis and Results\n",
    "\n",
    "### 3.1 HNSW Configuration Trade-offs\n",
    "\n",
    "Table 1 shows the performance comparison across different HNSW configurations. As M increases from 16 to 64,\n",
    "we observe significant improvements in recall (0.89 to 0.97) but at the cost of increased latency (2.1ms to 8.7ms)\n",
    "and memory usage (1.2GB to 3.8GB). The sweet spot for most real-world workloads is M=32 with ef_construction=200,\n",
    "which achieves 0.94 recall with 4.3ms latency.\n",
    "\n",
    "Table 1: HNSW Performance Comparison\n",
    "| M  | ef_construction | Recall@10 | Latency (ms) | Memory (GB) | Build Time (min) |\n",
    "|----|-----------------|-----------|--------------|-------------|------------------|\n",
    "| 16 | 100            | 0.89      | 2.1          | 1.2         | 8                |\n",
    "| 32 | 200            | 0.94      | 4.3          | 2.1         | 15               |\n",
    "| 64 | 400            | 0.97      | 8.7          | 3.8         | 32               |\n",
    "\n",
    "The data clearly demonstrates the fundamental trade-off between search quality and resource consumption.\n",
    "For applications requiring high recall (>0.95), the increased latency and memory costs are unavoidable.\n",
    "\n",
    "### 3.2 Mathematical Model\n",
    "\n",
    "The recall-latency trade-off can be modeled as a quadratic function of the HNSW parameters:\n",
    "\n",
    "Latency(M, ef) = Î±Â·MÂ² + Î²Â·ef + Î³\n",
    "\n",
    "Where:\n",
    "- M = number of connections per layer (controls graph connectivity)\n",
    "- ef = size of dynamic candidate list (controls search breadth)\n",
    "- Î±, Î², Î³ = dataset-specific constants (fitted from experimental data)\n",
    "\n",
    "For our e-commerce dataset, we fitted: Î±=0.002, Î²=0.015, Î³=1.2 (RÂ²=0.94)\n",
    "\n",
    "## 4. Implementation Recommendations\n",
    "\n",
    "Based on our findings, we recommend the following configuration for real-world deployments:\n",
    "\n",
    "```python\n",
    "# Optimal HNSW configuration for balanced performance\n",
    "index_params = {\n",
    "    \"M\": 32,                  # Balance recall and latency\n",
    "    \"ef_construction\": 200,   # Higher quality index\n",
    "    \"ef_runtime\": 100         # Fast search with good recall\n",
    "}\n",
    "```\n",
    "\n",
    "This configuration achieves 0.94 recall with 4.3ms p95 latency, suitable for most real-time applications.\n",
    "\n",
    "## 5. Discussion and Conclusion\n",
    "Our findings demonstrate that vector search optimization is fundamentally about understanding\n",
    "YOUR specific requirements and constraints. There is no one-size-fits-all configuration.\n",
    "\"\"\"\n",
    "\n",
    "paper_tokens = count_tokens(research_paper)\n",
    "print(f\"Token count: {paper_tokens:,} | Words: ~{len(research_paper.split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea9d2e3472e35a",
   "metadata": {},
   "source": [
    "**ðŸ“Š Analysis: Research Paper Example**\n",
    "\n",
    "**Document:** \"Optimizing Vector Search Performance in Redis\"\n",
    "\n",
    "**Structure:** Abstract, Introduction, Background, Methodology, Results, Discussion\n",
    "\n",
    "**Chunking needed?** âœ… **YES**\n",
    "\n",
    "**Why This Document May Benefit from Chunking:**\n",
    "\n",
    "> **Note:** Modern LLMs can handle 128K+ tokens, so \"fitting in context\" isn't the issue. The real value of chunking is **better data modeling and retrieval precision**.\n",
    "\n",
    "**1. Retrieval Precision vs. Recall Trade-off**\n",
    "\n",
    "Without chunking (embed entire paper):\n",
    "- Query: \"What compression techniques were used?\"\n",
    "- Retrieved: Entire 15,000-token paper\n",
    "- Problem: 80% of retrieved content is irrelevant to the query\n",
    "\n",
    "With chunking (embed by section):\n",
    "- Query: \"What compression techniques were used?\"\n",
    "- Retrieved: Methodology section (800 tokens)\n",
    "- Result: 90%+ of retrieved content is directly relevant\n",
    "\n",
    "**2. Query-Specific Retrieval Patterns**\n",
    "\n",
    "| Query | Needs | Without Chunking | With Chunking |\n",
    "|-------|-------|------------------|---------------|\n",
    "| \"What compression techniques?\" | Methodology section | Entire paper (15K tokens) | Methodology (800 tokens) |\n",
    "| \"What were recall results?\" | Results + Table 1 | Entire paper (15K tokens) | Results section (600 tokens) |\n",
    "| \"How does HNSW work?\" | Background + Formula | Entire paper (15K tokens) | Background (500 tokens) |\n",
    "| \"What's the recommended config?\" | Discussion + Code | Entire paper (15K tokens) | Discussion (400 tokens) |\n",
    "\n",
    "**Impact:** 10-20x reduction in irrelevant context, leading to faster responses and better quality.\n",
    "\n",
    "**ðŸ’¡ Key Insight:** Chunking isn't about fitting in context windows - it's about **data modeling for retrieval**. Just like you wouldn't store all customer data in one database row, you shouldn't embed all document content in one vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a777303724f63ba",
   "metadata": {},
   "source": [
    "### Chunking Strategies: Engineering Trade-Offs\n",
    "\n",
    "Once you've determined that your data needs chunking, the next question is: **How should you chunk it?**\n",
    "\n",
    "There's no single \"best\" chunking strategy - the optimal approach depends on YOUR data characteristics and query patterns.\n",
    "\n",
    "### Strategy 1: Document-Based Chunking (Structure-Aware)\n",
    "\n",
    "**Concept:** Split documents based on their inherent structure (sections, paragraphs, headings).\n",
    "\n",
    "**Best for:** Structured documents with clear logical divisions (research papers, technical docs, books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24beec7e9b3f03f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:10.531504Z",
     "iopub.status.busy": "2025-12-05T00:14:10.531358Z",
     "iopub.status.idle": "2025-12-05T00:14:10.536014Z",
     "shell.execute_reply": "2025-12-05T00:14:10.535484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Strategy 1: Document-Based (Structure-Aware) Chunking\n",
      "================================================================================\n",
      "Original document: 890 tokens\n",
      "Number of chunks: 7\n",
      "\n",
      "Chunk breakdown:\n",
      "\n",
      "   Chunk 1: 8 tokens - # Optimizing Vector Search Performance in Redis...\n",
      "\n",
      "   Chunk 2: 108 tokens - ## Abstract This paper presents a comprehensive analysis of vector search optimization techniques in Redis, examining the trade-offs between search quality, latency, and memory usage. We evaluate multiple indexing strategies including HNSW and FLAT indexes across datasets ranging from 10K to 10M vec...\n",
      "\n",
      "   Chunk 3: 98 tokens - ## 1. Introduction Vector databases have become essential infrastructure for modern AI applications, enabling semantic search, recommendation systems, and retrieval-augmented generation (RAG). Redis, traditionally known as an in-memory data structure store, has evolved to support high-performance ve...\n",
      "\n",
      "   Chunk 4: 98 tokens - ## 2. Background and Related Work Previous work on vector search optimization has focused primarily on algorithmic improvements to approximate nearest neighbor (ANN) search. Malkov and Yashunin (2018) introduced HNSW, which has become the de facto standard for high-dimensional vector search. Johnson...\n",
      "\n",
      "   Chunk 5: 431 tokens - ## 3. Performance Analysis and Results  ### 3.1 HNSW Configuration Trade-offs  Table 1 shows the performance comparison across different HNSW configurations. As M increases from 16 to 64, we observe significant improvements in recall (0.89 to 0.97) but at the cost of increased latency (2.1ms to 8.7m...\n",
      "\n",
      "   Chunk 6: 111 tokens - ## 4. Implementation Recommendations  Based on our findings, we recommend the following configuration for real-world deployments:  ```python # Optimal HNSW configuration for balanced performance index_params = {     \"M\": 32,                  # Balance recall and latency     \"ef_construction\": 200,  ...\n",
      "\n",
      "   Chunk 7: 36 tokens - ## 5. Discussion and Conclusion Our findings demonstrate that vector search optimization is fundamentally about understanding YOUR specific requirements and constraints. There is no one-size-fits-all configuration....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Document-Based Chunking\n",
    "# Split research paper by sections (using markdown headers)\n",
    "\n",
    "\n",
    "def chunk_by_structure(text: str, separator: str = \"\\n## \") -> List[str]:\n",
    "    \"\"\"Split text by structural markers (e.g., markdown headers).\"\"\"\n",
    "\n",
    "    # Split by headers\n",
    "    sections = text.split(separator)\n",
    "\n",
    "    # Clean and format chunks\n",
    "    chunks = []\n",
    "    for i, section in enumerate(sections):\n",
    "        if section.strip():\n",
    "            # Add header back (except for first chunk which is title)\n",
    "            if i > 0:\n",
    "                chunk = \"## \" + section\n",
    "            else:\n",
    "                chunk = section\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Apply to research paper\n",
    "structure_chunks = chunk_by_structure(research_paper)\n",
    "\n",
    "print(f\"\"\"ðŸ“Š Strategy 1: Document-Based (Structure-Aware) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Number of chunks: {len(structure_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\")\n",
    "\n",
    "for i, chunk in enumerate(structure_chunks):\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    # Show first 100 chars of each chunk\n",
    "    preview = chunk[:300].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc17b33b9ad872",
   "metadata": {},
   "source": [
    "**Strategy 1 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- Respects document structure (sections stay together)\n",
    "- Semantically coherent (each chunk is a complete section)\n",
    "- Easy to implement for structured documents\n",
    "- **Keeps tables, formulas, and code WITH their context**\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Variable chunk sizes (some sections longer than others)\n",
    "- Requires documents to have clear structure\n",
    "- May create chunks that are still too large\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Research papers with clear sections\n",
    "- Technical documentation with headers\n",
    "- Books with chapters/sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285948b57b2bbe6",
   "metadata": {},
   "source": [
    "### Strategy 2: Fixed-Size Chunking (Token-Based)\n",
    "\n",
    "**Concept:** Split text into chunks of a predetermined size (e.g., 512 tokens) with overlap.\n",
    "\n",
    "**Best for:** Unstructured text, quick prototyping, when you need consistent chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f5e04659be266ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:10.537640Z",
     "iopub.status.busy": "2025-12-05T00:14:10.537534Z",
     "iopub.status.idle": "2025-12-05T00:14:27.870355Z",
     "shell.execute_reply": "2025-12-05T00:14:27.869948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Running fixed-size chunking with LangChain...\n",
      "   Trying to split on: paragraphs â†’ sentences â†’ words â†’ characters\n",
      "\n",
      "ðŸ“Š Strategy 2: Fixed-Size (LangChain) Chunking\n",
      "================================================================================\n",
      "Original document: 890 tokens\n",
      "Target chunk size: 800 characters (~200 words)\n",
      "Overlap: 100 characters\n",
      "Number of chunks: 7\n",
      "\n",
      "Chunk breakdown:\n",
      "\n",
      "   Chunk 1: 117 tokens - # Optimizing Vector Search Performance in Redis  ## Abstract This paper presents a comprehensive ana...\n",
      "   Chunk 2: 98 tokens - ## 1. Introduction Vector databases have become essential infrastructure for modern AI applications,...\n",
      "   Chunk 3: 134 tokens - [... 5,000 more words covering methodology, experiments, results, discussion ...]  ## 2. Background ...\n",
      "   Chunk 4: 129 tokens - ## 3. Performance Analysis and Results  ### 3.1 HNSW Configuration Trade-offs  Table 1 shows the per...\n",
      "   Chunk 5: 206 tokens - Table 1: HNSW Performance Comparison | M  | ef_construction | Recall@10 | Latency (ms) | Memory (GB)...\n",
      "... (2 more chunks)\n"
     ]
    }
   ],
   "source": [
    "# Strategy 2: Fixed-Size Chunking (Using LangChain)\n",
    "# Industry-standard approach with smart boundary detection\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter with smart boundary detection\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # Target chunk size in characters\n",
    "    chunk_overlap=100,  # Overlap to preserve context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Try these in order\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "print(\"ðŸ”„ Running fixed-size chunking with LangChain...\")\n",
    "print(\"   Trying to split on: paragraphs â†’ sentences â†’ words â†’ characters\\n\")\n",
    "\n",
    "# Apply to research paper\n",
    "fixed_chunks_docs = text_splitter.create_documents([research_paper])\n",
    "fixed_chunks = [doc.page_content for doc in fixed_chunks_docs]\n",
    "\n",
    "print(f\"\"\"ðŸ“Š Strategy 2: Fixed-Size (LangChain) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Target chunk size: 800 characters (~200 words)\n",
    "Overlap: 100 characters\n",
    "Number of chunks: {len(fixed_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\")\n",
    "\n",
    "for i, chunk in enumerate(fixed_chunks[:5]):  # Show first 5\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:100].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "print(f\"... ({len(fixed_chunks) - 5} more chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10171c485110d2e9",
   "metadata": {},
   "source": [
    "**Strategy 2 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- **Respects natural boundaries**: Tries paragraphs â†’ sentences â†’ words â†’ characters\n",
    "- Consistent chunk sizes (predictable token usage)\n",
    "- Works on any text (structured or unstructured)\n",
    "- **Doesn't split mid-sentence** (unless absolutely necessary)\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Ignores document structure (doesn't understand sections)\n",
    "- Can break semantic coherence (may split related content)\n",
    "- Overlap creates redundancy (increases storage/cost)\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Unstructured text (no clear sections)\n",
    "- Quick prototyping and baselines\n",
    "- When consistent chunk sizes are required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938cfbf6f4ce4b6",
   "metadata": {},
   "source": [
    "### Strategy 3: Semantic Chunking (Meaning-Based)\n",
    "\n",
    "**Concept:** Split text based on semantic similarity using embeddings - create new chunks when topic changes significantly.\n",
    "\n",
    "**How it works:**\n",
    "1. Split text into sentences or paragraphs\n",
    "2. Generate embeddings for each segment\n",
    "3. Calculate similarity between consecutive segments\n",
    "4. Create chunk boundaries where similarity drops (topic shift detected)\n",
    "\n",
    "**Best for:** Dense academic text, legal documents, narratives where semantic boundaries don't align with structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6baa8f66656a8dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:27.871498Z",
     "iopub.status.busy": "2025-12-05T00:14:27.871340Z",
     "iopub.status.idle": "2025-12-05T00:14:29.702609Z",
     "shell.execute_reply": "2025-12-05T00:14:29.702089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:27 sentence_transformers.SentenceTransformer INFO   Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Running semantic chunking with LangChain...\n",
      "   Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\n",
      "   Breakpoint detection: 25th percentile of similarity scores\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Strategy 3: Semantic (LangChain) Chunking\n",
      "================================================================================\n",
      "Original document: 890 tokens\n",
      "Number of chunks: 20\n",
      "\n",
      "Chunk breakdown:\n",
      "\n",
      "   Chunk 1: 70 tokens -  # Optimizing Vector Search Performance in Redis  ## Abstract This paper presents a comprehensive an...\n",
      "   Chunk 2: 26 tokens - Our results demonstrate that careful index configuration can improve search latency by up to 10x whi...\n",
      "   Chunk 3: 22 tokens - We also introduce novel compression techniques that reduce memory usage by 75% with minimal impact o...\n",
      "   Chunk 4: 4 tokens - ## 1....\n",
      "   Chunk 5: 60 tokens - Introduction Vector databases have become essential infrastructure for modern AI applications, enabl...\n",
      "... (15 more chunks)\n"
     ]
    }
   ],
   "source": [
    "# Strategy 3: Semantic Chunking (Using LangChain)\n",
    "# Industry-standard approach with local embeddings (no API costs!)\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Suppress tokenizer warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Initialize local embeddings (no API costs!)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "# Create semantic chunker with percentile-based breakpoint detection\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",  # Split at bottom 25% of similarities\n",
    "    breakpoint_threshold_amount=25,  # 25th percentile\n",
    "    buffer_size=1,  # Compare consecutive sentences\n",
    ")\n",
    "\n",
    "print(\"ðŸ”„ Running semantic chunking with LangChain...\")\n",
    "print(\"   Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\")\n",
    "print(\"   Breakpoint detection: 25th percentile of similarity scores\\n\")\n",
    "\n",
    "# Apply to research paper\n",
    "semantic_chunks_docs = semantic_chunker.create_documents([research_paper])\n",
    "\n",
    "# Extract text from Document objects\n",
    "semantic_chunks = [doc.page_content for doc in semantic_chunks_docs]\n",
    "\n",
    "print(f\"\"\"ðŸ“Š Strategy 3: Semantic (LangChain) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Number of chunks: {len(semantic_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\")\n",
    "\n",
    "for i, chunk in enumerate(semantic_chunks[:5]):  # Show first 5\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:100].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "if len(semantic_chunks) > 5:\n",
    "    print(f\"... ({len(semantic_chunks) - 5} more chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71132b3e19bbea8",
   "metadata": {},
   "source": [
    "**Strategy 3 Analysis:**\n",
    "\n",
    "âœ… **Advantages:**\n",
    "- **Meaning-aware**: Chunks based on topic shifts, not arbitrary boundaries\n",
    "- **Adaptive**: Chunk sizes vary based on content coherence\n",
    "- **Better retrieval**: Each chunk is semantically focused\n",
    "- **Free**: Uses local embeddings (no API costs)\n",
    "\n",
    "âš ï¸ **Trade-offs:**\n",
    "- Slower processing (requires embedding generation)\n",
    "- Variable chunk sizes (harder to predict token usage)\n",
    "- May not respect document structure (sections, headers)\n",
    "- Requires tuning (threshold, buffer size)\n",
    "\n",
    "ðŸŽ¯ **Best for:**\n",
    "- Dense academic text\n",
    "- Legal documents\n",
    "- Narratives and stories\n",
    "- Content where semantic boundaries don't align with structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcce73c6937c9d8",
   "metadata": {},
   "source": [
    "### Comparing Chunking Strategies: Decision Framework\n",
    "\n",
    "Now let's compare all strategies side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "356ff7c61de8291d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:29.703836Z",
     "iopub.status.busy": "2025-12-05T00:14:29.703719Z",
     "iopub.status.idle": "2025-12-05T00:14:29.706988Z",
     "shell.execute_reply": "2025-12-05T00:14:29.706557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHUNKING STRATEGY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Document: Research Paper (890 tokens)\n",
      "\n",
      "Strategy              | Chunks | Avg Size | Complexity | Best For\n",
      "--------------------- | ------ | -------- | ---------- | --------\n",
      "Document-Based        |      7 |      127 | Low        | Structured docs\n",
      "Fixed-Size            |      7 |      137 | Low        | Unstructured text\n",
      "Semantic              |     20 |       44 | High       | Dense academic text\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "{'=' * 80}\n",
    "CHUNKING STRATEGY COMPARISON\n",
    "{'=' * 80}\n",
    "\n",
    "Document: Research Paper ({paper_tokens:,} tokens)\n",
    "\n",
    "Strategy              | Chunks | Avg Size | Complexity | Best For\n",
    "--------------------- | ------ | -------- | ---------- | --------\n",
    "Document-Based        | {len(structure_chunks):>6} | {sum(count_tokens(c) for c in structure_chunks) // len(structure_chunks):>8} | Low        | Structured docs\n",
    "Fixed-Size            | {len(fixed_chunks):>6} | {sum(count_tokens(c) for c in fixed_chunks) // len(fixed_chunks):>8} | Low        | Unstructured text\n",
    "Semantic              | {len(semantic_chunks):>6} | {sum(count_tokens(c) for c in semantic_chunks) // len(semantic_chunks):>8} | High       | Dense academic text\n",
    "\n",
    "{'=' * 80}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5942972932ee4",
   "metadata": {},
   "source": [
    "### YOUR Chunking Decision Framework\n",
    "\n",
    "Chunking strategy is a **design choice** that depends on your specific context. There's no universal \"correct\" chunk size.\n",
    "\n",
    "**Step 1: Start with Document Type**\n",
    "\n",
    "| Document Type | Default Approach | Reasoning |\n",
    "|---------------|------------------|----------|\n",
    "| **Structured records** (courses, products, FAQs) | Don't chunk | Natural boundaries already exist |\n",
    "| **Long-form text** (papers, books, docs) | Consider chunking | May need retrieval precision |\n",
    "| **PDFs with visual layout** | Page-level | Preserves tables, figures |\n",
    "| **Code** | Function/class boundaries | Semantic structure matters |\n",
    "\n",
    "**Step 2: Evaluate These Factors**\n",
    "\n",
    "1. **Semantic completeness:** Is each item self-contained?\n",
    "   - âœ… Yes â†’ Don't chunk (preserve natural boundaries)\n",
    "   - âŒ No â†’ Consider chunking strategy\n",
    "\n",
    "2. **Query patterns:** What will users ask?\n",
    "   - Specific facts â†’ Smaller, focused chunks help\n",
    "   - Summaries/overviews â†’ Larger chunks or hierarchical\n",
    "   - Mixed â†’ Consider hierarchical approach\n",
    "\n",
    "3. **Topic density:** How many distinct topics per document?\n",
    "   - Single topic â†’ Whole-document embedding often works\n",
    "   - Multiple distinct topics â†’ Chunking may improve precision\n",
    "\n",
    "**Example Decisions:**\n",
    "\n",
    "| Domain | Data Characteristics | Decision | Why |\n",
    "|--------|---------------------|----------|-----|\n",
    "| **Course Catalog** | Small, self-contained records | **Don't chunk** | Each course is a complete retrieval unit |\n",
    "| **Research Papers** | Multi-section, dense topics | Document-Based | Sections are natural semantic units |\n",
    "| **Support Tickets** | Single issue per ticket | **Don't chunk** | Already at optimal granularity |\n",
    "| **Legal Contracts** | Nested structure, many clauses | Hierarchical | Need both overview and clause-level detail |\n",
    "\n",
    "> ðŸ’¡ **Key Takeaway:** Ask \"What is my natural retrieval unit?\" before deciding on a chunking strategy. For many structured data use cases, the answer is \"don't chunk.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d695bbb1b79c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Building Practical Context Pipelines\n",
    "\n",
    "Now that you understand data transformation and chunking, let's discuss how to build reusable pipelines.\n",
    "\n",
    "### Three Pipeline Architectures\n",
    "\n",
    "There are three main approaches to context preparation in real-world applications:\n",
    "\n",
    "### Architecture 1: Request-Time Processing\n",
    "\n",
    "**Concept:** Transform data on-the-fly when a query arrives.\n",
    "\n",
    "```\n",
    "User Query â†’ Retrieve Raw Data â†’ Transform â†’ Chunk (if needed) â†’ Embed â†’ Search â†’ Return Context\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Always up-to-date (no stale data)\n",
    "- âœ… No pre-processing required\n",
    "- âœ… Simple to implement\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Higher latency (processing happens during request)\n",
    "- âŒ Repeated work (same transformations for every query)\n",
    "- âŒ Not suitable for large datasets\n",
    "\n",
    "**Best for:**\n",
    "- Small datasets (< 1,000 documents)\n",
    "- Frequently changing data\n",
    "- Simple transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab7a964eb64893",
   "metadata": {},
   "source": [
    "### Architecture 2: Batch Processing\n",
    "\n",
    "**Concept:** Pre-process all data in batches (nightly, weekly) and store results.\n",
    "\n",
    "```\n",
    "[Scheduled Job]\n",
    "Raw Data â†’ Extract â†’ Clean â†’ Transform â†’ Chunk â†’ Embed â†’ Store in Vector DB\n",
    "\n",
    "[Query Time]\n",
    "User Query â†’ Search Vector DB â†’ Return Pre-Processed Context\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Fast query time (all processing done ahead)\n",
    "- âœ… Efficient (process once, use many times)\n",
    "- âœ… Can use expensive transformations (LLM-based chunking, semantic analysis)\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Data can be stale (until next batch run)\n",
    "- âŒ Requires scheduling infrastructure\n",
    "- âŒ Higher storage costs (store processed data)\n",
    "\n",
    "**Best for:**\n",
    "- Large datasets (> 10,000 documents)\n",
    "- Infrequently changing data (daily/weekly updates)\n",
    "- Complex transformations (semantic chunking, LLM summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3f45f0e0cf8d4",
   "metadata": {},
   "source": [
    "### Architecture 3: Event-Driven Processing\n",
    "\n",
    "**Concept:** Process data as it changes (real-time updates).\n",
    "\n",
    "```\n",
    "Data Change Event â†’ Trigger Pipeline â†’ Extract â†’ Clean â†’ Transform â†’ Chunk â†’ Embed â†’ Update Vector DB\n",
    "\n",
    "[Query Time]\n",
    "User Query â†’ Search Vector DB â†’ Return Context\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Always up-to-date (real-time)\n",
    "- âœ… Fast query time (pre-processed)\n",
    "- âœ… Efficient (only process changed data)\n",
    "\n",
    "**Cons:**\n",
    "- âŒ Complex infrastructure (event streams, queues)\n",
    "- âŒ Requires change detection\n",
    "- âŒ Higher operational complexity\n",
    "\n",
    "**Best for:**\n",
    "- Real-time data (news, social media, live updates)\n",
    "- Large datasets that change frequently\n",
    "- When both freshness and speed are critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f87c3052b1128d",
   "metadata": {},
   "source": [
    "### Choosing YOUR Pipeline Architecture\n",
    "\n",
    "Use this decision tree:\n",
    "\n",
    "**Question 1: How often does your data change?**\n",
    "- Real-time (seconds/minutes) â†’ Event-Driven\n",
    "- Frequently (hourly/daily) â†’ Batch or Event-Driven\n",
    "- Infrequently (weekly/monthly) â†’ Batch\n",
    "- Rarely (manual updates) â†’ Request-Time or Batch\n",
    "\n",
    "**Question 2: How large is your dataset?**\n",
    "- Small (< 1,000 docs) â†’ Request-Time\n",
    "- Medium (1,000-100,000 docs) â†’ Batch\n",
    "- Large (> 100,000 docs) â†’ Batch or Event-Driven\n",
    "\n",
    "**Question 3: What are your latency requirements?**\n",
    "- Real-time (< 100ms) â†’ Batch or Event-Driven (pre-processed)\n",
    "- Interactive (< 1s) â†’ Any approach\n",
    "- Batch queries â†’ Request-Time acceptable\n",
    "\n",
    "**Example Decision:**\n",
    "\n",
    "For Redis University:\n",
    "- âœ… **Data changes:** Weekly (new courses added)\n",
    "- âœ… **Dataset size:** 100-500 courses (medium)\n",
    "- âœ… **Latency:** Interactive (< 1s acceptable)\n",
    "- âœ… **Transformations:** Moderate (structured views + embeddings)\n",
    "- âœ… **Decision:** **Batch Processing** (weekly job to rebuild catalog + embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba92cfb52e16d73",
   "metadata": {},
   "source": [
    "### Example: Batch Processing Pipeline for Redis University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38369bc830366f9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:29.708055Z",
     "iopub.status.busy": "2025-12-05T00:14:29.707985Z",
     "iopub.status.idle": "2025-12-05T00:14:30.210251Z",
     "shell.execute_reply": "2025-12-05T00:14:30.209538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BATCH PROCESSING PIPELINE - Redis University Courses\n",
      "================================================================================\n",
      "\n",
      "[Step 1/5] Extracting course data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:30 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Extracted 50 courses\n",
      "\n",
      "   ðŸ“„ Sample raw course:\n",
      "      CS008: Natural Language Processing\n",
      "      Department: Computer Science, Credits: 3, Level: advanced\n",
      "\n",
      "[Step 2/5] Cleaning data...\n",
      "   âœ… Cleaned: 50 courses (removed 0 test courses)\n",
      "\n",
      "[Step 3/5] Transforming to LLM-friendly format...\n",
      "   âœ… Transformed: 50 courses (4,765 total tokens)\n",
      "\n",
      "[Step 4/5] Creating structured catalog view...\n",
      "   âœ… Created catalog view (259 tokens)\n",
      "   âœ… Cached in Redis\n",
      "\n",
      "[Step 5/5] Storing processed data...\n",
      "   âœ… Stored 50 processed courses in Redis\n",
      "\n",
      "================================================================================\n",
      "BATCH PROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Summary:\n",
      "- Courses processed: 50\n",
      "- Total tokens: 4,765\n",
      "- Catalog view tokens: 259\n",
      "- Storage: Redis\n",
      "- Next run: 2024-10-07 (weekly)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Batch Processing Pipeline\n",
    "# This would run as a scheduled job (e.g., weekly)\n",
    "\n",
    "\n",
    "async def batch_process_courses():\n",
    "    \"\"\"\n",
    "    Batch processing pipeline for Redis University courses.\n",
    "    Runs weekly to update catalog and embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"BATCH PROCESSING PIPELINE - Redis University Courses\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: Extract\n",
    "    print(\"\\n[Step 1/5] Extracting course data...\")\n",
    "    all_courses_batch = await course_manager.get_all_courses()\n",
    "    print(f\"   âœ… Extracted {len(all_courses_batch)} courses\")\n",
    "\n",
    "    # Show sample raw data\n",
    "    if all_courses_batch:\n",
    "        sample = all_courses_batch[0]\n",
    "        print(f\"\\n   ðŸ“„ Sample raw course:\")\n",
    "        print(f\"      {sample.course_code}: {sample.title}\")\n",
    "        print(f\"      Department: {sample.department}, Credits: {sample.credits}, Level: {sample.difficulty_level.value}\")\n",
    "\n",
    "    # Step 2: Clean\n",
    "    print(\"\\n[Step 2/5] Cleaning data...\")\n",
    "    # Remove test courses, validate fields, etc.\n",
    "    cleaned_courses = [\n",
    "        c for c in all_courses_batch if c.course_code.startswith((\"RU\", \"CS\", \"MATH\"))\n",
    "    ]\n",
    "    print(f\"   âœ… Cleaned: {len(cleaned_courses)} courses (removed {len(all_courses_batch) - len(cleaned_courses)} test courses)\")\n",
    "\n",
    "    # Step 3: Transform\n",
    "    print(\"\\n[Step 3/5] Transforming to LLM-friendly format...\")\n",
    "    transformed_courses = [transform_course_to_text(c) for c in cleaned_courses]\n",
    "    total_tokens = sum(count_tokens(t) for t in transformed_courses)\n",
    "    print(f\"   âœ… Transformed: {len(transformed_courses)} courses ({total_tokens:,} total tokens)\")\n",
    "\n",
    "    # Step 4: Create Structured Views\n",
    "    print(\"\\n[Step 4/5] Creating structured catalog view...\")\n",
    "    catalog_view_batch = await create_catalog_view()\n",
    "    catalog_tokens_batch = count_tokens(catalog_view_batch)\n",
    "    redis_client.set(\"course_catalog_view\", catalog_view_batch)\n",
    "    redis_client.set(\"course_catalog_view:updated\", \"2024-09-30\")\n",
    "    print(f\"   âœ… Created catalog view ({catalog_tokens_batch:,} tokens)\")\n",
    "    print(f\"   âœ… Cached in Redis\")\n",
    "\n",
    "    # Step 5: Store (in production, would also create embeddings and store in vector DB)\n",
    "    print(\"\\n[Step 5/5] Storing processed data...\")\n",
    "    for i, (course, text) in enumerate(zip(cleaned_courses, transformed_courses)):\n",
    "        key = f\"course:processed:{course.course_code}\"\n",
    "        redis_client.set(key, text)\n",
    "    print(f\"   âœ… Stored {len(cleaned_courses)} processed courses in Redis\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BATCH PROCESSING COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\"\"\n",
    "Summary:\n",
    "- Courses processed: {len(cleaned_courses)}\n",
    "- Total tokens: {total_tokens:,}\n",
    "- Catalog view tokens: {catalog_tokens_batch:,}\n",
    "- Storage: Redis\n",
    "- Next run: 2024-10-07 (weekly)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Run the batch pipeline\n",
    "await batch_process_courses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd754a72b0f164b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Quality Optimization - Measuring and Improving Context\n",
    "\n",
    "### The Systematic Optimization Process\n",
    "\n",
    "Now that you understand data engineering and context pipelines, let's learn how to systematically optimize context quality.\n",
    "\n",
    "**The Process:**\n",
    "```\n",
    "1. Define Quality Metrics (domain-specific)\n",
    "   â†“\n",
    "2. Establish Baseline (measure current performance)\n",
    "   â†“\n",
    "3. Experiment (try different approaches)\n",
    "   â†“\n",
    "4. Measure (compare against metrics)\n",
    "   â†“\n",
    "5. Iterate (refine based on results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e75796a4e7add7c",
   "metadata": {},
   "source": [
    "### Step 1: Define Quality Metrics for YOUR Domain\n",
    "\n",
    "**The Problem with Generic Metrics:**\n",
    "\n",
    "Don't aim for \"95% accuracy on benchmark X\" - that benchmark wasn't designed for YOUR domain.\n",
    "\n",
    "**DO this instead:** Define what \"quality\" means for YOUR domain, then measure it.\n",
    "\n",
    "### The Four Quality Dimensions\n",
    "\n",
    "Every context engineering solution should be evaluated across four dimensions:\n",
    "\n",
    "1. **Relevance** - Does context include information needed to answer the query?\n",
    "2. **Completeness** - Does context include ALL necessary information?\n",
    "3. **Efficiency** - Is context optimized for token usage?\n",
    "4. **Accuracy** - Is context factually correct and up-to-date?\n",
    "\n",
    "Different domains prioritize these differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "242ab497225bd307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:30.212509Z",
     "iopub.status.busy": "2025-12-05T00:14:30.212316Z",
     "iopub.status.idle": "2025-12-05T00:14:30.216878Z",
     "shell.execute_reply": "2025-12-05T00:14:30.216293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUALITY METRICS FOR REDIS UNIVERSITY COURSE ADVISOR\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "Relevance:\n",
      "  Definition: Does context include courses relevant to the user's query?\n",
      "  Metric: % of queries where retrieved courses match query intent\n",
      "  How to measure: Manual review of 50 sample queries\n",
      "  Target: >90%\n",
      "  Why important: Irrelevant courses waste tokens and confuse users\n",
      "\n",
      "Completeness:\n",
      "  Definition: Does context include all information needed to answer?\n",
      "  Metric: % of responses that mention all prerequisites when asked\n",
      "  How to measure: Automated check: parse response for prerequisite mentions\n",
      "  Target: 100%\n",
      "  Why important: Missing prerequisites leads to hallucinations\n",
      "\n",
      "Efficiency:\n",
      "  Definition: Is context optimized for token usage?\n",
      "  Metric: Average tokens per query\n",
      "  How to measure: Token counter on all context strings\n",
      "  Target: <5,000 tokens\n",
      "  Why important: Exceeding budget increases cost and latency\n",
      "\n",
      "Accuracy:\n",
      "  Definition: Is context factually correct and up-to-date?\n",
      "  Metric: % of responses with correct course information\n",
      "  How to measure: Manual review against course database\n",
      "  Target: >95%\n",
      "  Why important: Incorrect information damages trust\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
     ]
    }
   ],
   "source": [
    "# Define domain-specific quality metrics\n",
    "\n",
    "quality_metrics = {\n",
    "    \"Relevance\": {\n",
    "        \"definition\": \"Does context include courses relevant to the user's query?\",\n",
    "        \"metric\": \"% of queries where retrieved courses match query intent\",\n",
    "        \"measurement\": \"Manual review of 50 sample queries\",\n",
    "        \"target\": \">90%\",\n",
    "        \"why_important\": \"Irrelevant courses waste tokens and confuse users\",\n",
    "    },\n",
    "    \"Completeness\": {\n",
    "        \"definition\": \"Does context include all information needed to answer?\",\n",
    "        \"metric\": \"% of responses that mention all prerequisites when asked\",\n",
    "        \"measurement\": \"Automated check: parse response for prerequisite mentions\",\n",
    "        \"target\": \"100%\",\n",
    "        \"why_important\": \"Missing prerequisites leads to hallucinations\",\n",
    "    },\n",
    "    \"Efficiency\": {\n",
    "        \"definition\": \"Is context optimized for token usage?\",\n",
    "        \"metric\": \"Average tokens per query\",\n",
    "        \"measurement\": \"Token counter on all context strings\",\n",
    "        \"target\": \"<5,000 tokens\",\n",
    "        \"why_important\": \"Exceeding budget increases cost and latency\",\n",
    "    },\n",
    "    \"Accuracy\": {\n",
    "        \"definition\": \"Is context factually correct and up-to-date?\",\n",
    "        \"metric\": \"% of responses with correct course information\",\n",
    "        \"measurement\": \"Manual review against course database\",\n",
    "        \"target\": \">95%\",\n",
    "        \"why_important\": \"Incorrect information damages trust\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\"\"QUALITY METRICS FOR REDIS UNIVERSITY COURSE ADVISOR\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\"\")\n",
    "for dimension, details in quality_metrics.items():\n",
    "    print(f\"\"\"\n",
    "{dimension}:\n",
    "  Definition: {details['definition']}\n",
    "  Metric: {details['metric']}\n",
    "  How to measure: {details['measurement']}\n",
    "  Target: {details['target']}\n",
    "  Why important: {details['why_important']}\"\"\")\n",
    "print(\"\\n\" + \"â”\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc169c21d0b0351b",
   "metadata": {},
   "source": [
    "### Key Insight: Metrics Must Be Domain-Specific\n",
    "\n",
    "Notice how these metrics are specific to the course advisor domain:\n",
    "\n",
    "**Relevance metric:**\n",
    "- âŒ Generic: \"Cosine similarity > 0.8\"\n",
    "- âœ… Domain-specific: \"Retrieved courses match query intent\"\n",
    "\n",
    "**Completeness metric:**\n",
    "- âŒ Generic: \"Context includes top-5 search results\"\n",
    "- âœ… Domain-specific: \"All prerequisites mentioned when asked\"\n",
    "\n",
    "**Efficiency metric:**\n",
    "- âŒ Generic: \"Minimize tokens\"\n",
    "- âœ… Domain-specific: \"<5,000 tokens (fits our budget)\"\n",
    "\n",
    "**Accuracy metric:**\n",
    "- âŒ Generic: \"95% on MMLU benchmark\"\n",
    "- âœ… Domain-specific: \"Correct course information vs. database\"\n",
    "\n",
    "**Your metrics should reflect YOUR domain's requirements, not generic benchmarks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8079a92dc6b0de",
   "metadata": {},
   "source": [
    "### Key Takeaways: Quality Optimization\n",
    "\n",
    "1. **Define Domain-Specific Metrics** - Don't rely on generic benchmarks\n",
    "2. **Measure Systematically** - Baseline â†’ Experiment â†’ Measure â†’ Iterate\n",
    "3. **Balance Trade-offs** - Relevance vs. Efficiency, Completeness vs. Token Budget\n",
    "4. **Test Before Deployment** - Validate with real queries from your domain\n",
    "5. **Iterate Continuously** - Quality optimization is ongoing, not one-time\n",
    "\n",
    "**The Engineering Mindset:**\n",
    "- Context quality is measurable\n",
    "- Optimization is systematic, not guesswork\n",
    "- Domain-specific metrics matter more than generic benchmarks\n",
    "- Testing and iteration are essential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892a4fce2970721",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "**1. Context is Data - and Data Requires Engineering**\n",
    "- Context isn't just \"data you feed to an LLM\"\n",
    "- It requires systematic transformation: Raw â†’ Clean â†’ Transform â†’ Optimize â†’ Store\n",
    "- Engineering discipline: requirements analysis, design decisions, quality metrics, testing\n",
    "\n",
    "**2. The Data Engineering Pipeline**\n",
    "- Extract: Get raw data from sources\n",
    "- Clean: Remove noise, fix inconsistencies\n",
    "- Transform: Structure for LLM consumption\n",
    "- Optimize: Reduce tokens, improve clarity\n",
    "- Store: Choose storage strategy (RAG, Views, Hybrid)\n",
    "\n",
    "**3. Three Engineering Approaches**\n",
    "- **RAG:** Semantic search for relevant data (good for specific queries)\n",
    "- **Structured Views:** Pre-computed summaries (excellent for overviews)\n",
    "- **Hybrid:** Combine both (best for real-world use)\n",
    "\n",
    "**4. Chunking is a Design Choice, Not a Default**\n",
    "- **\"Don't chunk\"** is a valid strategy for structured data (courses, products, FAQs)\n",
    "- Consider chunking when documents have multiple distinct topics or need retrieval precision\n",
    "- Three strategies: Document-Based, Fixed-Size, Semantic\n",
    "- Choose based on YOUR data type, query patterns, and application requirements\n",
    "\n",
    "**5. Context Pipeline Architectures**\n",
    "- **Request-Time:** Process on-the-fly (simple, always fresh, higher latency)\n",
    "- **Batch:** Pre-process in batches (fast queries, can be stale)\n",
    "- **Event-Driven:** Process on changes (real-time, complex infrastructure)\n",
    "\n",
    "**6. Quality Optimization**\n",
    "- Define domain-specific quality metrics\n",
    "- Systematic optimization: Baseline â†’ Experiment â†’ Measure â†’ Iterate\n",
    "- Four dimensions: Relevance, Completeness, Efficiency, Accuracy\n",
    "\n",
    "### The Engineering Mindset\n",
    "\n",
    "Every decision should be based on **YOUR specific requirements:**\n",
    "\n",
    "1. **Analyze YOUR data:** Size, structure, update frequency, topic density\n",
    "2. **Analyze YOUR queries:** Specific vs. overview, single vs. cross-section\n",
    "3. **Analyze YOUR constraints:** Token budget, latency, quality requirements\n",
    "4. **Make informed decisions:** Choose approaches that match YOUR needs\n",
    "5. **Measure and iterate:** Test with real queries, measure quality, optimize\n",
    "\n",
    "**Remember:** There is no \"best practice\" that works for everyone. Context engineering is about making deliberate, informed choices based on YOUR domain, application, and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d00fbb9e938f38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Analyze Your Data\n",
    "Think about a dataset you work with. Answer these questions:\n",
    "1. What is the natural retrieval unit? (record, document, section?)\n",
    "2. Does it need chunking? Why or why not?\n",
    "3. Which engineering approach would you use? (RAG, Structured Views, Hybrid)\n",
    "\n",
    "### Exercise 2: Design Quality Metrics\n",
    "For your domain, define:\n",
    "1. What does \"relevance\" mean?\n",
    "2. What does \"completeness\" mean?\n",
    "3. What is your token budget?\n",
    "4. How will you measure accuracy?\n",
    "\n",
    "### Exercise 3: Choose a Pipeline Architecture\n",
    "Based on your data characteristics:\n",
    "1. How often does your data change?\n",
    "2. How large is your dataset?\n",
    "3. What are your latency requirements?\n",
    "4. Which architecture would you choose? (Request-Time, Batch, Event-Driven)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7aba5a3919a3d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "### Module 4: Memory Systems for Context Engineering\n",
    "\n",
    "Now that you can engineer high-quality retrieved context, you'll learn to manage conversation context:\n",
    "- **Working Memory:** Track conversation history within a session\n",
    "- **Long-term Memory:** Remember user preferences across sessions\n",
    "- **Memory-Enhanced RAG:** Combine all four context types\n",
    "- **Redis Agent Memory Server:** Automatic memory extraction and retrieval\n",
    "\n",
    "```\n",
    "Module 1: Context Engineering Fundamentals\n",
    "    â†“\n",
    "Module 2: RAG Fundamentals â† Completed\n",
    "    â†“\n",
    "Module 3: Data Engineering for Context â† You are here\n",
    "    â†“\n",
    "Module 4: Memory Systems â† Next\n",
    "    â†“\n",
    "Module 5: Building Agents (Complete System)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "**Chunking Strategies:**\n",
    "- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "- [LlamaIndex Node Parsers](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/)\n",
    "\n",
    "**Data Engineering for LLMs:**\n",
    "- [OpenAI Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "\n",
    "**Vector Databases:**\n",
    "- [Redis Vector Search Documentation](https://redis.io/docs/stack/search/reference/vectors/)\n",
    "- [RedisVL Python Library](https://github.com/RedisVentures/redisvl)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
