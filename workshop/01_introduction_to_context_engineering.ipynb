{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/redislabs-training/ce-redis-langchain/blob/main/section-1-context-engineering-foundations/01_Introduction_to_context_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Over the past few years, AI applications have undergone rapid evolution. Early on, engineers built a polite clerk behind a glass window: users would slide a note - \"draft this email,\" \"explain this code\" â€” and the clerk would read it, scribble an answer, and slide it back. Their answers were generally good, but not always what we wanted. \n",
    "\n",
    "Users quickly realized that if they *precisely* engineer what the note says, they can, in turn, achieve better results from the clerk. So, they used more precise instructions â€” \"act as my editor,\" \"use this tone,\" \"return a table\" â€” and the outputs became sharper. This, in turn, became the practice of prompt engineering. \n",
    "\n",
    "Prompt engineering took users further, but eventually showed its limits â€” especially with our clerk behind the glass. We kept hoping the wording would stick, but longer notes got slow and contradictory. Tiny phrasing swings changed behavior in ways that were hard to predict or control, and no amount of wordsmithing seemed to fully fix it.\n",
    "\n",
    "For a while, if you were building with these systems, a common playbook for solving these issues was simple: upgrade the model. Swap in the latest version and your clerk gets sharper responses, a better grasp of nuance, and enough capability to attempt specialized tasks like research or creative exploration.\n",
    "\n",
    "However, teams weren't just waiting for better models, they were experimenting with everything they could. Some tried teaching the clerk through examples right there in the note: \\\"Here's how you handled a similar question before â€” now do it again.\\\" This became known as few-shot prompting. Others added step-by-step reasoning scaffolds â€” \\\"Think through this carefully, show your work\\\" â€” which we now call chain-of-thought. A few teams went further, fine-tuning their own versions of the clerk for specific domains, essentially retraining them on company-specific knowledge.\n",
    "\n",
    "And then there was retrieval. Instead of hoping the clerk remembered everything, engineers started handing them reference documents alongside each note: \\\"Here's the relevant policy,\\\" \\\"Here's what the customer said last week.\\\" This pattern â€” Retrieval Augmented Generation, or RAG â€” turned out to be one of the most practical techniques of the era.\n",
    "\n",
    "These approaches helped! The clerk began to catch more nuances in the notes that were passed. It could juggle longer instructions, synthesize more information, and make fewer obvious mistakes. For many teams, it felt like there were finally multiple knobs to turn, not just one.\n",
    "\n",
    "But under the hood, the old cracks were still there. The clerk was still working mostly from whatever happened to be on the note in front of them. They still forgot everything between one visit and the next. When they didnâ€™t know something, they still guessed instead of checking. In other words, weâ€™d upgraded the clerk, but not the glass, not the office, and not the way work flowed around them.\n",
    "\n",
    "Then came agents.\n",
    "\n",
    "Instead of a clerk answering one note at a time, we started asking that same clerk to run tasks, such as breaking down a goal, fetching information from the internet, and reading and updating complex code. Our polite assistant became a kind of project manager.\n",
    "\n",
    "In our metaphor, it was as if we took the clerk from behind the glass and said: \"Feel free to walk the halls. Use the phone, dig through the filing cabinets, talk to other departments, and come back with a finished result.\"\n",
    "\n",
    "This unlocked striking capabilities â€” but it also made the old problems impossible to ignore. Small ambiguities in instructions ballooned over ten or twenty steps. Missing background information meant the clerk confidently walked in the wrong direction for minutes at a time. Every task and its associated subtasks came with its own little note â€” its own tiny prompt â€” and suddenly the clerk wasn't wrestling with one interaction, but with a swarm of them, all slightly inconsistent.\n",
    "\n",
    "So, what do these issues look like if we were to crack open one of these agentic applications? Where do \"missing background,\" \"forgotten preferences,\" and that \"swarm of tiny prompts\" actually live?\n",
    "\n",
    "In practice, they all appear as distinct components of context surrounding a model call: the state and history you carry forward, the prompts and policies you apply, the memories you load, the documents you retrieve, and the structured outputs you expect to receive back. Visually, it looks something like this:\n",
    "\n",
    "![Context engineering overview](../public/ce-overview.png)\n",
    "\n",
    "Things like user input, operational databases, web APIs, and data warehouses all feed into the applicationâ€™s context layer. Everything inside that dashed red circle is what we, as engineers, design and control before a single token ever reaches the LLM model. Instead of only asking, \"Whatâ€™s the right prompt?\" or \"Whatâ€™s the right model?\", we are now shifting to ask, â€œWhat information should be present at every step of this process, and how do we control it?â€\n",
    "\n",
    "That shift â€” from crafting individual notes to deliberately shaping the information surrounding the application â€” is what has become known as context engineering.\n",
    "\n",
    "Let's now ground this idea in an example application for a Redis University course agent and watch where context engineering makes the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Engineering in Action\n",
    "\n",
    "We can imagine, that at most universities today, one of the biggest pain points for learners is the time-suck of researching and selecting their academic classes for the year. In most cases, this involves a tedious process of waiting for their registration time slot to open (as often more senior students have priority), scrolling various pages of courses/descriptions, finding if they meet the prerequisites, and so on. There are even dedicated roles designed to help students decide on what classes are the right fit and assist them with their planning. \n",
    "\n",
    "As an engineer part of the Redis University Innovation Team, our hope is to solve this issue by creating a seamless interface whereby a agent can help expedite the research and selection process of a student's courses. \n",
    "\n",
    "Let's now imagine we did indeed build this agent and are running a pilot of the results with a subset of students. Lets examine a common scenario we want the agent to deal with: A student seeking advice on what classes to take to help build a career in AI. \n",
    "\n",
    "We'll use a sample student profile as our base: \n",
    "\n",
    "- Name: Olivia Jansen\n",
    "- Major: Computer Science\n",
    "- Year: Sophomore\n",
    "- Completed: Intro to Programming (CS101), Data Structures (CS201), Calculus I\n",
    "- Interests: Machine learning, Data science\n",
    "- Preferences: Prefers online courses, learns best with hands-on projects\n",
    "- Goal: Build a career in AI\n",
    "\n",
    "Olivia goes ahead and interacts with the agent and has the following results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>ğŸ”¶ Click the dropdown to review the conversation with the agent</summary>\n",
    "  \n",
    "**Turn 1:**\n",
    "\n",
    "*Student:* \"I'm interested in machine learning courses\"\n",
    "\n",
    "*Agent:* \"Here are all machine learning courses available:\n",
    "- CS301: Introduction to Machine Learning\n",
    "- CS401: Advanced Machine Learning\n",
    "- CS402: Deep Learning\n",
    "- CS403: Natural Language Processing\n",
    "- CS404: Computer Vision\n",
    "\n",
    "All courses meet twice per week on campus.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Turn 2:**\n",
    "\n",
    "*Student:* \"I prefer online courses.\"\n",
    "\n",
    "*Agent:* \"Here are all machine learning courses available:\n",
    "- CS301: Introduction to Machine Learning\n",
    "- CS401: Advanced Machine Learning\n",
    "...\"\n",
    "\n",
    "Notice that the result is the same list, ignoring Olivia's preferences.\n",
    "\n",
    "---\n",
    "\n",
    "**Turn 3:**\n",
    "\n",
    "*Student:* \"Can I take CS401?\"\n",
    "\n",
    "*Agent:* \"CS401 requires CS301 as a prerequisite. Do you meet this requirement?\"\n",
    "\n",
    "The agent does not respond adequately as it doesn't know what courses the student has completed\n",
    "\n",
    "---\n",
    "\n",
    "**Turn 4:**\n",
    "\n",
    "*Student:* \"What about that intro course you mentioned earlier?\"\n",
    "\n",
    "*Agent:* \"I'm not sure which course you're referring to. Could you provide more details?\"\n",
    "\n",
    "The agent does not remember its old replies and has no context on what intro course was mentioned. \n",
    "\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This advisor is intentionally simple, but its failure modes mirror those of real systems: it has no memory (canâ€™t recall prior turns), no personalization (ignores her preferences and background), and no awareness (doesnâ€™t know her history, canâ€™t check prerequisites, and cannot connect relevant information). The result is a frustrating experience that wastes time and delivers little value to the student.\n",
    "\n",
    "We don't want to leave our students feeling disappointed, so we spend some time applying some common context engineering techniques to our agent to improve the results. Now, let's examine the agent, but with proper context management: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>ğŸ”¶ Click the dropdown to review the conversation with the agent with proper context management </summary>\n",
    "\n",
    "**Turn 1:**\n",
    "\n",
    "*Student:* \"I'm interested in machine learning courses.\"\n",
    "\n",
    "*Agent:* \"Great! Based on your Computer Science major and the courses you've completed (CS101 and CS201), you're ready for CS301: Introduction to Machine Learning.\n",
    "\n",
    "Since you prefer online courses and hands-on learning, I should mention that CS301 has:\n",
    "- A fully online section is available\n",
    "- Weekly programming assignments with real datasets\n",
    "- A final project where you build your own ML system\n",
    "\n",
    "This course aligns perfectly with your goal of building an AI career and serves as the foundation for more advanced ML courses.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Turn 2:**\n",
    "\n",
    "*Student:* \"That sounds perfect! What comes after that?\"\n",
    "\n",
    "*Agent:* \"After completing CS301, you'll have several great options:\n",
    "\n",
    "**Immediately Available:**\n",
    "- CS402: Deep Learning (Online, project-based)\n",
    "  Focus: Neural networks, CNNs, RNNs\n",
    "  \n",
    "- CS403: Natural Language Processing (Online, project-based)\n",
    "  Focus: Text analysis, transformers, LLMs\n",
    "\n",
    "**Recommended Path for AI Career:**\n",
    "1. CS301 (Introduction) â† Start here\n",
    "2. CS402 (Deep Learning) â† Most relevant to modern AI\n",
    "3. CS403 (NLP) or CS404 (Computer Vision) â† Specialize\n",
    "\n",
    "Given your interest in data science, I'd especially recommend the NLP path - it combines well with your data structures background.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Turn 3:**\n",
    "\n",
    "*Student:* \"How long will that path take?\"\n",
    "\n",
    "*Agent:* \"Let me break it down for you:\n",
    "\n",
    "**CS301:** 14 weeks (3 credits)\n",
    "**CS402:** 14 weeks (3 credits)  \n",
    "**CS403:** 14 weeks (3 credits)\n",
    "\n",
    "If you take one course per semester:\n",
    "- Fall 2024: CS301\n",
    "- Spring 2025: CS402\n",
    "- Fall 2025: CS403\n",
    "\n",
    "Since you're currently a Junior, this timeline would have you completing this sequence before graduation, with one semester remaining for an internship or capstone project.\n",
    "\n",
    "Alternatively, since these are online courses, you could potentially take CS301 this fall and CS402 in a winter intersession to accelerate your path.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Turn 4:**\n",
    "\n",
    "*Student:* \"I like that accelerated option. Can I register now?\"\n",
    "\n",
    "*Agent:* \"Yes! Since you've met the prerequisites for CS301, I can guide you through registration. The online section (CS301-W01) has 15 seats remaining.\n",
    "\n",
    "Would you like me to:\n",
    "1. Reserve a seat for you in CS301-W01 for Fall 2024?\n",
    "2. Add you to the notification list for the Winter CS402 section (registration opens in October)?\n",
    "3. Send you the course syllabus and professor information?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in this interaction is stark compared to the previous. With proper context engineering, the advisor carries the context instead of the student: it remembers who Olivia is (major, completions, preferences), filters to relevant options (online, project-based ML), checks prerequisites automatically, turns choices into a sequenced plan and timeline, and can even propose recommended actions (register, notify, send syllabi). The result is a fast, personalized, high-confidence experience that actually delivers value.\n",
    "\n",
    "So, while in our example, we waved a magic wand and the agent was suddenly more effective, what actually goes into making this improvement happen? To grasp the techniques used, we must first explore the types of contexts that an AI system manages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a complete context: the four pillars\n",
    "\n",
    "We can breakdown the context that goes into an AI application into four distinct types: \n",
    "\n",
    "1. System context\n",
    "2. User context\n",
    "3. Conversation context\n",
    "4. Retrieved context \n",
    "\n",
    "Each of these types plays a crucial role in making sure that an AI system has what it needs to properly respond. In this section, we'll explore each context type in more detail and setup a sample context to call a LLM (OpenAI). As you go through, make sure to run each code block that is provided as we build up the context and send it to a LLM at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. System Context: \"What Am I?\"\n",
    "\n",
    "The system context defines the AI's identity, capabilities, and knowledge. It is static, universal, and always present.\n",
    "\n",
    "It typically contains: \n",
    "- A role definition (\"You are a course advisor\")\n",
    "- Available tools and actions (`search_courses(query)`)\n",
    "- Domain knowledge and business rules (\"CS401 requires CS301\")\n",
    "- Behavioral guidelines (\"Ask one clarifying question at a time\")\n",
    "\n",
    "Below is an example of system context (also commonly called the system prompt) for our Redis University agent. Run the codeblock to create the context. We'll use it shortly when we assemble the full context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T23:08:05.910676Z",
     "start_time": "2025-12-03T23:08:05.908025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System context created successfully!\n",
      "You are a Redis University course advisor.\n",
      "\n",
      "Guidelines:\n",
      "1. Always provide specific course recommendations with clear reasoning\n",
      "2. Consider the student's background, completed courses, and interests\n",
      "3. Ensure prerequisites are met before recommending advanced courses\n",
      "4. Be encouraging and supportive in your guidance\n",
      "5. Do not offer courses that are not part of the curriculum\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_context = \"\"\"You are a Redis University course advisor.\n",
    "\n",
    "Guidelines:\n",
    "1. Always provide specific course recommendations with clear reasoning\n",
    "2. Consider the student's background, completed courses, and interests\n",
    "3. Ensure prerequisites are met before recommending advanced courses\n",
    "4. Be encouraging and supportive in your guidance\n",
    "5. Do not offer courses that are not part of the curriculum\n",
    "\"\"\"\n",
    "\n",
    "print(\"System context created successfully!\")\n",
    "print(system_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. User Context: \"Who Are You?\"\n",
    "\n",
    "The user context contains personal information about the specific user. It is typically dynamic, personalized, and retrieved from storage. \n",
    "\n",
    "It typically contains: \n",
    "- Profile information (major, year, background)\n",
    "- Preferences and learning style (\"prefers online/asynchronous\")\n",
    "- History and achievements (\"completed CS101/CS201/Calc I\")\n",
    "- Goals and constraints (\"Tue/Thu availability\")\n",
    "\n",
    "We have already seen an example of user context when we defined the user profile and preferences for Olivia earlier. Run the code block below to create the sample profile in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T23:08:10.248769Z",
     "start_time": "2025-12-03T23:08:10.246634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User context for Olivia created successfully!\n",
      "{'name': 'Olivia Jansen', 'background': 'Python developer, 2 years experience', 'completed_courses': ['RU101'], 'interests': ['machine learning', 'data science', 'vector search'], 'time_availability': 'evenings and weekends', 'goal': \"Build a RAG system for my company's documentation\"}\n"
     ]
    }
   ],
   "source": [
    "olivia_profile = {\n",
    "    \"name\": \"Olivia Jansen\",\n",
    "    \"background\": \"Python developer, 2 years experience\",\n",
    "    \"completed_courses\": [\"RU101\"],\n",
    "    \"interests\": [\"machine learning\", \"data science\", \"vector search\"],\n",
    "    \"time_availability\": \"evenings and weekends\",\n",
    "    \"goal\": \"Build a RAG system for my company's documentation\",\n",
    "}\n",
    "\n",
    "print(\"User context for Olivia created successfully!\")\n",
    "print(olivia_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in production, the above context would likely come from a database, but we have hardcoded it above for demonstration purposes. \n",
    "\n",
    "The OpenAI API requires message content to be a string, so we need to convert our dictionary to text. We have two options:\n",
    "\n",
    "1. Send it as JSON string: `json.dumps(olivia_profile)`\n",
    "2. Format it as natural language: Use a formatting function for better readability and control\n",
    "\n",
    "We'll use option 2 because it's more readable and gives us precise control over how the information is presented to the LLM. This type of data transformation â€” converting structured data into LLM-friendly formats â€” is actually a core context engineering technique that we'll explore in further detail throughout this course.\n",
    "\n",
    "Now, run the code block below to create the function and format Olivia's profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T23:08:16.139331Z",
     "start_time": "2025-12-03T23:08:16.133495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Context: Student Profile:\n",
      "- Name: Olivia Jansen\n",
      "- Background: Python developer, 2 years experience\n",
      "- Completed Courses: RU101\n",
      "- Interests: machine learning, data science, vector search\n",
      "- Availability: evenings and weekends\n",
      "- Goal: Build a RAG system for my company's documentation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_user_context(profile):\n",
    "    \"\"\"Convert user profile dictionary to formatted context string\"\"\"\n",
    "    return f\"\"\"Student Profile:\n",
    "- Name: {profile['name']}\n",
    "- Background: {profile['background']}\n",
    "- Completed Courses: {', '.join(profile['completed_courses'])}\n",
    "- Interests: {', '.join(profile['interests'])}\n",
    "- Availability: {profile['time_availability']}\n",
    "- Goal: {profile['goal']}\n",
    "\"\"\"\n",
    "\n",
    "user_context = format_user_context(olivia_profile)\n",
    "print(\"Formatted Context:\", user_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conversation Context: \"What Have We Discussed?\"\n",
    "\n",
    "Conversation context is one of the most important parts of context since it represents the history of the current conversation. Typically this is implemented in the form of memory. It is dynamic, session-specific, and will grow over time. \n",
    "\n",
    "It typically contains: \n",
    "- Previous user messages\n",
    "- Previous AI responses\n",
    "- Decisions and commitments made\n",
    "- Topics explored\n",
    "\n",
    "We have also seen an example of conversation context earlier:\n",
    "```\n",
    "Turn 1:\n",
    "User: \"I'm interested in machine learning courses.\"\n",
    "AI: \"I recommend CS301: Introduction to Machine Learning...\"\n",
    "\n",
    "Turn 2:\n",
    "User: \"What comes after that course?\"\n",
    "[Current query - needs context from Turn 1 to understand \"that course\"]\n",
    "AI: \"After CS301, you can take CS402 or CS403...\"\n",
    "\n",
    "Turn 3:\n",
    "User: \"How long will that path take?\"\n",
    "[Current query - needs context from Turn 2 to understand \"that path\"]\n",
    "```\n",
    "\n",
    "In this notebook, since we don't have a chat interface, we're going to simulate conversation history by manually creating a list of message dictionaries. In a real application, this list would be dynamically built as users interact with the system, but here we'll pre-populate it to demonstrate how conversation context works.\n",
    "\n",
    "Examine and then run the following codeblock (it will execute silently as we will observe it in the next codeblock):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T23:21:49.954592Z",
     "start_time": "2025-12-03T23:21:49.950979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Start with an empty conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Step 2: Add the first user message\n",
    "conversation_history.append(\n",
    "    {\"role\": \"user\", \"content\": \"What Redis course should I take next?\"}\n",
    ")\n",
    "\n",
    "# Step 3: Add the AI's response (simulated)\n",
    "conversation_history.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\"Based on your Python background and completion of RU101, \n",
    "I recommend RU201: Redis for Python Developers. This course will teach you \n",
    "how to build Redis applications using redis-py, which aligns perfectly with \n",
    "your goal of building a RAG system.\"\"\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step 4: Add a follow-up question that references previous context\n",
    "conversation_history.append(\n",
    "    {\"role\": \"user\", \"content\": \"How long will that take me to complete?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the user said \"that\" instead of \"RU201\". The AI needs the conversation history to understand what \"that\" refers to. Let's view the complete conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T23:21:50.596947Z",
     "start_time": "2025-12-03T23:21:50.595242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1 (user):\n",
      "What Redis course should I take next?\n",
      "\n",
      "Turn 2 (assistant):\n",
      "Based on your Python background and completion of RU101, \n",
      "I recommend RU201: Redis for Python Developers. This course will teach you \n",
      "how to build Redis applications using redis-py, which aligns perfectly with \n",
      "your goal of building a RAG system.\n",
      "\n",
      "Turn 3 (user):\n",
      "How long will that take me to complete?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate(conversation_history, 1):\n",
    "    print(f\"Turn {i} ({msg['role']}):\")\n",
    "    print(f\"{msg['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retrieved Context: \"What Information Is Relevant?\"\n",
    "\n",
    "Lastly, there is retrieved context. This is information fetched on-demand based on the current query. This involves the system retrieving specific and relevant context across other systems.\n",
    "\n",
    "It typically contains: \n",
    "- Database records (course details, schedules)\n",
    "- Search results (relevant documents, FAQs)\n",
    "- API responses (real-time data, availability)\n",
    "- Computed information (eligibility checks, recommendations)\n",
    "\n",
    "For example, if a student asked the Redis University agent about \"CS301\", the system would need to find any relevant context to respond to the query. For instance, it would call a course catalog database, find the CS301 course and retrieve information like course details (description, prerequisites, format), current availability (X seats in online section), and professor ratings and reviews.\n",
    "\n",
    "Let's simulate what retrieved context will look like by hard-coding course content. Run the codeblock below (it will execute silently):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T23:21:51.314066Z",
     "start_time": "2025-12-03T23:21:51.311276Z"
    }
   },
   "outputs": [],
   "source": [
    "course_database = {\n",
    "    \"RU101\": {\n",
    "        \"title\": \"Introduction to Redis Data Structures\",\n",
    "        \"level\": \"Beginner\",\n",
    "        \"description\": \"Master Redis fundamentals: strings, hashes, lists, sets, and sorted sets\",\n",
    "        \"duration\": \"4-6 hours\",\n",
    "        \"prerequisites\": [],\n",
    "        \"topics\": [\"Data structures\", \"Basic commands\", \"Use cases\"],\n",
    "    },\n",
    "    \"RU201\": {\n",
    "        \"title\": \"Redis for Python Developers\",\n",
    "        \"level\": \"Intermediate\",\n",
    "        \"description\": \"Build production Redis applications with Python and redis-py\",\n",
    "        \"duration\": \"6-8 hours\",\n",
    "        \"prerequisites\": [\"RU101\", \"Python experience\"],\n",
    "        \"topics\": [\"redis-py library\", \"Connection pooling\", \"Pipelining\", \"Pub/Sub\"],\n",
    "    },\n",
    "    \"RU202\": {\n",
    "        \"title\": \"Redis for Java Developers\",\n",
    "        \"level\": \"Intermediate\",\n",
    "        \"description\": \"Build production Redis applications with Java and Jedis\",\n",
    "        \"duration\": \"6-8 hours\",\n",
    "        \"prerequisites\": [\"RU101\", \"Java experience\"],\n",
    "        \"topics\": [\n",
    "            \"Jedis library\",\n",
    "            \"Connection pooling\",\n",
    "            \"Transactions\",\n",
    "            \"Redis Streams\",\n",
    "        ],\n",
    "    },\n",
    "    \"RU301\": {\n",
    "        \"title\": \"Vector Similarity Search with Redis\",\n",
    "        \"level\": \"Advanced\",\n",
    "        \"description\": \"Implement semantic search and RAG systems with Redis vector capabilities\",\n",
    "        \"duration\": \"8-10 hours\",\n",
    "        \"prerequisites\": [\"RU201 or RU202\", \"ML/AI interest\"],\n",
    "        \"topics\": [\n",
    "            \"Vector embeddings\",\n",
    "            \"Semantic search\",\n",
    "            \"RAG architecture\",\n",
    "            \"Hybrid search\",\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's simulate retrieving course information based on a query. Just like before, we'll need to transform the data. Run the code block below to create the function and simulate the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T23:21:52.040792Z",
     "start_time": "2025-12-03T23:21:52.038317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course Details:\n",
      "        Code: RU201\n",
      "        Title: Redis for Python Developers\n",
      "        Level: Intermediate\n",
      "        Description: Build production Redis applications with Python and redis-py\n",
      "        Duration: 6-8 hours\n",
      "        Prerequisites: RU101, Python experience\n",
      "        Topics Covered: redis-py library, Connection pooling, Pipelining, Pub/Sub\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create a retrieval function\n",
    "\n",
    "def retrieve_course_info(course_code):\n",
    "    \"\"\"Retrieve detailed information about a specific course\"\"\"\n",
    "    course = course_database.get(course_code)\n",
    "    if not course:\n",
    "        return None\n",
    "\n",
    "    return f\"\"\"Course Details:\n",
    "        Code: {course_code}\n",
    "        Title: {course['title']}\n",
    "        Level: {course['level']}\n",
    "        Description: {course['description']}\n",
    "        Duration: {course['duration']}\n",
    "        Prerequisites: {', '.join(course['prerequisites']) if course['prerequisites'] else 'None'}\n",
    "        Topics Covered: {', '.join(course['topics'])}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Retrieve information about RU201\n",
    "retrieved_context = retrieve_course_info(\"RU201\")\n",
    "print(retrieved_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing It All Together: Complete Context Integration\n",
    "\n",
    "Now that we understand each context type individually, let's see how they work together to create an intelligent, personalized response.\n",
    "\n",
    "Here's how all four context types combine in a single LLM call:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           COMPLETE LLM REQUEST              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 1. SYSTEM CONTEXT (Static)                  â”‚\n",
    "â”‚    - Role: \"You are a course advisor\"       â”‚\n",
    "â”‚    - Domain: Available courses              â”‚\n",
    "â”‚    - Rules: Guidelines and constraints      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 2. USER CONTEXT (Dynamic - User Specific)   â”‚\n",
    "â”‚    - Profile: Sarah Chen, Python dev        â”‚\n",
    "â”‚    - History: Completed RU101               â”‚\n",
    "â”‚    - Goal: Build RAG system                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 3. CONVERSATION CONTEXT (Dynamic - Session) â”‚\n",
    "â”‚    - User: \"What course should I take?\"     â”‚\n",
    "â”‚    - AI: \"I recommend RU201...\"             â”‚\n",
    "â”‚    - User: \"How long will that take?\"       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 4. RETRIEVED CONTEXT (Dynamic - Query)      â”‚\n",
    "â”‚    - RU201 course details                   â”‚\n",
    "â”‚    - Duration, prerequisites, topics        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ RESULT: Personalized, context-aware answer  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Let's add them together into a messages list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T23:21:52.865996Z",
     "start_time": "2025-12-03T23:21:52.863806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 1 (system):\n",
      "You are a Redis University course advisor.\n",
      "\n",
      "Guidelines:\n",
      "1. Always provide specific course recommendations with clear reasoning\n",
      "2. Consider the student's background, completed courses, and interests\n",
      "3. Ensure prerequisites are met before recommending advanced courses\n",
      "4. Be encouraging and supportive in your guidance\n",
      "5. Do not offer courses that are not part of the curriculum\n",
      "\n",
      "\n",
      "Message 2 (system):\n",
      "Student Profile:\n",
      "- Name: Olivia Jansen\n",
      "- Background: Python developer, 2 years experience\n",
      "- Completed Courses: RU101\n",
      "- Interests: machine learning, data science, vector search\n",
      "- Availability: evenings and weekends\n",
      "- Goal: Build a RAG system for my company's documentation\n",
      "\n",
      "\n",
      "Message 3 (user):\n",
      "What Redis course should I take next?\n",
      "\n",
      "Message 4 (assistant):\n",
      "Based on your Python background and completion of RU101, \n",
      "I recommend RU201: Redis for Python Developers. This course will teach you \n",
      "how to build Redis applications using redis-py, which aligns perfectly with \n",
      "your goal of building a RAG system.\n",
      "\n",
      "Message 5 (user):\n",
      "How long will that take me to complete?\n",
      "\n",
      "Message 6 (system):\n",
      "Course Details:\n",
      "        Code: RU201\n",
      "        Title: Redis for Python Developers\n",
      "        Level: Intermediate\n",
      "        Description: Build production Redis applications with Python and redis-py\n",
      "        Duration: 6-8 hours\n",
      "        Prerequisites: RU101, Python experience\n",
      "        Topics Covered: redis-py library, Connection pooling, Pipelining, Pub/Sub\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Start with system context\n",
    "messages = [{\"role\": \"system\", \"content\": system_context}]\n",
    "\n",
    "# Step 2: Add user context\n",
    "messages.append({\"role\": \"system\", \"content\": user_context})\n",
    "\n",
    "# Step 3: Add conversation history\n",
    "messages.extend(conversation_history)\n",
    "\n",
    "# Step 4: Add retrieved context\n",
    "messages.append({\"role\": \"system\", \"content\": retrieved_context})\n",
    "\n",
    "# Check the final messages structure\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    print(f\"Message {i} ({msg['role']}):\")\n",
    "    print(f\"{msg['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the context to call the LLM:\n",
    "\n",
    "> âš ï¸ Note: You'll need your own [OpenAI API](https://platform.openai.com/api-keys) key to make the LLM call. Make sure to replace it in the code block below before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T23:26:55.721294Z",
     "start_time": "2025-12-03T23:26:53.559489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      "The RU201: Redis for Python Developers course will take you approximately 6-8 hours to complete. Given your experience and background, you should be able to manage this within your availability on evenings and weekends. This course will be a great opportunity for you to deepen your understanding of Redis and enhance your skills in building applications relevant to your goal of implementing a RAG system. Good luck, and enjoy the learning experience!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()  # Uses OPENAI_API_KEY from environment\n",
    "\n",
    "# Make the LLM call with complete context\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", messages=messages, temperature=0.7\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(\"AI Response:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap what just happened, we sent an assembled context to the LLM. It received all four context types and used them to generate a personalized response:\n",
    "\n",
    "1. **System Context** told it to act as a course advisor and provided course information\n",
    "2. **User Context** gave it Sarah's background, interests, and goals\n",
    "3. **Conversation Context** showed that \"that\" refers to RU201\n",
    "4. **Retrieved Context** provided detailed information about RU201's duration and topics\n",
    "\n",
    "The result is a response that:\n",
    "- Understands what course \"that\" refers to\n",
    "- Considers Sarah's available time (evenings and weekends)\n",
    "- Relates the duration to her specific situation\n",
    "- Stays aligned with her goal of building a RAG system\n",
    "\n",
    "While this was a simple example, it demonstrated all four layers of context that we will be managing throughout this course. As we build more complex agents, these same principles will apply â€” but the techniques for managing, optimizing, and orchestrating these contexts will become increasingly sophisticated.\n",
    "\n",
    "Yet even with perfect context engineering, every AI system hits the same wall: the finite size of the model's context window. Let's explore this fundamental constraint next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Token Usage\n",
    "\n",
    "Before we discuss context windows, let's build a practical tool to measure how many tokens our context actually uses. Understanding token consumption is essential for staying within limits and optimizing your context strategy.\n",
    "\n",
    "Run the following code to create token estimation and analysis functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CONTEXT TOKEN ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Token Usage by Role:\n",
      "  System context:      249 tokens\n",
      "  User messages:       18 tokens\n",
      "  Assistant messages:  61 tokens\n",
      "\n",
      "  TOTAL:               328 tokens\n",
      "  Model limit:         128,000 tokens\n",
      "  Remaining:           127,672 tokens\n",
      "  Usage:               0.26%\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'system': 249, 'user': 18, 'assistant': 61}, 328, 127672)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_tokens(text):\n",
    "    \"\"\"Rough token estimation (approximately 4 characters per token for English)\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "\n",
    "def analyze_context_usage(messages, model_limit=128000):\n",
    "    \"\"\"Analyze token usage across different context types in our messages\"\"\"\n",
    "    usage = {\"system\": 0, \"user\": 0, \"assistant\": 0}\n",
    "\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        tokens = estimate_tokens(msg[\"content\"])\n",
    "        usage[role] += tokens\n",
    "\n",
    "    total = sum(usage.values())\n",
    "    remaining = model_limit - total\n",
    "    percent_used = (total / model_limit) * 100\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"CONTEXT TOKEN ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nToken Usage by Role:\")\n",
    "    print(f\"  System context:      {usage['system']:,} tokens\")\n",
    "    print(f\"  User messages:       {usage['user']:,} tokens\")\n",
    "    print(f\"  Assistant messages:  {usage['assistant']:,} tokens\")\n",
    "    print(f\"\\n  TOTAL:               {total:,} tokens\")\n",
    "    print(f\"  Model limit:         {model_limit:,} tokens\")\n",
    "    print(f\"  Remaining:           {remaining:,} tokens\")\n",
    "    print(f\"  Usage:               {percent_used:.2f}%\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return usage, total, remaining\n",
    "\n",
    "\n",
    "# Analyze our current context\n",
    "analyze_context_usage(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple analysis shows that our current context uses only a small fraction of the available tokens. However, as conversations grow longer and we retrieve more documents, token usage can quickly balloon. Monitoring this helps you make informed decisions about what context to include or compress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Fundamental Challenge: Context Windows\n",
    "\n",
    "Even the most powerful AI models can only process a fixed amount of text in a single request. This is called the **context window**, which is essentially the size of our anecdotal clerk's desk. Here are the context window sizes of some of the most popular models available:\n",
    "\n",
    "| Model | Context Window |\n",
    "|-------|----------------|\n",
    "| GPT-4o | 128,000 tokens (~96,000 words) |\n",
    "| GPT-4o-mini | 128,000 tokens (~96,000 words) |\n",
    "| Claude 3.5 Sonnet | 200,000 tokens (~150,000 words) |\n",
    "\n",
    "This limit might look generous, but when we break it down, we have a ton of different context competing for space. Here is an example context budget breakdown for a single turn using a modeL:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   CONTEXT WINDOW - 128K tokens      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ System Context    â”‚ 2,000 tokens    â”‚  â† AI's role and rules\n",
    "â”‚ User Context      â”‚ 1,000 tokens    â”‚  â† Your profile\n",
    "â”‚ Conversation      â”‚ 4,000 tokens    â”‚  â† What we've discussed\n",
    "â”‚ Retrieved Info    â”‚ 5,000 tokens    â”‚  â† Relevant data\n",
    "â”‚ Your Query        â”‚ 100 tokens      â”‚  â† Current question\n",
    "â”‚ Response Space    â”‚ 4,000 tokens    â”‚  â† AI's answer\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ TOTAL TOKENS      â”‚ 16,100 tokens   â”‚\n",
    "â”‚ REMAINING TOKENS  â”‚ 111,900 tokens  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "That remaining space (111,900) might look comforting but remember this is just one turn. Our inputs will likely only grow longer (especially around conversation history), and as they do models have a harder time producing effective results. In fact [Chroma](https://www.trychroma.com/) revealed a critical challenge in working with LLMs: [context rot](https://research.trychroma.com/context-rot) - the phenomenon where LLM performance degrades non-uniformly as input context length increases, even on simple tasks.\n",
    "\n",
    "If we summarized the research findings around the size of context in a AI system, it would break out into the following: \n",
    "\n",
    "1. Longer context â‰  better context: Reliability drops and accelerates as inputs grow. This means we should treat the window like working memory; budget tokens when possible and keep the slice small.\n",
    "\n",
    "2. Relevance rules: High semantic match drives accuracy; loosely related text hurts. We should prioritize optimizing retrieval precision (rerank, tighten filters, rewrite queries).\n",
    "\n",
    "3. Distractors are toxic: Similar but wrong snippets do disproportionate damage as length increases. To reduce distractors, we should aggressively filter/threshold, dedupe, and avoid â€œjust in caseâ€ context.\n",
    "\n",
    "4. Placement & structure matter: Models attend unevenly across the window; key facts can be missed when buried. We should surface essentials early, chunk clearly, and summarize instead of pasting raw dumps. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Remember, every token spent on one thing is a token NOT available for another. This means you must constantly decide:\n",
    "\n",
    "- Which context is most relevant?\n",
    "- What can be omitted without hurting quality?\n",
    "- When to retrieve more vs. use what you have?\n",
    "- How to compress long conversations?\n",
    "\n",
    "This makes context engineering a practice of optimization within constraints. As conversations grow longer, systems accumulate more data, and applications become more sophisticated, context management becomes increasingly critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Challenges\n",
    "\n",
    "While managing the size of your context window is critical, simply fitting everything in isn't the only hurdle. As D. Breunig highlights in [How Contexts Fail (and How to Fix Them)](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html), there are several other ways that context can degrade the performance of your agent, even when you are well within the token limits.\n",
    "\n",
    "He mentions four key failure modes to watch out for:\n",
    "\n",
    "1.  **Context Poisoning**: This occurs when an error or hallucination makes its way into your context (e.g., conversation history) and is then treated as fact in subsequent turns. The model \"learns\" the wrong information from its own previous output, leading to a loop of compounding errors.\n",
    "\n",
    "2.  **Context Distraction**: Sometimes, having *too much* history can be detrimental. If an agent has a massive log of past actions, it may start to over-focus on repeating those past behaviors rather than reasoning about the current, novel situation. It relies on its \"contextual habit\" instead of its training.\n",
    "\n",
    "3.  **Context Confusion**: This happens when the context is cluttered with superfluous informationâ€”such as too many available tools or irrelevant document snippets. Even if the model *could* ignore them, the mere presence of these \"distractors\" increases the probability of the model getting confused or trying to use a tool that isn't needed.\n",
    "\n",
    "4.  **Context Clash**: This is when different parts of your context contradict each other. For example, your system prompt might say one thing, but a retrieved document or an early turn in the conversation history says the opposite. When the model faces this internal conflict, its reasoning capabilities often degrade significantly.\n",
    "\n",
    "Understanding these failure modes reinforces why context engineering is not just about *compression*â€”it's about *curation*. We must actively manage, clean, and sometimes even \"forget\" context to keep our agents sharp.\n",
    "\n",
    "In this course, we will not solve every one of these failure modes. Some, like context poisoning, remain as complex open research problems in AI. However, we will systematically dismantle the most common ones by building a series of \"progressive agents,\" each designed to overcome specific context limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Context Strategies\n",
    "\n",
    "To see the real impact of context engineering, let's compare how the same query performs with different amounts of context. This demonstrates why adding the right context matters.\n",
    "\n",
    "We'll test three strategies:\n",
    "1. **Minimal** - System context only (generic responses)\n",
    "2. **With User** - System + User context (personalized responses)\n",
    "3. **With Retrieval** - System + User + Retrieved context (expert responses)\n",
    "\n",
    "Run the following code to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What course should I take next?\n",
      "============================================================\n",
      "\n",
      "ğŸ”¹ MINIMAL strategy:\n",
      "To provide you with the best recommendation for your next course, I need to know a bit more about your background. Could you please share the courses you have already completed and any specific areas of interest you have within the context of Redis? This information will help me suggest a course that aligns with your skills and goals.\n",
      "\n",
      "\n",
      "ğŸ”¹ WITH_USER strategy:\n",
      "Hi Olivia!\n",
      "\n",
      "It's great to see your enthusiasm for building a RAG (Retrieval-Augmented Generation) system, especially leveraging your background in Python and your interest in machine learning and data science! \n",
      "\n",
      "Since you've already completed RU101, the next recommended course for you would be **RU102: Redis for Data Science**. This course dives into how Redis can be effectively used for data science applications, which aligns perfectly with your goal of developing a system for documentation retrieval. Additionally, you will learn about handling vector searchesâ€”crucial for implementing RAG systemsâ€”enhancing your machine learning projects.\n",
      "\n",
      "After completing RU102, if you're looking to further deepen your understanding, you could consider **RU201: Advanced Redis Techniques**, particularly to explore more complex\n",
      "\n",
      "\n",
      "ğŸ”¹ WITH_RETRIEVAL strategy:\n",
      "Given your background as a Python developer with 2 years of experience, and considering that you have already completed RU101, I highly recommend you take **RU201: Redis for Python Developers** next.\n",
      "\n",
      "**Here's why RU201 is a great fit for you:**\n",
      "\n",
      "1. **Prerequisites**: You have already completed RU101 and have the necessary Python experience, both of which are prerequisites for RU201. \n",
      "2. **Relevance to Your Interests**: RU201 will deepen your understanding of building production-grade Redis applications using Python, which is essential for your goal of developing a RAG system for your company's documentation.\n",
      "3. **Immediate Application**: The skills youâ€™ll gain from RU201 will be critical as you learn about connection pooling,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_context_strategies(user_profile, test_query):\n",
    "    \"\"\"Compare responses with different context strategies\"\"\"\n",
    "\n",
    "    strategies = {\n",
    "        \"minimal\": [\n",
    "            {\"role\": \"system\", \"content\": system_context},\n",
    "            {\"role\": \"user\", \"content\": test_query},\n",
    "        ],\n",
    "        \"with_user\": [\n",
    "            {\"role\": \"system\", \"content\": system_context},\n",
    "            {\"role\": \"system\", \"content\": format_user_context(user_profile)},\n",
    "            {\"role\": \"user\", \"content\": test_query},\n",
    "        ],\n",
    "        \"with_retrieval\": [\n",
    "            {\"role\": \"system\", \"content\": system_context},\n",
    "            {\"role\": \"system\", \"content\": format_user_context(user_profile)},\n",
    "            {\"role\": \"system\", \"content\": retrieve_course_info(\"RU201\")},\n",
    "            {\"role\": \"system\", \"content\": retrieve_course_info(\"RU301\")},\n",
    "            {\"role\": \"user\", \"content\": test_query},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(f\"Query: {test_query}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for strategy_name, messages in strategies.items():\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\", messages=messages, max_tokens=150\n",
    "        )\n",
    "\n",
    "        print(f\"\\nğŸ”¹ {strategy_name.upper()} strategy:\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print()\n",
    "\n",
    "\n",
    "# Test with a sample query\n",
    "test_context_strategies(olivia_profile, \"What course should I take next?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Observed\n",
    "\n",
    "Notice the dramatic difference in response quality:\n",
    "\n",
    "| Strategy | Behavior | Result |\n",
    "|----------|----------|--------|\n",
    "| **Minimal** | Asks clarifying questions | \"What's your background? What are your goals?\" |\n",
    "| **With User** | Personalizes immediately | \"Based on your Python experience and RU101...\" |\n",
    "| **With Retrieval** | Provides specific details | Includes course duration, topics, prerequisites |\n",
    "\n",
    "**Key Insight:** More relevant context = fewer back-and-forth interactions = better user experience.\n",
    "\n",
    "- **Minimal context** requires 3-4 interactions to gather enough information\n",
    "- **Rich context** provides a complete answer in 1 interaction\n",
    "\n",
    "The challenge becomes: determining what context is \"relevant\" while managing token budgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Context Engineering\n",
    "\n",
    "Based on what we've learned, here are five essential practices to follow:\n",
    "\n",
    "1. **Start simple, add complexity gradually** \n",
    "   - Begin with system context only\n",
    "   - Add user context when you have it\n",
    "   - Retrieve additional context only when needed\n",
    "\n",
    "2. **Measure token usage continuously**\n",
    "   - Track how much of your context window is being used\n",
    "   - Set alerts before hitting limits\n",
    "   - Monitor growth patterns over conversation length\n",
    "\n",
    "3. **Optimize for relevance, not completeness**\n",
    "   - Don't dump all available information into context\n",
    "   - Retrieve only what's relevant to the current query\n",
    "   - Filter and rank retrieved content by relevance\n",
    "\n",
    "4. **Use clear, structured formatting**\n",
    "   - Organize context with headers and sections\n",
    "   - Use bullet points and key-value pairs\n",
    "   - Make it easy for the model to find information\n",
    "\n",
    "5. **Test different context combinations**\n",
    "   - Experiment with what to include vs. exclude\n",
    "   - Measure response quality with different strategies\n",
    "   - Iterate based on real user interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up\n",
    "\n",
    "In this introduction, we started to explore how real AI applications depend on carefully constructed context. More specifically we covered the following:\n",
    "\n",
    "1. Context engineering is the practice of deliberately shaping the information surrounding an application, rather than just crafting individual prompts.\n",
    "2. There are four types of context we will focus on: system context, user context, conversation context, and retrieved context.\n",
    "3. All LLMs have fixed context windows, creating a fundamental constraint where every token used for one purpose is unavailable for another.\n",
    "4. Context rot is a phenomenon where LLM performance degrades as input context length increases, even on simple tasks.\n",
    "5. Context failures such as distraction, confusion, and clash occur when irrelevant or contradictory information is present in the context window.\n",
    "\n",
    "We are now ready to start building. In **Module 2: Data Engineering for Context**, we will learn how to prepare data for LLM consumptionâ€”the critical step before implementing RAG. We'll cover the data pipeline, chunking decisions, and see how context engineering can achieve 91% token reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
